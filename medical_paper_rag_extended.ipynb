{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š ì˜í•™/ê³¼í•™ ë…¼ë¬¸ ê²€ìƒ‰ â†’ RAG ì‹œìŠ¤í…œ (í™•ì¥íŒ)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **arXiv, PubMed** ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ ,\n",
    "**BioBERT, PubMedBERT** ë“± ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "## âœ¨ ì£¼ìš” ê¸°ëŠ¥\n",
    "\n",
    "### ğŸ“– ë…¼ë¬¸ ê²€ìƒ‰ ì†ŒìŠ¤\n",
    "| ì†ŒìŠ¤ | ë¶„ì•¼ | íŠ¹ì§• |\n",
    "|------|------|------|\n",
    "| **arXiv** | CS, ë¬¼ë¦¬, ìˆ˜í•™ | ë¬´ë£Œ PDF ì œê³µ |\n",
    "| **PubMed** | ì˜í•™, ìƒë¬¼í•™ | 3ì²œë§Œ+ ë…¼ë¬¸ |\n",
    "\n",
    "### ğŸ§  ì„ë² ë”© ëª¨ë¸\n",
    "| ëª¨ë¸ | íŠ¹í™” ë¶„ì•¼ | íŠ¹ì§• |\n",
    "|------|----------|------|\n",
    "| **all-MiniLM** | ì¼ë°˜ | ê°€ë³ê³  ë¹ ë¦„ |\n",
    "| **BioBERT** | ì˜í•™/ìƒë¬¼í•™ | ìƒì˜í•™ í…ìŠ¤íŠ¸ íŠ¹í™” |\n",
    "| **PubMedBERT** | ì˜í•™ ë…¼ë¬¸ | PubMed ë…¼ë¬¸ìœ¼ë¡œ í•™ìŠµ |\n",
    "| **SciBERT** | ê³¼í•™ ì „ë°˜ | ê³¼í•™ ë…¼ë¬¸ íŠ¹í™” |\n",
    "\n",
    "## ì „ì²´ íë¦„\n",
    "```\n",
    "1. ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ (arXiv / PubMed)\n",
    "       â†“\n",
    "2. ë…¼ë¬¸ ê²€ìƒ‰ & PDF ë‹¤ìš´ë¡œë“œ\n",
    "       â†“\n",
    "3. í…ìŠ¤íŠ¸ ì¶”ì¶œ & ì²­í‚¹\n",
    "       â†“\n",
    "4. ì„ë² ë”© ëª¨ë¸ ì„ íƒ (MiniLM / BioBERT / PubMedBERT / SciBERT)\n",
    "       â†“\n",
    "5. ë²¡í„° DB ìƒì„± (FAISS)\n",
    "       â†“\n",
    "6. RAG ì§ˆì˜ì‘ë‹µ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”§ Step 0: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n!pip install arxiv biopython pypdf2 pdfplumber -q\n!pip install langchain langchain-community langchain-openai -q\n!pip install faiss-cpu -q\n!pip install sentence-transformers transformers -q\n!pip install openai tiktoken -q\n!pip install requests xmltodict -q\n!pip install \"numpy<2\" -q  # NumPy 2.x í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°\n\nprint(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“¦ Step 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# ë…¼ë¬¸ ê²€ìƒ‰\n",
    "import arxiv\n",
    "import xmltodict\n",
    "\n",
    "# PDF ì²˜ë¦¬\n",
    "from PyPDF2 import PdfReader\n",
    "import pdfplumber\n",
    "\n",
    "# LangChain ê´€ë ¨\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "PAPERS_DIR = \"./papers\"\n",
    "VECTORSTORE_DIR = \"./vectorstore\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(PAPERS_DIR, exist_ok=True)\n",
    "os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ ë…¼ë¬¸ ì €ì¥ í´ë”: {PAPERS_DIR}\")\n",
    "print(f\"ğŸ“ ë²¡í„° DB í´ë”: {VECTORSTORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âš™ï¸ Step 2: ì„¤ì • ì„ íƒ\n",
    "\n",
    "### ğŸ¯ ì—¬ê¸°ì„œ ì›í•˜ëŠ” ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#         ğŸ¯ ì„¤ì • ì„ íƒ (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!)      #\n",
    "#############################################\n",
    "\n",
    "# ========== 1. ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ ==========\n",
    "# 'arxiv'   : ì»´í“¨í„°ê³¼í•™, ë¬¼ë¦¬í•™, ìˆ˜í•™ ë…¼ë¬¸\n",
    "# 'pubmed'  : ì˜í•™, ìƒë¬¼í•™ ë…¼ë¬¸\n",
    "# 'both'    : ë‘˜ ë‹¤ ê²€ìƒ‰\n",
    "SEARCH_SOURCE = 'pubmed'  # 'arxiv', 'pubmed', 'both' ì¤‘ ì„ íƒ\n",
    "\n",
    "# ========== 2. ê²€ìƒ‰ì–´ ì„¤ì • ==========\n",
    "SEARCH_QUERY = \"COVID-19 vaccine efficacy\"  # ì›í•˜ëŠ” ê²€ìƒ‰ì–´ ì…ë ¥\n",
    "MAX_RESULTS = 10  # ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜\n",
    "\n",
    "# ========== 3. ì„ë² ë”© ëª¨ë¸ ì„ íƒ ==========\n",
    "# 'minilm'      : ì¼ë°˜ ëª©ì  (ê°€ë³ê³  ë¹ ë¦„)\n",
    "# 'biobert'     : ì˜í•™/ìƒë¬¼í•™ íŠ¹í™”\n",
    "# 'pubmedbert'  : PubMed ë…¼ë¬¸ íŠ¹í™” (ì˜í•™ ìµœì )\n",
    "# 'scibert'     : ê³¼í•™ ë…¼ë¬¸ ì „ë°˜\n",
    "EMBEDDING_MODEL = 'pubmedbert'  # 'minilm', 'biobert', 'pubmedbert', 'scibert' ì¤‘ ì„ íƒ\n",
    "\n",
    "# ========== 4. LLM ì„¤ì • (ì„ íƒì‚¬í•­) ==========\n",
    "USE_OPENAI = True# OpenAI API ì‚¬ìš© ì—¬ë¶€\n",
    "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY_HERE\"  # OpenAI API í‚¤ (USE_OPENAI=Trueì¼ ë•Œ í•„ìš”)\n",
    "\n",
    "# ========== 5. ì²­í‚¹ ì„¤ì • ==========\n",
    "CHUNK_SIZE = 1000  # ì²­í¬ í¬ê¸° (ê¸€ì ìˆ˜)\n",
    "CHUNK_OVERLAP = 200  # ì²­í¬ ê²¹ì¹¨\n",
    "\n",
    "print(\"âš™ï¸ í˜„ì¬ ì„¤ì •:\")\n",
    "print(f\"   ğŸ“– ê²€ìƒ‰ ì†ŒìŠ¤: {SEARCH_SOURCE}\")\n",
    "print(f\"   ğŸ” ê²€ìƒ‰ì–´: {SEARCH_QUERY}\")\n",
    "print(f\"   ğŸ“„ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜: {MAX_RESULTS}\")\n",
    "print(f\"   ğŸ§  ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}\")\n",
    "print(f\"   ğŸ¤– OpenAI ì‚¬ìš©: {USE_OPENAI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ” Step 3: ë…¼ë¬¸ ê²€ìƒ‰ í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperSearcher:\n",
    "    \"\"\"\n",
    "    ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ëŠ” í´ë˜ìŠ¤\n",
    "    ì§€ì›: arXiv, PubMed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.papers = []\n",
    "    \n",
    "    # ==================== arXiv ê²€ìƒ‰ ====================\n",
    "    def search_arxiv(self, query: str, max_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        arXivì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str - ê²€ìƒ‰ì–´\n",
    "        max_results : int - ìµœëŒ€ ê²°ê³¼ ìˆ˜\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List[Dict] : ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ” arXivì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...\")\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        \n",
    "        papers = []\n",
    "        for result in search.results():\n",
    "            paper = {\n",
    "                'source': 'arXiv',\n",
    "                'title': result.title,\n",
    "                'authors': [author.name for author in result.authors],\n",
    "                'abstract': result.summary,\n",
    "                'pdf_url': result.pdf_url,\n",
    "                'published': result.published.strftime('%Y-%m-%d'),\n",
    "                'id': result.entry_id.split('/')[-1]\n",
    "            }\n",
    "            papers.append(paper)\n",
    "            print(f\"   ğŸ“„ {paper['title'][:60]}...\")\n",
    "        \n",
    "        print(f\"   âœ… arXiv: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬\")\n",
    "        return papers\n",
    "    \n",
    "    # ==================== PubMed ê²€ìƒ‰ ====================\n",
    "    def search_pubmed(self, query: str, max_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        PubMedì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ (NCBI E-utilities API ì‚¬ìš©)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str - ê²€ìƒ‰ì–´\n",
    "        max_results : int - ìµœëŒ€ ê²°ê³¼ ìˆ˜\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List[Dict] : ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ” PubMedì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...\")\n",
    "        \n",
    "        # Step 1: ê²€ìƒ‰í•˜ì—¬ PMID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "        search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "        search_params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': query,\n",
    "            'retmax': max_results,\n",
    "            'retmode': 'json',\n",
    "            'sort': 'relevance'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, params=search_params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            search_data = response.json()\n",
    "            pmids = search_data.get('esearchresult', {}).get('idlist', [])\n",
    "            \n",
    "            if not pmids:\n",
    "                print(\"   âš ï¸ PubMed: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "                return []\n",
    "            \n",
    "            # Step 2: PMIDë¡œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "            fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "            fetch_params = {\n",
    "                'db': 'pubmed',\n",
    "                'id': ','.join(pmids),\n",
    "                'retmode': 'xml'\n",
    "            }\n",
    "            \n",
    "            time.sleep(0.5)  # API ì œí•œ ì¤€ìˆ˜\n",
    "            response = requests.get(fetch_url, params=fetch_params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # XML íŒŒì‹±\n",
    "            data = xmltodict.parse(response.content)\n",
    "            articles = data.get('PubmedArticleSet', {}).get('PubmedArticle', [])\n",
    "            \n",
    "            # ë‹¨ì¼ ê²°ê³¼ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "            if isinstance(articles, dict):\n",
    "                articles = [articles]\n",
    "            \n",
    "            papers = []\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    medline = article.get('MedlineCitation', {})\n",
    "                    article_data = medline.get('Article', {})\n",
    "                    \n",
    "                    # ì œëª© ì¶”ì¶œ\n",
    "                    title = article_data.get('ArticleTitle', 'No Title')\n",
    "                    if isinstance(title, dict):\n",
    "                        title = title.get('#text', 'No Title')\n",
    "                    \n",
    "                    # ì´ˆë¡ ì¶”ì¶œ\n",
    "                    abstract_data = article_data.get('Abstract', {}).get('AbstractText', '')\n",
    "                    if isinstance(abstract_data, list):\n",
    "                        abstract = ' '.join([a.get('#text', str(a)) if isinstance(a, dict) else str(a) for a in abstract_data])\n",
    "                    elif isinstance(abstract_data, dict):\n",
    "                        abstract = abstract_data.get('#text', str(abstract_data))\n",
    "                    else:\n",
    "                        abstract = str(abstract_data) if abstract_data else 'No abstract available'\n",
    "                    \n",
    "                    # ì €ì ì¶”ì¶œ\n",
    "                    author_list = article_data.get('AuthorList', {}).get('Author', [])\n",
    "                    if isinstance(author_list, dict):\n",
    "                        author_list = [author_list]\n",
    "                    authors = []\n",
    "                    for author in author_list[:5]:  # ìµœëŒ€ 5ëª…\n",
    "                        if isinstance(author, dict):\n",
    "                            last = author.get('LastName', '')\n",
    "                            first = author.get('ForeName', '')\n",
    "                            if last:\n",
    "                                authors.append(f\"{last} {first}\".strip())\n",
    "                    \n",
    "                    # PMID\n",
    "                    pmid = medline.get('PMID', {}).get('#text', 'Unknown')\n",
    "                    \n",
    "                    # ë°œí–‰ì¼\n",
    "                    pub_date = article_data.get('Journal', {}).get('JournalIssue', {}).get('PubDate', {})\n",
    "                    year = pub_date.get('Year', 'Unknown')\n",
    "                    \n",
    "                    paper = {\n",
    "                        'source': 'PubMed',\n",
    "                        'title': title,\n",
    "                        'authors': authors if authors else ['Unknown'],\n",
    "                        'abstract': abstract,\n",
    "                        'pdf_url': None,  # PubMedëŠ” ì§ì ‘ PDF ë§í¬ ì œê³µ ì•ˆ í•¨\n",
    "                        'pubmed_url': f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\",\n",
    "                        'pmc_url': self._get_pmc_pdf_url(pmid),\n",
    "                        'published': str(year),\n",
    "                        'id': f\"PMID_{pmid}\"\n",
    "                    }\n",
    "                    papers.append(paper)\n",
    "                    print(f\"   ğŸ“„ {paper['title'][:60]}...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"   âœ… PubMed: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬\")\n",
    "            return papers\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ PubMed ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _get_pmc_pdf_url(self, pmid: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        PubMed Centralì—ì„œ ë¬´ë£Œ PDF URL ì°¾ê¸°\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # PMC ID í™•ì¸\n",
    "            link_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\"\n",
    "            params = {\n",
    "                'dbfrom': 'pubmed',\n",
    "                'db': 'pmc',\n",
    "                'id': pmid,\n",
    "                'retmode': 'json'\n",
    "            }\n",
    "            time.sleep(0.3)\n",
    "            response = requests.get(link_url, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            linksets = data.get('linksets', [{}])[0]\n",
    "            linksetdbs = linksets.get('linksetdbs', [])\n",
    "            \n",
    "            for linksetdb in linksetdbs:\n",
    "                if linksetdb.get('dbto') == 'pmc':\n",
    "                    pmc_ids = linksetdb.get('links', [])\n",
    "                    if pmc_ids:\n",
    "                        pmc_id = pmc_ids[0]\n",
    "                        return f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/pdf/\"\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # ==================== í†µí•© ê²€ìƒ‰ ====================\n",
    "    def search(self, query: str, source: str = 'both', max_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ì„ íƒí•œ ì†ŒìŠ¤ì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str - ê²€ìƒ‰ì–´\n",
    "        source : str - 'arxiv', 'pubmed', 'both'\n",
    "        max_results : int - ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List[Dict] : ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        papers = []\n",
    "        \n",
    "        if source in ['arxiv', 'both']:\n",
    "            papers.extend(self.search_arxiv(query, max_results))\n",
    "        \n",
    "        if source in ['pubmed', 'both']:\n",
    "            papers.extend(self.search_pubmed(query, max_results))\n",
    "        \n",
    "        self.papers = papers\n",
    "        return papers\n",
    "\n",
    "print(\"âœ… PaperSearcher í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“¥ Step 4: PDF ë‹¤ìš´ë¡œë“œ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFDownloader:\n",
    "    \"\"\"\n",
    "    ë…¼ë¬¸ PDF ë‹¤ìš´ë¡œë“œ í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir: str = PAPERS_DIR):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    def download(self, paper: Dict) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        ë…¼ë¬¸ PDF ë‹¤ìš´ë¡œë“œ\n",
    "        \"\"\"\n",
    "        # íŒŒì¼ëª… ìƒì„±\n",
    "        safe_title = re.sub(r'[^\\w\\s-]', '', paper['title'])[:50]\n",
    "        filename = f\"{paper['id']}_{safe_title}.pdf\"\n",
    "        filepath = os.path.join(self.save_dir, filename)\n",
    "        \n",
    "        # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"   â­ï¸ ì´ë¯¸ ì¡´ì¬: {filename[:40]}...\")\n",
    "            return filepath\n",
    "        \n",
    "        # PDF URL ê²°ì •\n",
    "        pdf_url = paper.get('pdf_url') or paper.get('pmc_url')\n",
    "        \n",
    "        if not pdf_url:\n",
    "            # PubMed ë…¼ë¬¸ì˜ ê²½ìš° ì´ˆë¡ë§Œ ì €ì¥\n",
    "            if paper['source'] == 'PubMed':\n",
    "                return self._save_abstract_as_text(paper, filename)\n",
    "            print(f\"   âš ï¸ PDF URL ì—†ìŒ: {paper['title'][:40]}...\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (compatible; ResearchBot/1.0)'}\n",
    "            response = requests.get(pdf_url, headers=headers, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            print(f\"   âœ… ë‹¤ìš´ë¡œë“œ: {filename[:40]}...\")\n",
    "            return filepath\n",
    "            \n",
    "        except Exception as e:\n",
    "            # PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì´ˆë¡ ì €ì¥\n",
    "            print(f\"   âš ï¸ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ì´ˆë¡ ì €ì¥: {paper['title'][:30]}...\")\n",
    "            return self._save_abstract_as_text(paper, filename)\n",
    "    \n",
    "    def _save_abstract_as_text(self, paper: Dict, filename: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        PDF ì—†ì„ ë•Œ ì´ˆë¡ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥\n",
    "        \"\"\"\n",
    "        txt_filename = filename.replace('.pdf', '.txt')\n",
    "        filepath = os.path.join(self.save_dir, txt_filename)\n",
    "        \n",
    "        content = f\"\"\"Title: {paper['title']}\n",
    "\n",
    "Authors: {', '.join(paper['authors'])}\n",
    "\n",
    "Source: {paper['source']}\n",
    "\n",
    "Published: {paper['published']}\n",
    "\n",
    "Abstract:\n",
    "{paper['abstract']}\n",
    "\n",
    "URL: {paper.get('pubmed_url', paper.get('pdf_url', 'N/A'))}\n",
    "\"\"\"\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    def download_all(self, papers: List[Dict]) -> List[str]:\n",
    "        \"\"\"\n",
    "        ëª¨ë“  ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“¥ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\\n\")\n",
    "        \n",
    "        downloaded = []\n",
    "        for paper in papers:\n",
    "            filepath = self.download(paper)\n",
    "            if filepath:\n",
    "                downloaded.append(filepath)\n",
    "            time.sleep(0.5)  # API ì œí•œ ì¤€ìˆ˜\n",
    "        \n",
    "        print(f\"\\nğŸ“ ì´ {len(downloaded)}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "        return downloaded\n",
    "\n",
    "print(\"âœ… PDFDownloader í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“„ Step 5: í…ìŠ¤íŠ¸ ì¶”ì¶œ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor:\n",
    "    \"\"\"\n",
    "    PDF/TXTì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract(filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        \"\"\"\n",
    "        if filepath.endswith('.txt'):\n",
    "            return TextExtractor._extract_from_txt(filepath)\n",
    "        elif filepath.endswith('.pdf'):\n",
    "            return TextExtractor._extract_from_pdf(filepath)\n",
    "        return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_from_txt(filepath: str) -> str:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_from_pdf(filepath: str) -> str:\n",
    "        text = \"\"\n",
    "        try:\n",
    "            # PyPDF2 ì‹œë„\n",
    "            reader = PdfReader(filepath)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ pdfplumber ì‹œë„\n",
    "            if len(text) < 100:\n",
    "                text = \"\"\n",
    "                with pdfplumber.open(filepath) as pdf:\n",
    "                    for page in pdf.pages:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            text += page_text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"      âš ï¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)[:50]}\")\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_all(filepaths: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ëª¨ë“  íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\\n\")\n",
    "        \n",
    "        documents = []\n",
    "        for filepath in filepaths:\n",
    "            filename = os.path.basename(filepath)\n",
    "            print(f\"   ğŸ“– {filename[:50]}...\")\n",
    "            \n",
    "            text = TextExtractor.extract(filepath)\n",
    "            if text:\n",
    "                documents.append({\n",
    "                    'text': text,\n",
    "                    'source': filename,\n",
    "                    'filepath': filepath\n",
    "                })\n",
    "                print(f\"      âœ… {len(text):,} ê¸€ì ì¶”ì¶œ\")\n",
    "            else:\n",
    "                print(f\"      âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "        return documents\n",
    "\n",
    "print(\"âœ… TextExtractor í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§  Step 6: ì„ë² ë”© ëª¨ë¸ ì„ íƒ í´ë˜ìŠ¤\n",
    "\n",
    "### ì§€ì› ëª¨ë¸\n",
    "| ëª¨ë¸ | ì„¤ëª… | ì°¨ì› |\n",
    "|------|------|------|\n",
    "| **all-MiniLM-L6-v2** | ì¼ë°˜ ëª©ì , ê°€ë³ê³  ë¹ ë¦„ | 384 |\n",
    "| **BioBERT** | ìƒì˜í•™ í…ìŠ¤íŠ¸ íŠ¹í™” | 768 |\n",
    "| **PubMedBERT** | PubMed ë…¼ë¬¸ í•™ìŠµ, ì˜í•™ ìµœì  | 768 |\n",
    "| **SciBERT** | ê³¼í•™ ë…¼ë¬¸ ì „ë°˜ | 768 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModelFactory:\n",
    "    \"\"\"\n",
    "    ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "    \n",
    "    # ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡\n",
    "    MODELS = {\n",
    "        'minilm': {\n",
    "            'name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "            'description': 'ì¼ë°˜ ëª©ì  (ê°€ë³ê³  ë¹ ë¦„)',\n",
    "            'dimension': 384\n",
    "        },\n",
    "        'biobert': {\n",
    "            'name': 'dmis-lab/biobert-base-cased-v1.2',\n",
    "            'description': 'ì˜í•™/ìƒë¬¼í•™ íŠ¹í™”',\n",
    "            'dimension': 768\n",
    "        },\n",
    "        'pubmedbert': {\n",
    "            'name': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "            'description': 'PubMed ë…¼ë¬¸ íŠ¹í™” (ì˜í•™ ìµœì )',\n",
    "            'dimension': 768\n",
    "        },\n",
    "        'scibert': {\n",
    "            'name': 'allenai/scibert_scivocab_uncased',\n",
    "            'description': 'ê³¼í•™ ë…¼ë¬¸ ì „ë°˜',\n",
    "            'dimension': 768\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def list_models(cls):\n",
    "        \"\"\"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥\"\"\"\n",
    "        print(\"\\nğŸ§  ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸:\\n\")\n",
    "        for key, info in cls.MODELS.items():\n",
    "            print(f\"   â€¢ {key}: {info['description']}\")\n",
    "            print(f\"     ëª¨ë¸: {info['name']}\")\n",
    "            print(f\"     ì°¨ì›: {info['dimension']}\")\n",
    "            print()\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, model_type: str = 'minilm', device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            'minilm', 'biobert', 'pubmedbert', 'scibert' ì¤‘ ì„ íƒ\n",
    "        device : str\n",
    "            'cpu' ë˜ëŠ” 'cuda'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        HuggingFaceEmbeddings : ì„ë² ë”© ëª¨ë¸\n",
    "        \"\"\"\n",
    "        if model_type not in cls.MODELS:\n",
    "            print(f\"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_type}. 'minilm' ì‚¬ìš©\")\n",
    "            model_type = 'minilm'\n",
    "        \n",
    "        model_info = cls.MODELS[model_type]\n",
    "        model_name = model_info['name']\n",
    "        \n",
    "        print(f\"\\nğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        print(f\"   ëª¨ë¸: {model_type}\")\n",
    "        print(f\"   ì„¤ëª…: {model_info['description']}\")\n",
    "        print(f\"   ì°¨ì›: {model_info['dimension']}\")\n",
    "        print(f\"   (ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\\n\")\n",
    "        \n",
    "        try:\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={'device': device},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            print(f\"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "            return embeddings\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "            print(\"   ê¸°ë³¸ ëª¨ë¸(minilm)ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...\")\n",
    "            \n",
    "            return HuggingFaceEmbeddings(\n",
    "                model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                model_kwargs={'device': device},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸\n",
    "EmbeddingModelFactory.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”§ Step 7: RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.embeddings = embeddings\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.vectorstore = None\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def build_vectorstore(self, documents: List[Dict]) -> FAISS:\n",
    "        \"\"\"\n",
    "        ë¬¸ì„œë“¤ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "        \"\"\"\n",
    "        print(\"\\nâœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...\")\n",
    "        \n",
    "        all_chunks = []\n",
    "        all_metadata = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            chunks = self.text_splitter.split_text(doc['text'])\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                all_chunks.append(chunk)\n",
    "                all_metadata.append({\n",
    "                    'source': doc['source'],\n",
    "                    'chunk_id': i\n",
    "                })\n",
    "            print(f\"   ğŸ“„ {doc['source'][:40]}...: {len(chunks)} ì²­í¬\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}ê°œ\")\n",
    "        \n",
    "        print(\"\\nğŸ’¾ ë²¡í„° DB ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "        \n",
    "        self.vectorstore = FAISS.from_texts(\n",
    "            texts=all_chunks,\n",
    "            embedding=self.embeddings,\n",
    "            metadatas=all_metadata\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ!\")\n",
    "        return self.vectorstore\n",
    "    \n",
    "    def save_vectorstore(self, path: str = VECTORSTORE_DIR):\n",
    "        \"\"\"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\"\"\"\n",
    "        if self.vectorstore:\n",
    "            self.vectorstore.save_local(path)\n",
    "            print(f\"ğŸ’¾ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {path}\")\n",
    "    \n",
    "    def load_vectorstore(self, path: str = VECTORSTORE_DIR):\n",
    "        \"\"\"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ\"\"\"\n",
    "        self.vectorstore = FAISS.load_local(\n",
    "            path,\n",
    "            self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ: {path}\")\n",
    "        return self.vectorstore\n",
    "    \n",
    "    def search(self, query: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰\n",
    "        \"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"âŒ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_vectorstore()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "            return []\n",
    "        \n",
    "        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        results = []\n",
    "        for doc, score in docs_with_scores:\n",
    "            results.append({\n",
    "                'content': doc.page_content,\n",
    "                'source': doc.metadata.get('source', 'Unknown'),\n",
    "                'score': float(score)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def answer(self, question: str, k: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜)\n",
    "        \"\"\"\n",
    "        results = self.search(question, k=k)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'contexts': results,\n",
    "            'sources': list(set([r['source'] for r in results]))\n",
    "        }\n",
    "\n",
    "print(\"âœ… RAGSystem í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš€ Step 8: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 1. ë…¼ë¬¸ ê²€ìƒ‰ ====================\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“š Step 1: ë…¼ë¬¸ ê²€ìƒ‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "searcher = PaperSearcher()\n",
    "papers = searcher.search(\n",
    "    query=SEARCH_QUERY,\n",
    "    source=SEARCH_SOURCE,\n",
    "    max_results=MAX_RESULTS\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ì´ {len(papers)}ê°œ ë…¼ë¬¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë³´ê¸°\n",
    "print(\"\\nğŸ“‹ ê²€ìƒ‰ëœ ë…¼ë¬¸ ëª©ë¡:\\n\")\n",
    "for i, paper in enumerate(papers, 1):\n",
    "    print(f\"[{i}] {paper['source']} | {paper['title'][:70]}...\")\n",
    "    print(f\"    ì €ì: {', '.join(paper['authors'][:3])}\")\n",
    "    print(f\"    ë°œí–‰: {paper['published']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 2. PDF ë‹¤ìš´ë¡œë“œ ====================\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“¥ Step 2: PDF ë‹¤ìš´ë¡œë“œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "downloader = PDFDownloader(PAPERS_DIR)\n",
    "downloaded_files = downloader.download_all(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ ====================\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“„ Step 3: í…ìŠ¤íŠ¸ ì¶”ì¶œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "documents = TextExtractor.extract_all(downloaded_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 4. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ====================\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§  Step 4: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "embeddings = EmbeddingModelFactory.create(\n",
    "    model_type=EMBEDDING_MODEL,\n",
    "    device='cpu'  # GPU ìˆìœ¼ë©´ 'cuda'ë¡œ ë³€ê²½\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 5. RAG ì‹œìŠ¤í…œ êµ¬ì¶• ====================\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ’¾ Step 5: RAG ì‹œìŠ¤í…œ êµ¬ì¶•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rag = RAGSystem(\n",
    "    embeddings=embeddings,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "vectorstore = rag.build_vectorstore(documents)\n",
    "\n",
    "# ì €ì¥\n",
    "rag.save_vectorstore(VECTORSTORE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ’¬ Step 9: ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"â“ ì§ˆë¬¸: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = rag.answer(question, k=k)\n",
    "    \n",
    "    print(f\"\\nğŸ“š ê´€ë ¨ ë¬¸ì„œ {len(result['contexts'])}ê°œ ê²€ìƒ‰ë¨:\\n\")\n",
    "    \n",
    "    for i, ctx in enumerate(result['contexts'], 1):\n",
    "        similarity = 1 / (1 + ctx['score'])  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜\n",
    "        print(f\"[{i}] ì¶œì²˜: {ctx['source'][:50]}\")\n",
    "        print(f\"    ìœ ì‚¬ë„: {similarity:.2%}\")\n",
    "        print(f\"    ë‚´ìš©: {ctx['content'][:300]}...\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\nğŸ“– ì°¸ê³  ë¬¸ì„œ: {', '.join(result['sources'])}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸!\n",
    "ask(\"What is the efficacy of COVID-19 vaccines?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\n",
    "ask(\"What are the side effects?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ ì§ì ‘ ì§ˆë¬¸í•´ë³´ì„¸ìš”!\n",
    "YOUR_QUESTION = \"What is the mechanism of action?\"  # ì›í•˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë³€ê²½\n",
    "ask(YOUR_QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¤– Step 10: OpenAI LLM ì—°ë™ (ì„ íƒì‚¬í•­)\n",
    "\n",
    "OpenAI APIë¥¼ ì‚¬ìš©í•˜ë©´ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OPENAI:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    \n",
    "    # LLM ì„¤ì •\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "    prompt_template = \"\"\"\n",
    "    ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "    ë‹¤ìŒ ë¬¸ë§¥(Context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n",
    "    ë‹µì„ ëª¨ë¥´ë©´ \"ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë§í•˜ì„¸ìš”.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer (í•œêµ­ì–´ë¡œ):\n",
    "    \"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # RAG ì²´ì¸ ìƒì„±\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… OpenAI RAG ì²´ì¸ ìƒì„± ì™„ë£Œ!\")\n",
    "    \n",
    "    def ask_llm(question: str):\n",
    "        \"\"\"OpenAI LLMì„ ì‚¬ìš©í•œ ì§ˆì˜ì‘ë‹µ\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"â“ ì§ˆë¬¸: {question}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = qa_chain({\"query\": question})\n",
    "        \n",
    "        print(f\"\\nğŸ¤– AI ë‹µë³€:\\n{result['result']}\")\n",
    "        print(f\"\\nğŸ“– ì°¸ê³  ë¬¸ì„œ:\")\n",
    "        for i, doc in enumerate(result['source_documents'], 1):\n",
    "            print(f\"   [{i}] {doc.metadata.get('source', 'Unknown')[:50]}\")\n",
    "        \n",
    "        return result\n",
    "else:\n",
    "    print(\"â„¹ï¸ OpenAIë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   USE_OPENAI = Trueë¡œ ì„¤ì •í•˜ê³  API í‚¤ë¥¼ ì…ë ¥í•˜ë©´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI LLM ì§ˆë¬¸ (USE_OPENAI=Trueì¸ ê²½ìš°ë§Œ ì‘ë™)\n",
    "if USE_OPENAI:\n",
    "    ask_llm(\"COVID-19 ë°±ì‹ ì˜ íš¨ëŠ¥ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”„ Step 11: ì €ì¥ëœ ë²¡í„° DB ì¬ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚˜ì¤‘ì— ì €ì¥ëœ ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_existing_rag(embedding_model: str = 'pubmedbert'):\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ ë²¡í„° DBë¥¼ ë¶ˆëŸ¬ì™€ì„œ RAG ì‹œìŠ¤í…œ ì¬êµ¬ì„±\n",
    "    \"\"\"\n",
    "    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì €ì¥í•  ë•Œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•´ì•¼ í•¨!)\n",
    "    embeddings = EmbeddingModelFactory.create(embedding_model)\n",
    "    \n",
    "    # RAG ì‹œìŠ¤í…œ ìƒì„±\n",
    "    rag = RAGSystem(embeddings)\n",
    "    \n",
    "    # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ\n",
    "    rag.load_vectorstore(VECTORSTORE_DIR)\n",
    "    \n",
    "    return rag\n",
    "\n",
    "# ì˜ˆì‹œ: ì €ì¥ëœ DB ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# rag = load_existing_rag('pubmedbert')\n",
    "# ask(\"Your question here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š ìš”ì•½\n",
    "\n",
    "### ğŸ¯ ì™„ì„±ëœ ê¸°ëŠ¥\n",
    "\n",
    "| ê¸°ëŠ¥ | ì˜µì…˜ |\n",
    "|------|------|\n",
    "| **ê²€ìƒ‰ ì†ŒìŠ¤** | arXiv, PubMed, ë‘˜ ë‹¤ |\n",
    "| **ì„ë² ë”© ëª¨ë¸** | MiniLM, BioBERT, PubMedBERT, SciBERT |\n",
    "| **LLM** | OpenAI (ì„ íƒ) |\n",
    "\n",
    "### ğŸ§  ì„ë² ë”© ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "| ë¶„ì•¼ | ì¶”ì²œ ëª¨ë¸ |\n",
    "|------|----------|\n",
    "| ì¼ë°˜/ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ | minilm |\n",
    "| ì˜í•™/ìƒë¬¼í•™ | biobert, pubmedbert |\n",
    "| ê³¼í•™ ì „ë°˜ | scibert |\n",
    "| PubMed ë…¼ë¬¸ | pubmedbert (ìµœì ) |\n",
    "\n",
    "### ğŸ“– ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "| ë¶„ì•¼ | ì¶”ì²œ ì†ŒìŠ¤ |\n",
    "|------|----------|\n",
    "| ì»´í“¨í„° ê³¼í•™ | arxiv |\n",
    "| ë¬¼ë¦¬í•™/ìˆ˜í•™ | arxiv |\n",
    "| ì˜í•™/ìƒë¬¼í•™ | pubmed |\n",
    "| ì¢…í•© ê²€ìƒ‰ | both |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ‰ ì™„ë£Œ!\n",
    "\n",
    "ì´ì œ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì™€ ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ ë…¼ë¬¸ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "### ğŸ’¡ í™œìš© íŒ\n",
    "\n",
    "1. **ì˜í•™ ë…¼ë¬¸ ë¶„ì„**: `SEARCH_SOURCE='pubmed'`, `EMBEDDING_MODEL='pubmedbert'`\n",
    "2. **AI ë…¼ë¬¸ ë¶„ì„**: `SEARCH_SOURCE='arxiv'`, `EMBEDDING_MODEL='scibert'`\n",
    "3. **ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: `EMBEDDING_MODEL='minilm'`\n",
    "\n",
    "### ğŸ”— ì°¸ê³  ë§í¬\n",
    "- [arXiv](https://arxiv.org/)\n",
    "- [PubMed](https://pubmed.ncbi.nlm.nih.gov/)\n",
    "- [BioBERT](https://github.com/dmis-lab/biobert)\n",
    "- [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}