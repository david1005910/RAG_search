#!/usr/bin/env python3
"""
Medical/Scientific Paper RAG System
- ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ì§€ì›
- ë…¼ë¬¸ ìš”ì•½ ê¸°ëŠ¥ (OpenAI API)
- ë‹¤êµ­ì–´ ì§€ì› (ì˜ì–´/í•œêµ­ì–´)
- .env íŒŒì¼ì„ í†µí•œ API í‚¤ ê´€ë¦¬
"""
#!pip install langchain langchain-community langchain-text-splitters langchain-openai faiss-cpu

import os
import requests
import time
import re
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ë…¼ë¬¸ ê²€ìƒ‰
import arxiv
import xmltodict
# PDF ì²˜ë¦¬
from PyPDF2 import PdfReader
import pdfplumber

# LangChain ê´€ë ¨
from langchain_text_splitters import RecursiveCharacterTextSplitter
#from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
import warnings
warnings.filterwarnings('ignore')

# ë””ë ‰í† ë¦¬ ì„¤ì •
PAPERS_DIR = "./papers"
VECTORSTORE_DIR = "./vectorstore"
os.makedirs(PAPERS_DIR, exist_ok=True)
os.makedirs(VECTORSTORE_DIR, exist_ok=True)


# ==================== ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ====================
def detect_language(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€ (í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´)"""
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    total_chars = len(re.findall(r'[a-zA-Zê°€-í£]', text))

    if total_chars == 0:
        return 'en'

    korean_ratio = korean_chars / total_chars
    return 'ko' if korean_ratio > 0.3 else 'en'


def translate_to_english(text: str, openai_api_key: str = None) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ (ê²€ìƒ‰ìš©)"""
    if not openai_api_key:
        # API í‚¤ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì˜í•™ ìš©ì–´ ë§¤í•‘ ì‚¬ìš©
        medical_terms = {
            'ë‹¹ë‡¨ë³‘': 'diabetes mellitus',
            'ë‹¹ë‡¨': 'diabetes',
            'ì¹˜ë£Œ': 'treatment',
            'ì¹˜ë£Œë²•': 'treatment therapy',
            'ì•”': 'cancer',
            'íì•”': 'lung cancer',
            'ìœ ë°©ì•”': 'breast cancer',
            'ìœ„ì•”': 'gastric cancer stomach cancer',
            'ê°„ì•”': 'liver cancer hepatocellular carcinoma',
            'ëŒ€ì¥ì•”': 'colon cancer colorectal cancer',
            'ê³ í˜ˆì••': 'hypertension',
            'ì‹¬ì¥ë³‘': 'heart disease cardiovascular disease',
            'ë‡Œì¡¸ì¤‘': 'stroke cerebrovascular accident',
            'ì¹˜ë§¤': 'dementia alzheimer',
            'ìš°ìš¸ì¦': 'depression',
            'ë¹„ë§Œ': 'obesity',
            'ê³¨ë‹¤ê³µì¦': 'osteoporosis',
            'ê´€ì ˆì—¼': 'arthritis',
            'ì²œì‹': 'asthma',
            'ì•Œë ˆë¥´ê¸°': 'allergy',
            'ê°ì—¼': 'infection',
            'ë°”ì´ëŸ¬ìŠ¤': 'virus viral',
            'ë°±ì‹ ': 'vaccine vaccination',
            'í•­ìƒì œ': 'antibiotic',
            'ë©´ì—­': 'immunity immune',
            'ì§„ë‹¨': 'diagnosis diagnostic',
            'ì˜ˆë°©': 'prevention preventive',
            'ì¦ìƒ': 'symptoms',
            'ë¶€ì‘ìš©': 'side effects adverse effects',
            'ì„ìƒì‹œí—˜': 'clinical trial',
            'ì•½ë¬¼': 'drug medication',
            'ìˆ˜ìˆ ': 'surgery surgical',
            'ë°©ì‚¬ì„ ': 'radiation radiotherapy',
            'í™”í•™ìš”ë²•': 'chemotherapy',
        }

        translated = text
        for ko, en in medical_terms.items():
            if ko in translated:
                translated = translated.replace(ko, en)

        # ë‚¨ì€ í•œê¸€ ì œê±°
        translated = re.sub(r'[ê°€-í£]+', '', translated).strip()
        return translated if translated else text

    try:
        from openai import OpenAI
        client = OpenAI(api_key=openai_api_key)

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a medical/scientific translator. Translate the following Korean medical query to English. Only output the English translation, nothing else."},
                {"role": "user", "content": text}
            ],
            max_tokens=100,
            temperature=0.1
        )

        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"   âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©: {str(e)[:30]}")
        return translate_to_english(text, None)  # API ì—†ì´ ì¬ì‹œë„


# ==================== ì„¤ì • í´ë˜ìŠ¤ ====================
class Config:
    """RAG ì‹œìŠ¤í…œ ì„¤ì •"""
    def __init__(self):
        self.search_source = 'pubmed'
        self.search_query = ''  # ì›ë³¸ ê²€ìƒ‰ì–´ (í•œêµ­ì–´ ê°€ëŠ¥)
        self.search_query_en = ''  # ì˜ì–´ë¡œ ë²ˆì—­ëœ ê²€ìƒ‰ì–´ (ì‹¤ì œ ê²€ìƒ‰ìš©)
        self.max_results = 5
        self.embedding_model = 'pubmedbert'  # ê¸°ë³¸ê°’ì„ PubMedBERTë¡œ ë³€ê²½
        self.sparse_method = 'bm25'  # 'bm25' ë˜ëŠ” 'splade'
        self.vector_db = 'faiss'  # 'faiss' ë˜ëŠ” 'qdrant'
        self.chunk_size = 1000
        self.chunk_overlap = 200
        # Reranking ì„¤ì •
        self.use_reranking = False
        self.reranker_model = 'ms-marco'  # 'ms-marco', 'ms-marco-large', 'bge-reranker', 'bge-reranker-large'
        # .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
        self.pubmed_api_key = os.getenv('PUBMED_API_KEY') or None
        self.pubmed_email = os.getenv('PUBMED_EMAIL') or None
        self.openai_api_key = os.getenv('OPENAI_API_KEY') or None
        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY') or None
        self.language = 'en'  # ê°ì§€ëœ ì–¸ì–´

    def interactive_setup(self):
        """ëŒ€í™”í˜• ì„¤ì •"""
        print("\n" + "=" * 60)
        print("âš™ï¸  RAG ì‹œìŠ¤í…œ ì„¤ì •")
        print("=" * 60)

        # ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ
        print("\nğŸ“– ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ:")
        print("   1. PubMed (ì˜í•™/ìƒë¬¼í•™)")
        print("   2. arXiv (CS/ë¬¼ë¦¬/ìˆ˜í•™)")
        print("   3. ë‘˜ ë‹¤")
        choice = input("ì„ íƒ [1]: ").strip() or "1"
        self.search_source = {'1': 'pubmed', '2': 'arxiv', '3': 'both'}.get(choice, 'pubmed')

        # ê²€ìƒ‰ì–´ ì…ë ¥
        self.search_query = input("\nğŸ” ê²€ìƒ‰ì–´ ì…ë ¥: ").strip()
        if not self.search_query:
            self.search_query = "COVID-19 vaccine efficacy"
            print(f"   ê¸°ë³¸ê°’ ì‚¬ìš©: {self.search_query}")

        # ì–¸ì–´ ê°ì§€
        self.language = detect_language(self.search_query)
        lang_name = "í•œêµ­ì–´" if self.language == 'ko' else "English"
        print(f"   ğŸŒ ê°ì§€ëœ ì–¸ì–´: {lang_name}")

        # ìµœëŒ€ ê²°ê³¼ ìˆ˜
        max_res = input(f"\nğŸ“„ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜ [{self.max_results}]: ").strip()
        if max_res.isdigit():
            self.max_results = int(max_res)

        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
        print("\nğŸ§  Dense ì„ë² ë”© ëª¨ë¸ ì„ íƒ:")
        print("   [PubMed/ì˜í•™ íŠ¹í™” - ë¡œì»¬ ì‹¤í–‰]")
        print("   1. PubMedBERT Full (PubMed ì „ë¬¸ í•™ìŠµ) [ê¶Œì¥]")
        print("   2. PubMedBERT Abstract (PubMed ì´ˆë¡ íŠ¹í™”)")
        print("   3. BioBERT (ì˜í•™/ìƒë¬¼í•™)")
        print("   4. BioLinkBERT (ì˜í•™ ë¬¸í—Œ ë§í¬ í•™ìŠµ)")
        print("   5. SciBERT (ê³¼í•™ ë…¼ë¬¸ ì „ë°˜)")
        print("   [ì¼ë°˜ ëª¨ë¸]")
        print("   6. BERT-base (ì¼ë°˜)")
        print("   [OpenAI API - ë¹ ë¥´ê³  ì •í™•]")
        print("   7. OpenAI Small (ë¹ ë¦„, ì €ë ´)")
        print("   8. OpenAI Large (ê³ ì„±ëŠ¥)")
        model_choice = input("ì„ íƒ [1 - PubMedBERT]: ").strip() or "1"
        self.embedding_model = {
            '1': 'pubmedbert',
            '2': 'pubmedbert-abs',
            '3': 'biobert',
            '4': 'biolinkbert',
            '5': 'scibert',
            '6': 'bert-base',
            '7': 'openai-small',
            '8': 'openai-large'
        }.get(model_choice, 'pubmedbert')

        # Sparse ê²€ìƒ‰ ë°©ì‹ ì„ íƒ
        print("\nğŸ” Sparse ê²€ìƒ‰ ë°©ì‹ ì„ íƒ:")
        print("   1. BM25 (ì „í†µì  í‚¤ì›Œë“œ ë§¤ì¹­) [ê¸°ë³¸ê°’]")
        print("   2. SPLADE (ì‹ ê²½ë§ ê¸°ë°˜ í™•ì¥ ê²€ìƒ‰)")
        sparse_choice = input("ì„ íƒ [1 - BM25]: ").strip() or "1"
        self.sparse_method = {
            '1': 'bm25',
            '2': 'splade'
        }.get(sparse_choice, 'bm25')

        # Vector DB ì„ íƒ
        print("\nğŸ—„ï¸ Vector DB ì„ íƒ:")
        print("   1. FAISS (Facebook AI, ë¡œì»¬ íŒŒì¼ ì €ì¥) [ê¸°ë³¸ê°’]")
        print("   2. Qdrant (Named Vectors, HNSW ì¸ë±ìŠ¤, Payload í•„í„°ë§)")
        db_choice = input("ì„ íƒ [1 - FAISS]: ").strip() or "1"
        self.vector_db = {
            '1': 'faiss',
            '2': 'qdrant'
        }.get(db_choice, 'faiss')

        # Reranking ì„¤ì •
        print("\nğŸ¯ Reranking ì„¤ì • (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ):")
        print("   1. ì‚¬ìš© ì•ˆí•¨ [ê¸°ë³¸ê°’]")
        print("   2. MS-MARCO MiniLM (ë¹ ë¦„)")
        print("   3. MS-MARCO MiniLM Large (ì •í™•)")
        print("   4. BGE Reranker (ë‹¤êµ­ì–´)")
        print("   5. BGE Reranker Large (ê³ ì •í™•ë„)")
        rerank_choice = input("ì„ íƒ [1]: ").strip() or "1"
        if rerank_choice == '1':
            self.use_reranking = False
        else:
            self.use_reranking = True
            self.reranker_model = {
                '2': 'ms-marco',
                '3': 'ms-marco-large',
                '4': 'bge-reranker',
                '5': 'bge-reranker-large'
            }.get(rerank_choice, 'ms-marco')

        # PubMed API ì„¤ì •
        if self.search_source in ['pubmed', 'both']:
            print("\nğŸ”‘ PubMed API ì„¤ì • (ì„ íƒì‚¬í•­ - ì†ë„ í–¥ìƒ):")
            if self.pubmed_api_key:
                print(f"   âœ… .envì—ì„œ ë¡œë“œë¨ (Key: {self.pubmed_api_key[:8]}...)")
            else:
                print("   API í‚¤ê°€ ì—†ìœ¼ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
                print("   ë°œê¸‰: https://www.ncbi.nlm.nih.gov/account/settings/")
                self.pubmed_api_key = input("   API Key: ").strip() or None
                if self.pubmed_api_key:
                    self.pubmed_email = input("   Email: ").strip() or None

        # OpenAI API ì„¤ì • (ë…¼ë¬¸ ìš”ì•½ ë° ë²ˆì—­ìš©)
        print("\nğŸ¤– OpenAI API ì„¤ì • (ë…¼ë¬¸ ìš”ì•½ ë° í•œâ†’ì˜ ë²ˆì—­ìš©):")
        if self.openai_api_key:
            print(f"   âœ… .envì—ì„œ ë¡œë“œë¨ (Key: {self.openai_api_key[:12]}...)")
        else:
            print("   API í‚¤ê°€ ì—†ìœ¼ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            print("   ë°œê¸‰: https://platform.openai.com/api-keys")
            self.openai_api_key = input("   OpenAI API Key: ").strip() or None

        # í•œêµ­ì–´ ê²€ìƒ‰ì–´ì¸ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­
        if self.language == 'ko':
            print("\nğŸ”„ í•œêµ­ì–´ ê²€ìƒ‰ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘...")
            self.search_query_en = translate_to_english(self.search_query, self.openai_api_key)
            print(f"   ğŸ‡°ğŸ‡· ì›ë³¸: {self.search_query}")
            print(f"   ğŸ‡ºğŸ‡¸ ë²ˆì—­: {self.search_query_en}")
        else:
            self.search_query_en = self.search_query

        print("\n" + "-" * 60)
        print("âœ… ì„¤ì • ì™„ë£Œ!")
        print(f"   ğŸ“– ì†ŒìŠ¤: {self.search_source}")
        print(f"   ğŸ” ê²€ìƒ‰ì–´: {self.search_query}")
        if self.language == 'ko':
            print(f"   ğŸ” ê²€ìƒ‰ì–´(ì˜ë¬¸): {self.search_query_en}")
        print(f"   ğŸŒ ì–¸ì–´: {lang_name} (ì‘ë‹µë„ {lang_name}ë¡œ)")
        print(f"   ğŸ“„ ìµœëŒ€ ë…¼ë¬¸: {self.max_results}")
        print(f"   ğŸ§  Dense ëª¨ë¸: {self.embedding_model}")
        print(f"   ğŸ”¤ Sparse ë°©ì‹: {self.sparse_method.upper()}")
        print(f"   ğŸ—„ï¸ Vector DB: {self.vector_db.upper()}")
        if self.use_reranking:
            print(f"   ğŸ¯ Reranking: {self.reranker_model.upper()}")
        else:
            print(f"   ğŸ¯ Reranking: ì‚¬ìš© ì•ˆí•¨")
        if self.pubmed_api_key:
            print(f"   ğŸ”‘ PubMed API: ì„¤ì •ë¨")
        if self.openai_api_key:
            print(f"   ğŸ¤– OpenAI API: ì„¤ì •ë¨ (ìš”ì•½/ë²ˆì—­ í™œì„±í™”)")
        print("-" * 60)

        return self


# ==================== ë…¼ë¬¸ ìš”ì•½ í´ë˜ìŠ¤ ====================
class PaperSummarizer:
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ìš”ì•½"""

    def __init__(self, api_key: str = None, language: str = 'en'):
        self.api_key = api_key
        self.language = language

    def summarize(self, papers: List[Dict], documents: List[Dict]) -> List[Dict]:
        """ë…¼ë¬¸ë“¤ì„ ìš”ì•½"""
        if not self.api_key:
            print("\nâš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return papers

        print("\n" + "=" * 60)
        print("ğŸ“ ë…¼ë¬¸ ìš”ì•½ (OpenAI API)")
        print("=" * 60)
        print(f"   ğŸ“š ì´ {len(papers)}ê°œ ë…¼ë¬¸ ìš”ì•½ ì˜ˆì •\n")

        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.api_key)
        except ImportError:
            print("âš ï¸ OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai")
            return papers
        except Exception as e:
            print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:50]}")
            return papers

        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸
        if self.language == 'ko':
            system_prompt = """ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì„ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë…¼ë¬¸ì˜ ì œëª©, ì €ì, ì´ˆë¡, ë³¸ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:
- ì—°êµ¬ ëª©ì 
- ì£¼ìš” ë°©ë²•
- í•µì‹¬ ê²°ê³¼
- ê²°ë¡  ë° ì˜ì˜"""
        else:
            system_prompt = """You are an expert at summarizing medical/scientific papers.
Based on the title, authors, abstract, and content, provide a concise summary.
Follow this format:
- Research Objective
- Key Methods
- Main Results
- Conclusion & Significance"""

        summarized_papers = []
        success_count = 0
        fail_count = 0

        # tqdm ì§„í–‰ ë°”ë¡œ ìš”ì•½ ì§„í–‰ ìƒí™© í‘œì‹œ
        with tqdm(
            total=len(papers),
            desc="   ğŸ¤– AI ìš”ì•½ ìƒì„±",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='cyan'
        ) as pbar:
            for i, paper in enumerate(papers):
                title_short = paper['title'][:35] + "..." if len(paper['title']) > 35 else paper['title']
                pbar.set_postfix_str(f"ğŸ“„ {title_short}")

                # í•´ë‹¹ ë…¼ë¬¸ì˜ ë³¸ë¬¸ ì°¾ê¸°
                paper_content = ""
                for doc in documents:
                    if paper['id'] in doc['source']:
                        paper_content = doc['text'][:3000]  # í† í° ì œí•œ
                        break

                # ìš”ì•½í•  ë‚´ìš© êµ¬ì„±
                content_to_summarize = f"""
Title: {paper['title']}
Authors: {', '.join(paper['authors'][:5])}
Published: {paper['published']}
Source: {paper['source']}

Abstract:
{paper['abstract']}

Content:
{paper_content}
"""

                try:
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": content_to_summarize}
                        ],
                        max_tokens=500,
                        temperature=0.3
                    )

                    summary = response.choices[0].message.content
                    paper['summary'] = summary
                    success_count += 1

                except Exception as e:
                    # ì‹¤íŒ¨ ì‹œ ì „ì²´ ì´ˆë¡ ì‚¬ìš©
                    paper['summary'] = f"[ì´ˆë¡ ì›ë¬¸]\n{paper['abstract']}"
                    fail_count += 1

                summarized_papers.append(paper)
                pbar.update(1)
                time.sleep(0.5)  # API ì†ë„ ì œí•œ

        # ìš”ì•½ ê²°ê³¼ í†µê³„
        print("\n" + "-" * 60)
        print("ğŸ“Š ìš”ì•½ ê²°ê³¼ í†µê³„")
        print("-" * 60)
        print(f"   âœ… ìš”ì•½ ì„±ê³µ: {success_count}ê°œ")
        if fail_count > 0:
            print(f"   âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {fail_count}ê°œ (ì´ˆë¡ìœ¼ë¡œ ëŒ€ì²´)")
        print("-" * 60)

        # ìš”ì•½ ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 70)
        print("ğŸ“‹ ë…¼ë¬¸ ìš”ì•½ ê²°ê³¼")
        print("=" * 70)

        for i, paper in enumerate(summarized_papers):
            print(f"\n{'â”€' * 70}")
            print(f"ğŸ“„ [{i+1}/{len(summarized_papers)}] {paper['title']}")
            print(f"{'â”€' * 70}")
            print(f"ğŸ‘¤ ì €ì: {', '.join(paper['authors'][:3])}" + (" ì™¸" if len(paper['authors']) > 3 else ""))
            print(f"ğŸ“… ë°œí–‰: {paper['published'][:10] if paper.get('published') else 'N/A'}")
            print(f"ğŸ“– ì¶œì²˜: {paper['source']}")
            print(f"{'â”€' * 70}")
            print("ğŸ“ ìš”ì•½:")
            print()
            # ìš”ì•½ ë‚´ìš© ì „ì²´ ì¶œë ¥ (ì¤„ë°”ê¿ˆ ìœ ì§€)
            summary = paper.get('summary', 'No summary available')
            for line in summary.split('\n'):
                print(f"   {line}")
            print()

        print("=" * 70)
        input("\nâ Enterë¥¼ ëˆŒëŸ¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰...")

        return summarized_papers


# ==================== PaperSearcher í´ë˜ìŠ¤ ====================
class PaperSearcher:
    def __init__(self, api_key: str = None, email: str = None):
        self.papers = []
        self.api_key = api_key
        self.email = email

    def search_arxiv(self, query: str, max_results: int = 5) -> List[Dict]:
        print(f"\nğŸ” arXivì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")

        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance
        )

        papers = []
        for result in search.results():
            paper = {
                'source': 'arXiv',
                'title': result.title,
                'authors': [author.name for author in result.authors],
                'abstract': result.summary,
                'pdf_url': result.pdf_url,
                'published': result.published.strftime('%Y-%m-%d'),
                'id': result.entry_id.split('/')[-1]
            }
            papers.append(paper)
            print(f"   ğŸ“„ {paper['title'][:60]}...")

        print(f"   âœ… arXiv: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
        return papers

    def search_pubmed(self, query: str, max_results: int = 5) -> List[Dict]:
        print(f"\nğŸ” PubMedì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")

        search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        search_params = {
            'db': 'pubmed',
            'term': query,
            'retmax': max_results,
            'retmode': 'json',
            'sort': 'relevance'
        }

        if self.api_key:
            search_params['api_key'] = self.api_key
            print("   ğŸ”‘ API í‚¤ ì‚¬ìš© ì¤‘...")
        if self.email:
            search_params['email'] = self.email

        try:
            response = requests.get(search_url, params=search_params, timeout=30)
            response.raise_for_status()
            search_data = response.json()
            pmids = search_data.get('esearchresult', {}).get('idlist', [])

            if not pmids:
                print("   âš ï¸ PubMed: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return []

            fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
            fetch_params = {
                'db': 'pubmed',
                'id': ','.join(pmids),
                'retmode': 'xml'
            }
            if self.api_key:
                fetch_params['api_key'] = self.api_key

            if not self.api_key:
                time.sleep(0.35)

            response = requests.get(fetch_url, params=fetch_params, timeout=30)
            response.raise_for_status()

            data = xmltodict.parse(response.content)
            articles = data.get('PubmedArticleSet', {}).get('PubmedArticle', [])

            if isinstance(articles, dict):
                articles = [articles]

            papers = []
            for article in articles:
                try:
                    medline = article.get('MedlineCitation', {})
                    article_data = medline.get('Article', {})

                    title = article_data.get('ArticleTitle', 'No Title')
                    if isinstance(title, dict):
                        title = title.get('#text', 'No Title')

                    abstract_data = article_data.get('Abstract', {}).get('AbstractText', '')
                    if isinstance(abstract_data, list):
                        abstract = ' '.join([a.get('#text', str(a)) if isinstance(a, dict) else str(a) for a in abstract_data])
                    elif isinstance(abstract_data, dict):
                        abstract = abstract_data.get('#text', str(abstract_data))
                    else:
                        abstract = str(abstract_data) if abstract_data else 'No abstract available'

                    author_list = article_data.get('AuthorList', {}).get('Author', [])
                    if isinstance(author_list, dict):
                        author_list = [author_list]
                    authors = []
                    for author in author_list[:5]:
                        if isinstance(author, dict):
                            last = author.get('LastName', '')
                            first = author.get('ForeName', '')
                            if last:
                                authors.append(f"{last} {first}".strip())

                    pmid = medline.get('PMID', {}).get('#text', 'Unknown')
                    pub_date = article_data.get('Journal', {}).get('JournalIssue', {}).get('PubDate', {})
                    year = pub_date.get('Year', 'Unknown')

                    paper = {
                        'source': 'PubMed',
                        'title': title,
                        'authors': authors if authors else ['Unknown'],
                        'abstract': abstract,
                        'pdf_url': None,
                        'pubmed_url': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                        'pmc_url': None,
                        'published': str(year),
                        'id': f"PMID_{pmid}"
                    }
                    papers.append(paper)
                    print(f"   ğŸ“„ {paper['title'][:60]}...")

                except Exception as e:
                    continue

            print(f"   âœ… PubMed: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
            return papers

        except Exception as e:
            print(f"   âŒ PubMed ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []

    def search(self, query: str, source: str = 'both', max_results: int = 5) -> List[Dict]:
        papers = []

        if source in ['arxiv', 'both']:
            papers.extend(self.search_arxiv(query, max_results))

        if source in ['pubmed', 'both']:
            papers.extend(self.search_pubmed(query, max_results))

        self.papers = papers
        return papers


# ==================== íŠ¸ë Œë“œ ë¶„ì„ í´ë˜ìŠ¤ ====================
class TrendAnalyzer:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„"""

    def __init__(self, api_key: str = None, email: str = None, openai_api_key: str = None):
        self.searcher = PaperSearcher(api_key=api_key, email=email)
        self.papers = []
        self.trend_data = {}
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')

    def search_papers_for_trend(self, keyword: str, max_results: int = 50, source: str = 'pubmed') -> List[Dict]:
        """íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ë…¼ë¬¸ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼)"""
        print(f"\nğŸ” '{keyword}' íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...")
        self.papers = self.searcher.search(keyword, source=source, max_results=max_results)
        print(f"   ğŸ“Š ì´ {len(self.papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")
        return self.papers

    def analyze_publication_trend(self) -> Dict:
        """ì—°ë„ë³„ ì¶œíŒ íŠ¸ë Œë“œ ë¶„ì„"""
        from collections import Counter

        if not self.papers:
            return {}

        # ì—°ë„ ì¶”ì¶œ ë° ì§‘ê³„
        years = []
        for paper in self.papers:
            pub_date = paper.get('published', '')
            if pub_date:
                try:
                    year = int(pub_date[:4])
                    if 2000 <= year <= 2030:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„
                        years.append(year)
                except:
                    pass

        year_counts = Counter(years)
        sorted_years = sorted(year_counts.items())

        self.trend_data['year_trend'] = {
            'years': [y[0] for y in sorted_years],
            'counts': [y[1] for y in sorted_years],
            'total': len(years)
        }

        return self.trend_data['year_trend']

    def extract_key_terms(self, top_n: int = 20) -> Dict:
        """ë…¼ë¬¸ ì´ˆë¡ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        from collections import Counter
        import re

        if not self.papers:
            return {}

        # ë¶ˆìš©ì–´ (ì˜ì–´)
        stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
            'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these',
            'those', 'it', 'its', 'they', 'them', 'their', 'we', 'our', 'you',
            'your', 'i', 'me', 'my', 'he', 'she', 'his', 'her', 'which', 'who',
            'whom', 'what', 'when', 'where', 'why', 'how', 'all', 'each', 'every',
            'both', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',
            'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'just',
            'also', 'now', 'here', 'there', 'then', 'once', 'if', 'because',
            'while', 'although', 'though', 'after', 'before', 'since', 'until',
            'about', 'into', 'through', 'during', 'above', 'below', 'between',
            'under', 'again', 'further', 'then', 'once', 'study', 'studies',
            'results', 'conclusion', 'methods', 'background', 'objective',
            'aim', 'purpose', 'introduction', 'discussion', 'however', 'thus',
            'therefore', 'moreover', 'furthermore', 'additionally', 'including',
            'included', 'using', 'used', 'based', 'associated', 'related',
            'compared', 'among', 'within', 'without', 'between', 'across'
        }

        # ëª¨ë“  ì´ˆë¡ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
        all_words = []
        for paper in self.papers:
            abstract = paper.get('abstract', '')
            # ë‹¨ì–´ ì¶”ì¶œ (ì˜ë¬¸ìë§Œ)
            words = re.findall(r'\b[a-zA-Z]{3,}\b', abstract.lower())
            # ë¶ˆìš©ì–´ ì œê±°
            words = [w for w in words if w not in stopwords]
            all_words.extend(words)

        # ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(all_words)
        top_terms = word_counts.most_common(top_n)

        self.trend_data['key_terms'] = {
            'terms': [t[0] for t in top_terms],
            'counts': [t[1] for t in top_terms]
        }

        return self.trend_data['key_terms']

    def identify_emerging_topics(self) -> List[Dict]:
        """ìµœê·¼ ê¸‰ë¶€ìƒí•˜ëŠ” ì£¼ì œ ì‹ë³„"""
        from collections import Counter, defaultdict
        import re

        if not self.papers or len(self.papers) < 5:
            return []

        # ìµœê·¼ 2ë…„ vs ì´ì „ ë…¼ë¬¸ ë¹„êµ
        recent_papers = []
        older_papers = []

        current_year = 2024  # í˜„ì¬ ì—°ë„
        for paper in self.papers:
            pub_date = paper.get('published', '')
            try:
                year = int(pub_date[:4])
                if year >= current_year - 1:
                    recent_papers.append(paper)
                else:
                    older_papers.append(paper)
            except:
                pass

        if not recent_papers or not older_papers:
            return []

        # ë¶ˆìš©ì–´
        stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
            'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'study', 'studies', 'results', 'patients', 'treatment', 'using'
        }

        def extract_terms(papers):
            words = []
            for paper in papers:
                abstract = paper.get('abstract', '')
                w = re.findall(r'\b[a-zA-Z]{4,}\b', abstract.lower())
                words.extend([x for x in w if x not in stopwords])
            return Counter(words)

        recent_terms = extract_terms(recent_papers)
        older_terms = extract_terms(older_papers)

        # ìµœê·¼ì— ê¸‰ì¦í•œ ìš©ì–´ ì°¾ê¸°
        emerging = []
        for term, recent_count in recent_terms.most_common(50):
            older_count = older_terms.get(term, 0)
            if older_count == 0:
                growth = float('inf')
            else:
                growth = (recent_count / len(recent_papers)) / (older_count / len(older_papers))

            if growth > 1.5 and recent_count >= 3:
                emerging.append({
                    'term': term,
                    'recent_count': recent_count,
                    'older_count': older_count,
                    'growth_rate': growth if growth != float('inf') else 999
                })

        # ì„±ì¥ë¥  ìˆœ ì •ë ¬
        emerging.sort(key=lambda x: x['growth_rate'], reverse=True)
        self.trend_data['emerging_topics'] = emerging[:10]

        return self.trend_data['emerging_topics']

    def summarize_research_content(self, keyword: str) -> Dict:
        """ì—°êµ¬ ë‚´ìš© ìš”ì•½ ìƒì„±"""
        if not self.papers:
            return {}

        # 1. ì£¼ìš” ì—°êµ¬ ì£¼ì œ ì¶”ì¶œ (ì œëª© ê¸°ë°˜)
        titles = [p.get('title', '') for p in self.papers[:20]]

        # 2. ì£¼ìš” ì—°êµ¬ ë°©í–¥ íŒŒì•… (ì´ˆë¡ì—ì„œ íŒ¨í„´ ì¶”ì¶œ)
        research_themes = {
            'diagnosis': 0,      # ì§„ë‹¨
            'treatment': 0,      # ì¹˜ë£Œ
            'mechanism': 0,      # ë©”ì»¤ë‹ˆì¦˜
            'prevention': 0,     # ì˜ˆë°©
            'clinical': 0,       # ì„ìƒ
            'review': 0,         # ë¦¬ë·°/ê°œê´€
            'epidemiology': 0,   # ì—­í•™
            'genetics': 0,       # ìœ ì „í•™
        }

        theme_keywords = {
            'diagnosis': ['diagnosis', 'diagnostic', 'detection', 'screening', 'biomarker'],
            'treatment': ['treatment', 'therapy', 'therapeutic', 'drug', 'intervention', 'medication'],
            'mechanism': ['mechanism', 'pathway', 'molecular', 'cellular', 'signaling'],
            'prevention': ['prevention', 'preventive', 'risk factor', 'protective'],
            'clinical': ['clinical', 'trial', 'patient', 'outcome', 'efficacy'],
            'review': ['review', 'overview', 'comprehensive', 'systematic', 'meta-analysis'],
            'epidemiology': ['epidemiology', 'prevalence', 'incidence', 'population'],
            'genetics': ['genetic', 'gene', 'mutation', 'hereditary', 'genomic'],
        }

        for paper in self.papers:
            text = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()
            for theme, keywords in theme_keywords.items():
                for kw in keywords:
                    if kw in text:
                        research_themes[theme] += 1
                        break

        # ìƒìœ„ ì—°êµ¬ ì£¼ì œ
        sorted_themes = sorted(research_themes.items(), key=lambda x: x[1], reverse=True)
        top_themes = [(t, c) for t, c in sorted_themes if c > 0][:5]

        # 3. ëŒ€í‘œ ë…¼ë¬¸ ì„ ì • (ìµœì‹  + ê´€ë ¨ì„±) - 10ê°œ
        representative_papers = self.papers[:10]

        # 4. ì—°êµ¬ ë‚´ìš© ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
        summary_text = self._generate_content_summary(keyword, top_themes, representative_papers)

        self.trend_data['content_summary'] = {
            'themes': top_themes,
            'representative_papers': representative_papers,
            'summary_text': summary_text
        }

        return self.trend_data['content_summary']

    def generate_search_summary(self, keyword: str) -> Dict:
        """ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±"""
        if not self.papers:
            return {}

        # ê¸°ë³¸ í†µê³„
        total_papers = len(self.papers)

        # ì—°ë„ ë²”ìœ„ ê³„ì‚°
        years = []
        for paper in self.papers:
            pub_date = paper.get('published', '')
            if pub_date:
                try:
                    year = int(pub_date[:4])
                    years.append(year)
                except:
                    pass

        year_range = f"{min(years)}-{max(years)}" if years else "N/A"

        # ì†ŒìŠ¤ë³„ ë¶„ë¥˜
        sources = {}
        for paper in self.papers:
            src = paper.get('source', 'Unknown')
            sources[src] = sources.get(src, 0) + 1

        # AIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
        search_summary_text = self._generate_search_overview(keyword)

        self.trend_data['search_summary'] = {
            'total_papers': total_papers,
            'year_range': year_range,
            'sources': sources,
            'summary_text': search_summary_text
        }

        return self.trend_data['search_summary']

    def _generate_search_overview(self, keyword: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ê°œìš” ìš”ì•½ ìƒì„±"""
        if not self.openai_api_key or len(self.papers) < 2:
            return self._generate_basic_search_overview(keyword)

        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.openai_api_key)

            print("   ğŸ” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„± ì¤‘...")

            # ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ ìˆ˜ì§‘
            paper_info = []
            for paper in self.papers[:15]:  # ìµœëŒ€ 15ê°œ ë…¼ë¬¸ ë¶„ì„
                title = paper.get('title', '')
                abstract = paper.get('abstract', '')[:300]
                paper_info.append(f"ì œëª©: {title}\nì´ˆë¡: {abstract}")

            combined_info = "\n\n".join(paper_info)

            prompt = f"""ë‹¤ìŒì€ '{keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ëœ {len(self.papers)}ê°œì˜ í•™ìˆ  ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤.

{combined_info}

ìœ„ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:

ğŸ” **ê²€ìƒ‰ ê²°ê³¼ ê°œìš”**:
('{keyword}'ì— ëŒ€í•´ ì–´ë–¤ ë…¼ë¬¸ë“¤ì´ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…)

ğŸ“‘ **ì£¼ìš” ì—°êµ¬ ë‚´ìš©**:
- (ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì´ ë‹¤ë£¨ëŠ” í•µì‹¬ ì£¼ì œ 3-4ê°€ì§€ë¥¼ bullet pointë¡œ)

ğŸ¯ **ì—°êµ¬ ì´ˆì **:
(ì´ ë¶„ì•¼ ì—°êµ¬ìë“¤ì´ ì£¼ë¡œ ê´€ì‹¬ì„ ê°€ì§€ëŠ” ë¬¸ì œë‚˜ ì§ˆë¬¸ 1-2ë¬¸ì¥)

í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )

            summary = response.choices[0].message.content.strip()
            print("   âœ… ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì™„ë£Œ!")
            return summary

        except Exception as e:
            print(f"   âš ï¸ AI ìš”ì•½ ì‹¤íŒ¨: {str(e)[:50]}")
            return self._generate_basic_search_overview(keyword)

    def _generate_basic_search_overview(self, keyword: str) -> str:
        """ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ê°œìš” ìƒì„±"""
        if not self.papers:
            return ""

        # ì œëª©ì—ì„œ ê³µí†µ ì£¼ì œ ì¶”ì¶œ
        title_words = []
        for paper in self.papers[:10]:
            title = paper.get('title', '').lower()
            words = [w for w in title.split() if len(w) > 4 and w != keyword.lower()]
            title_words.extend(words)

        from collections import Counter
        common_words = Counter(title_words).most_common(5)
        common_topics = [w[0] for w in common_words]

        summary = f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼, ì´ {len(self.papers)}ê°œì˜ ë…¼ë¬¸ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        if common_topics:
            summary += f"ì£¼ìš” ê´€ë ¨ ì£¼ì œ: {', '.join(common_topics[:3])}"

        return summary

    def _generate_content_summary(self, keyword: str, themes: List, papers: List) -> str:
        """ì—°êµ¬ ë‚´ìš© ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±"""
        # OpenAI APIê°€ ìˆìœ¼ë©´ AI ìš”ì•½ ì‚¬ìš©
        if self.openai_api_key and len(self.papers) >= 3:
            ai_summary = self._generate_ai_summary(keyword)
            if ai_summary:
                return ai_summary

        # ê¸°ë³¸ ìš”ì•½ ìƒì„±
        summary_parts = []

        # ì£¼ìš” ì—°êµ¬ ë¶„ì•¼
        if themes:
            theme_names = {
                'diagnosis': 'ì§„ë‹¨/ê²€ì¶œ',
                'treatment': 'ì¹˜ë£Œ/ì•½ë¬¼',
                'mechanism': 'ë©”ì»¤ë‹ˆì¦˜/ê²½ë¡œ',
                'prevention': 'ì˜ˆë°©/ìœ„í—˜ìš”ì¸',
                'clinical': 'ì„ìƒì—°êµ¬',
                'review': 'ì¢…í•©ë¦¬ë·°',
                'epidemiology': 'ì—­í•™ì—°êµ¬',
                'genetics': 'ìœ ì „í•™ì—°êµ¬'
            }
            main_themes = [theme_names.get(t[0], t[0]) for t in themes[:3]]
            summary_parts.append(f"'{keyword}' ì—°êµ¬ëŠ” ì£¼ë¡œ {', '.join(main_themes)} ë¶„ì•¼ì— ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ìµœì‹  ì—°êµ¬ ë™í–¥
        if papers:
            recent_titles = [p.get('title', '')[:80] for p in papers[:3]]
            summary_parts.append(f"\nìµœê·¼ ì£¼ìš” ì—°êµ¬: ")
            for i, title in enumerate(recent_titles, 1):
                summary_parts.append(f"  {i}. {title}...")

        return "\n".join(summary_parts)

    def _generate_ai_summary(self, keyword: str) -> Optional[str]:
        """OpenAIë¥¼ ì‚¬ìš©í•œ AI ìš”ì•½ ìƒì„±"""
        if not self.openai_api_key:
            return None

        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.openai_api_key)

            print("   ğŸ¤– OpenAI GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ ë‚´ìš© ë¶„ì„ ì¤‘...")

            # ë…¼ë¬¸ ì´ˆë¡ ìˆ˜ì§‘ (ìµœëŒ€ 10ê°œë¡œ í™•ëŒ€)
            abstracts = []
            for paper in self.papers[:10]:
                title = paper.get('title', '')
                abstract = paper.get('abstract', '')[:600]
                year = paper.get('published', '')[:4]
                authors = paper.get('authors', '')[:50] if paper.get('authors') else ''
                abstracts.append(f"[{year}] {title}\nì €ì: {authors}\nì´ˆë¡: {abstract}")

            combined_text = "\n\n---\n\n".join(abstracts)

            # ì—°ë„ë³„ í†µê³„ ì •ë³´ ì¶”ê°€
            year_stats = ""
            if 'year_trend' in self.trend_data:
                years = self.trend_data['year_trend']['years'][-5:]
                counts = self.trend_data['year_trend']['counts'][-5:]
                year_stats = f"\n\nì—°ë„ë³„ ë…¼ë¬¸ ìˆ˜: " + ", ".join([f"{y}ë…„: {c}í¸" for y, c in zip(years, counts)])

            prompt = f"""ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ì—°êµ¬ ë™í–¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ '{keyword}'ì— ê´€í•œ ìµœê·¼ ì—°êµ¬ ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤.

ì´ ë¶„ì„ ë…¼ë¬¸ ìˆ˜: {len(self.papers)}í¸{year_stats}

=== ë…¼ë¬¸ ëª©ë¡ ===
{combined_text}

ìœ„ ì—°êµ¬ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ '{keyword}' ë¶„ì•¼ì˜ ì—°êµ¬ ë™í–¥ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ğŸ“Œ **ì—°êµ¬ ê°œìš”**: (ì´ ë¶„ì•¼ê°€ ë¬´ì—‡ì„ ë‹¤ë£¨ëŠ”ì§€ 1-2ë¬¸ì¥)

ğŸ”¬ **ì£¼ìš” ì—°êµ¬ ë°©í–¥**:
- (ìµœê·¼ ì—°êµ¬ë“¤ì´ ì§‘ì¤‘í•˜ëŠ” í•µì‹¬ ì£¼ì œ 2-3ê°€ì§€)

ğŸ’¡ **í•µì‹¬ ë°œê²¬/ê²°ê³¼**:
- (ì£¼ìš” ì—°êµ¬ ê²°ê³¼ë‚˜ ë°œê²¬ 2-3ê°€ì§€)

ğŸ”® **í–¥í›„ ì „ë§**:
- (ì—°êµ¬ íŠ¸ë Œë“œ ë° í–¥í›„ ë°©í–¥ 1-2ë¬¸ì¥)

í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì „ë¬¸ ìš©ì–´ëŠ” í•„ìš”ì‹œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."""

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ì—°êµ¬ ë™í–¥ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³µì¡í•œ ì—°êµ¬ ë‚´ìš©ì„ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.3
            )

            summary = response.choices[0].message.content.strip()
            print("   âœ… AI ìš”ì•½ ìƒì„± ì™„ë£Œ!")
            return summary

        except Exception as e:
            print(f"   âš ï¸ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)[:80]}")
            print("   ğŸ“ ê¸°ë³¸ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            return None

    def visualize_trends(self, keyword: str, save_path: str = None) -> str:
        """íŠ¸ë Œë“œ ì‹œê°í™”"""
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt

        try:
            plt.rcParams['font.family'] = 'AppleGothic'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'ğŸ“ˆ Research Trend Analysis: "{keyword}"',
                    fontsize=14, fontweight='bold')

        # 1. ì—°ë„ë³„ ì¶œíŒ íŠ¸ë Œë“œ
        ax1 = axes[0, 0]
        if 'year_trend' in self.trend_data:
            years = self.trend_data['year_trend']['years']
            counts = self.trend_data['year_trend']['counts']
            colors = plt.cm.Blues([0.3 + 0.7 * i / len(years) for i in range(len(years))])
            bars = ax1.bar(years, counts, color=colors, edgecolor='navy', alpha=0.8)
            ax1.set_xlabel('Year', fontweight='bold')
            ax1.set_ylabel('Number of Publications', fontweight='bold')
            ax1.set_title('ğŸ“… Publication Trend by Year', fontweight='bold')
            ax1.set_xticks(years)
            ax1.tick_params(axis='x', rotation=45)
            for bar, count in zip(bars, counts):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        str(count), ha='center', va='bottom', fontsize=9)

        # 2. í•µì‹¬ í‚¤ì›Œë“œ
        ax2 = axes[0, 1]
        if 'key_terms' in self.trend_data:
            terms = self.trend_data['key_terms']['terms'][:10]
            term_counts = self.trend_data['key_terms']['counts'][:10]
            colors = plt.cm.Greens([0.3 + 0.7 * i / len(terms) for i in range(len(terms))])
            bars = ax2.barh(terms[::-1], term_counts[::-1], color=colors[::-1], edgecolor='darkgreen', alpha=0.8)
            ax2.set_xlabel('Frequency', fontweight='bold')
            ax2.set_title('ğŸ”‘ Top Keywords in Abstracts', fontweight='bold')
            for bar, count in zip(bars, term_counts[::-1]):
                ax2.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
                        str(count), va='center', fontsize=9)

        # 3. ê¸‰ë¶€ìƒ ì£¼ì œ
        ax3 = axes[1, 0]
        if 'emerging_topics' in self.trend_data and self.trend_data['emerging_topics']:
            emerging = self.trend_data['emerging_topics'][:8]
            e_terms = [e['term'] for e in emerging]
            e_growth = [min(e['growth_rate'], 10) for e in emerging]  # ìµœëŒ€ 10ìœ¼ë¡œ ì œí•œ
            colors = plt.cm.Reds([0.3 + 0.7 * i / len(e_terms) for i in range(len(e_terms))])
            bars = ax3.barh(e_terms[::-1], e_growth[::-1], color=colors[::-1], edgecolor='darkred', alpha=0.8)
            ax3.set_xlabel('Growth Rate', fontweight='bold')
            ax3.set_title('ğŸš€ Emerging Topics (Recent vs Older)', fontweight='bold')
            for bar, growth in zip(bars, e_growth[::-1]):
                ax3.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                        f'{growth:.1f}x', va='center', fontsize=9)
        else:
            ax3.text(0.5, 0.5, 'Not enough data\nfor trend analysis',
                    ha='center', va='center', fontsize=12, transform=ax3.transAxes)
            ax3.set_title('ğŸš€ Emerging Topics', fontweight='bold')

        # 4. ìš”ì•½ ì •ë³´
        ax4 = axes[1, 1]
        ax4.axis('off')

        summary = f"ğŸ“Š Trend Analysis Summary\n{'='*40}\n\n"
        summary += f"ğŸ” Keyword: {keyword}\n"
        summary += f"ğŸ“„ Total Papers: {len(self.papers)}\n\n"

        if 'year_trend' in self.trend_data:
            years = self.trend_data['year_trend']['years']
            counts = self.trend_data['year_trend']['counts']
            if years:
                summary += f"ğŸ“… Year Range: {min(years)} - {max(years)}\n"
                max_idx = counts.index(max(counts))
                summary += f"ğŸ“ˆ Peak Year: {years[max_idx]} ({counts[max_idx]} papers)\n\n"

        if 'key_terms' in self.trend_data:
            top_5 = self.trend_data['key_terms']['terms'][:5]
            summary += f"ğŸ”‘ Top Keywords:\n"
            for i, term in enumerate(top_5, 1):
                summary += f"   {i}. {term}\n"

        if 'emerging_topics' in self.trend_data and self.trend_data['emerging_topics']:
            summary += f"\nğŸš€ Emerging Topics:\n"
            for i, e in enumerate(self.trend_data['emerging_topics'][:3], 1):
                summary += f"   {i}. {e['term']} ({e['growth_rate']:.1f}x growth)\n"

        ax4.text(0.05, 0.95, summary, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow',
                         alpha=0.9, edgecolor='orange'))

        plt.tight_layout()

        if save_path is None:
            save_path = f"trend_analysis_{keyword.replace(' ', '_')[:20]}.png"

        plt.savefig(save_path, dpi=100, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close(fig)

        print(f"   ğŸ“Š íŠ¸ë Œë“œ ì‹œê°í™” ì €ì¥: {save_path}")
        return save_path

    def generate_trend_report(self, keyword: str) -> str:
        """íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("\n" + "=" * 60)
        report.append(f"ğŸ“ˆ '{keyword}' ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 60)

        report.append(f"\nğŸ“Š ë¶„ì„ ëŒ€ìƒ: {len(self.papers)}ê°œ ë…¼ë¬¸\n")

        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        if 'search_summary' in self.trend_data and self.trend_data['search_summary']:
            search_data = self.trend_data['search_summary']
            report.append("-" * 60)
            report.append("ğŸ” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
            report.append("-" * 60)

            # ê¸°ë³¸ í†µê³„
            report.append(f"\n   ğŸ“š ê²€ìƒ‰ëœ ë…¼ë¬¸: {search_data.get('total_papers', 0)}ê°œ")
            report.append(f"   ğŸ“… ì—°ë„ ë²”ìœ„: {search_data.get('year_range', 'N/A')}")

            # ì†ŒìŠ¤ë³„ ë¶„ë¥˜
            sources = search_data.get('sources', {})
            if sources:
                source_str = ", ".join([f"{k}: {v}ê°œ" for k, v in sources.items()])
                report.append(f"   ğŸ“– ì¶œì²˜: {source_str}")

            # ìš”ì•½ í…ìŠ¤íŠ¸
            summary_text = search_data.get('summary_text', '')
            if summary_text:
                report.append("\n" + "-" * 40)
                summary_lines = summary_text.split('\n')
                for line in summary_lines:
                    if line.strip():
                        report.append(f"   {line}")
            report.append("")

        # ì—°ë„ë³„ íŠ¸ë Œë“œ
        if 'year_trend' in self.trend_data:
            years = self.trend_data['year_trend']['years']
            counts = self.trend_data['year_trend']['counts']
            if years:
                report.append("ğŸ“… ì—°ë„ë³„ ì¶œíŒ ë™í–¥:")
                for year, count in zip(years[-5:], counts[-5:]):  # ìµœê·¼ 5ë…„
                    bar = "â–ˆ" * (count * 2)
                    report.append(f"   {year}: {bar} ({count})")

                # íŠ¸ë Œë“œ ë¶„ì„
                if len(counts) >= 2:
                    recent_avg = sum(counts[-2:]) / 2
                    older_avg = sum(counts[:-2]) / max(len(counts) - 2, 1) if len(counts) > 2 else counts[0]
                    if recent_avg > older_avg * 1.2:
                        report.append(f"\n   ğŸ“ˆ íŠ¸ë Œë“œ: ìƒìŠ¹ì„¸ (ìµœê·¼ ì—°êµ¬ ì¦ê°€)")
                    elif recent_avg < older_avg * 0.8:
                        report.append(f"\n   ğŸ“‰ íŠ¸ë Œë“œ: í•˜ë½ì„¸ (ì—°êµ¬ ê´€ì‹¬ ê°ì†Œ)")
                    else:
                        report.append(f"\n   â¡ï¸ íŠ¸ë Œë“œ: ì•ˆì •ì  (ê¾¸ì¤€í•œ ì—°êµ¬)")

        # í•µì‹¬ í‚¤ì›Œë“œ
        if 'key_terms' in self.trend_data:
            report.append("\nğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ (Top 10):")
            terms = self.trend_data['key_terms']['terms'][:10]
            counts = self.trend_data['key_terms']['counts'][:10]
            for i, (term, count) in enumerate(zip(terms, counts), 1):
                report.append(f"   {i:2d}. {term} ({count}íšŒ)")

        # ê¸‰ë¶€ìƒ ì£¼ì œ
        if 'emerging_topics' in self.trend_data and self.trend_data['emerging_topics']:
            report.append("\nğŸš€ ê¸‰ë¶€ìƒ ì£¼ì œ (ìµœê·¼ vs ê³¼ê±°):")
            for i, e in enumerate(self.trend_data['emerging_topics'][:5], 1):
                growth = e['growth_rate']
                if growth == 999:
                    growth_str = "NEW!"
                else:
                    growth_str = f"{growth:.1f}x"
                report.append(f"   {i}. {e['term']} - {growth_str} ì„±ì¥")

        # ì—°êµ¬ ë‚´ìš© ìš”ì•½
        if 'content_summary' in self.trend_data and self.trend_data['content_summary']:
            summary_data = self.trend_data['content_summary']
            report.append("\n" + "-" * 60)
            report.append("ğŸ“‹ ì—°êµ¬ ë‚´ìš© ìš”ì•½")
            report.append("-" * 60)

            # ì£¼ìš” ì—°êµ¬ ì£¼ì œ
            if 'themes' in summary_data and summary_data['themes']:
                theme_names = {
                    'diagnosis': 'ì§„ë‹¨/ê²€ì¶œ',
                    'treatment': 'ì¹˜ë£Œ/ì•½ë¬¼',
                    'mechanism': 'ë©”ì»¤ë‹ˆì¦˜/ê²½ë¡œ',
                    'prevention': 'ì˜ˆë°©/ìœ„í—˜ìš”ì¸',
                    'clinical': 'ì„ìƒì—°êµ¬',
                    'review': 'ì¢…í•©ë¦¬ë·°',
                    'epidemiology': 'ì—­í•™ì—°êµ¬',
                    'genetics': 'ìœ ì „í•™ì—°êµ¬'
                }
                report.append("\nğŸ”¬ ì£¼ìš” ì—°êµ¬ ë¶„ì•¼:")
                for theme, count in summary_data['themes'][:5]:
                    theme_name = theme_names.get(theme, theme)
                    bar = "â–“" * min(count, 20)
                    report.append(f"   â€¢ {theme_name}: {bar} ({count}í¸)")

            # ìš”ì•½ í…ìŠ¤íŠ¸
            if 'summary_text' in summary_data and summary_data['summary_text']:
                report.append("\nğŸ“ ì—°êµ¬ ë™í–¥ ìš”ì•½:")
                summary_lines = summary_data['summary_text'].split('\n')
                for line in summary_lines:
                    if line.strip():
                        report.append(f"   {line}")

            # ëŒ€í‘œ ë…¼ë¬¸ (ìµœê·¼ 10ê°œ)
            if 'representative_papers' in summary_data and summary_data['representative_papers']:
                report.append("\nğŸ“š ëŒ€í‘œ ë…¼ë¬¸ (ìµœê·¼ 10ê°œ):")
                report.append("-" * 50)
                for i, paper in enumerate(summary_data['representative_papers'][:10], 1):
                    title = paper.get('title', '')[:65]
                    year = paper.get('published', paper.get('year', 'N/A'))[:4] if paper.get('published') or paper.get('year') else 'N/A'
                    authors = paper.get('authors', [])
                    if isinstance(authors, list):
                        author_str = ', '.join(authors[:2])
                        if len(authors) > 2:
                            author_str += ' et al.'
                    else:
                        author_str = str(authors)[:40]
                    source = paper.get('source', '')

                    report.append(f"\n   [{i:2d}] {title}...")
                    report.append(f"       ğŸ“… {year} | ğŸ‘¤ {author_str}")
                    if source:
                        report.append(f"       ğŸ“– {source}")

        report.append("\n" + "=" * 60)

        return "\n".join(report)

    def analyze(self, keyword: str, max_results: int = 50, source: str = 'pubmed') -> str:
        """ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰"""
        # 1. ë…¼ë¬¸ ê²€ìƒ‰
        self.search_papers_for_trend(keyword, max_results, source)

        if not self.papers:
            return f"âŒ '{keyword}'ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 2. ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
        print("\nğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        self.generate_search_summary(keyword)

        # 3. ë¶„ì„ ìˆ˜í–‰
        print("ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        self.analyze_publication_trend()
        self.extract_key_terms()
        self.identify_emerging_topics()

        # 4. ì—°êµ¬ ë‚´ìš© ìš”ì•½ ìƒì„±
        print("ğŸ“ ì—°êµ¬ ë‚´ìš© ìš”ì•½ ìƒì„± ì¤‘...")
        self.summarize_research_content(keyword)

        # 5. ì‹œê°í™”
        print("ğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
        self.visualize_trends(keyword)

        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_trend_report(keyword)
        print(report)

        return report


# ==================== PDFDownloader í´ë˜ìŠ¤ ====================
class PDFDownloader:
    def __init__(self, save_dir: str = PAPERS_DIR):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def download(self, paper: Dict, show_progress: bool = True) -> Optional[str]:
        safe_title = re.sub(r'[^\w\s-]', '', paper['title'])[:50]
        filename = f"{paper['id']}_{safe_title}.pdf"
        filepath = os.path.join(self.save_dir, filename)
        txt_filepath = filepath.replace('.pdf', '.txt')

        if os.path.exists(txt_filepath):
            return txt_filepath
        if os.path.exists(filepath):
            return filepath

        pdf_url = paper.get('pdf_url') or paper.get('pmc_url')

        if not pdf_url:
            if paper['source'] == 'PubMed':
                return self._save_abstract_as_text(paper, filename)
            return None

        try:
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; ResearchBot/1.0)'}
            response = requests.get(pdf_url, headers=headers, timeout=60, stream=True)
            response.raise_for_status()

            # Content-Type í™•ì¸
            content_type = response.headers.get('content-type', '')
            if 'html' in content_type or 'text' in content_type:
                return self._save_abstract_as_text(paper, filename)

            # íŒŒì¼ í¬ê¸° í™•ì¸
            total_size = int(response.headers.get('content-length', 0))

            # ì²˜ìŒ ëª‡ ë°”ì´íŠ¸ë¡œ HTML ì—¬ë¶€ í™•ì¸
            first_chunk = next(response.iter_content(chunk_size=5), b'')
            if first_chunk.startswith(b'<html') or first_chunk.startswith(b'<!DOC'):
                return self._save_abstract_as_text(paper, filename)

            # ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ with ì§„í–‰ ë°”
            with open(filepath, 'wb') as f:
                f.write(first_chunk)  # ì²« ì²­í¬ ê¸°ë¡

                if show_progress and total_size > 0:
                    # ì§„í–‰ ë°” í‘œì‹œ
                    with tqdm(
                        total=total_size,
                        initial=len(first_chunk),
                        unit='B',
                        unit_scale=True,
                        unit_divisor=1024,
                        desc=f"   ğŸ“¥ {filename[:30]}",
                        bar_format='{desc}: {percentage:3.0f}%|{bar:20}| {n_fmt}/{total_fmt} [{rate_fmt}]',
                        leave=False
                    ) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
                else:
                    # ì§„í–‰ ë°” ì—†ì´ ë‹¤ìš´ë¡œë“œ
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)

            return filepath

        except Exception as e:
            return self._save_abstract_as_text(paper, filename)

    def _save_abstract_as_text(self, paper: Dict, filename: str) -> Optional[str]:
        txt_filename = filename.replace('.pdf', '.txt')
        filepath = os.path.join(self.save_dir, txt_filename)

        content = f"""Title: {paper['title']}

Authors: {', '.join(paper['authors'])}

Source: {paper['source']}

Published: {paper['published']}

Abstract:
{paper['abstract']}

URL: {paper.get('pubmed_url', paper.get('pdf_url', 'N/A'))}
"""

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"   ğŸ“ ì´ˆë¡ ì €ì¥: {txt_filename[:40]}...")
        return filepath

    def download_all(self, papers: List[Dict]) -> List[str]:
        total = len(papers)
        if total == 0:
            print("   âš ï¸ ë‹¤ìš´ë¡œë“œí•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print("\n" + "=" * 60)
        print("ğŸ“¥ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ")
        print("=" * 60)
        print(f"   ì´ {total}ê°œ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì˜ˆì •\n")

        downloaded = []
        skipped = 0
        pdf_count = 0
        abstract_count = 0
        failed = 0

        # ì „ì²´ ì§„í–‰ ìƒí™© ë°”
        with tqdm(
            total=total,
            desc="   ğŸ“š ì „ì²´ ì§„í–‰",
            bar_format='{desc}: {percentage:3.0f}%|{bar:30}| {n}/{total} [{elapsed}<{remaining}]',
            colour='green'
        ) as pbar:
            for i, paper in enumerate(papers):
                title = paper.get('title', 'Unknown')[:35]

                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                safe_title = re.sub(r'[^\w\s-]', '', paper['title'])[:50]
                filename = f"{paper['id']}_{safe_title}.pdf"
                filepath = os.path.join(self.save_dir, filename)
                txt_filepath = filepath.replace('.pdf', '.txt')

                if os.path.exists(txt_filepath) or os.path.exists(filepath):
                    skipped += 1
                    pbar.set_postfix_str(f"â­ï¸ ìŠ¤í‚µ: {title}...")
                    downloaded.append(txt_filepath if os.path.exists(txt_filepath) else filepath)
                else:
                    pbar.set_postfix_str(f"ğŸ“¥ {title}...")
                    result_path = self.download(paper, show_progress=False)

                    if result_path:
                        downloaded.append(result_path)
                        if result_path.endswith('.pdf'):
                            pdf_count += 1
                        else:
                            abstract_count += 1
                    else:
                        failed += 1

                pbar.update(1)
                time.sleep(0.2)

        # ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½
        print("\n" + "-" * 60)
        print("ğŸ“Š ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½")
        print("-" * 60)
        print(f"   âœ… ì´ ë‹¤ìš´ë¡œë“œ: {len(downloaded)}ê°œ")
        print(f"      ğŸ“„ PDF íŒŒì¼: {pdf_count}ê°œ")
        print(f"      ğŸ“ ì´ˆë¡ ì €ì¥: {abstract_count}ê°œ")
        print(f"      â­ï¸ ê¸°ì¡´ íŒŒì¼: {skipped}ê°œ")
        if failed > 0:
            print(f"      âŒ ì‹¤íŒ¨: {failed}ê°œ")
        print("-" * 60)

        return downloaded


# ==================== TextExtractor í´ë˜ìŠ¤ ====================
class TextExtractor:
    @staticmethod
    def extract(filepath: str) -> str:
        if filepath.endswith('.txt'):
            return TextExtractor._extract_from_txt(filepath)
        elif filepath.endswith('.pdf'):
            return TextExtractor._extract_from_pdf(filepath)
        return ""

    @staticmethod
    def _extract_from_txt(filepath: str) -> str:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            return ""

    @staticmethod
    def _extract_from_pdf(filepath: str) -> str:
        text = ""
        try:
            reader = PdfReader(filepath)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"

            if len(text) < 100:
                text = ""
                with pdfplumber.open(filepath) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
        except Exception as e:
            print(f"      âš ï¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)[:50]}")

        return text.strip()

    @staticmethod
    def extract_all(filepaths: List[str]) -> List[Dict]:
        print("\nğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\n")

        documents = []
        for filepath in filepaths:
            filename = os.path.basename(filepath)
            print(f"   ğŸ“– {filename[:50]}...")

            text = TextExtractor.extract(filepath)
            if text:
                documents.append({
                    'text': text,
                    'source': filename,
                    'filepath': filepath
                })
                print(f"      âœ… {len(text):,} ê¸€ì ì¶”ì¶œ")
            else:
                print(f"      âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ")

        print(f"\nğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
        return documents


# ==================== ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤ (sentence-transformers ì—†ì´) ====================
from langchain_core.embeddings import Embeddings

os.environ["SAFETENSORS_FAST_GPU"] = "1"


class HuggingFaceEmbeddings(Embeddings):
    """HuggingFace Transformersë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© í´ë˜ìŠ¤ (sentence-transformers ë¶ˆí•„ìš”)"""

    def __init__(self, model_name: str, device: str = 'cpu'):
        import torch
        from transformers import AutoTokenizer, AutoModel

        self.device = device
        self.model_name = model_name

        print(f"   ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()

    def _mean_pooling(self, model_output, attention_mask):
        """Mean pooling - attention maskë¥¼ ê³ ë ¤í•œ í‰ê· """
        import torch
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def _encode(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        import torch
        import torch.nn.functional as F

        # í† í°í™”
        encoded_input = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            model_output = self.model(**encoded_input)

        # Mean pooling
        embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])

        # ì •ê·œí™”
        embeddings = F.normalize(embeddings, p=2, dim=1)

        return embeddings.cpu().numpy().tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”©"""
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = 8
        all_embeddings = []
        total_batches = (len(texts) + batch_size - 1) // batch_size

        if len(texts) > batch_size:
            # tqdm ì§„í–‰ ë°” ì‚¬ìš©
            with tqdm(
                total=len(texts),
                desc="   ğŸ§  ì„ë² ë”© ìƒì„±",
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}, {rate_fmt}]',
                unit="ë¬¸ì„œ",
                colour='cyan'
            ) as pbar:
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i+batch_size]
                    embeddings = self._encode(batch)
                    all_embeddings.extend(embeddings)
                    pbar.update(len(batch))
        else:
            # ì ì€ ìˆ˜ì˜ ë¬¸ì„œëŠ” ì§„í–‰ ë°” ì—†ì´ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                embeddings = self._encode(batch)
                all_embeddings.extend(embeddings)

        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        return self._encode([text])[0]


class OpenAIEmbeddings(Embeddings):
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© í´ë˜ìŠ¤"""

    def __init__(self, api_key: str, model: str = "text-embedding-3-small"):
        from openai import OpenAI
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.dimension = 1536 if "small" in model else 3072

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”©"""
        all_embeddings = []
        batch_size = 100  # OpenAI ë°°ì¹˜ ì œí•œ

        if len(texts) > batch_size:
            # tqdm ì§„í–‰ ë°” ì‚¬ìš©
            with tqdm(
                total=len(texts),
                desc="   ğŸ§  OpenAI ì„ë² ë”©",
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}, {rate_fmt}]',
                unit="ë¬¸ì„œ",
                colour='green'
            ) as pbar:
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i+batch_size]
                    response = self.client.embeddings.create(
                        model=self.model,
                        input=batch
                    )
                    batch_embeddings = [item.embedding for item in response.data]
                    all_embeddings.extend(batch_embeddings)
                    pbar.update(len(batch))
        else:
            # ì ì€ ìˆ˜ì˜ ë¬¸ì„œëŠ” ì§„í–‰ ë°” ì—†ì´ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                response = self.client.embeddings.create(
                    model=self.model,
                    input=batch
                )
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)

        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        response = self.client.embeddings.create(
            model=self.model,
            input=[text]
        )
        return response.data[0].embedding


# ==================== Reranker í´ë˜ìŠ¤ ====================
class Reranker:
    """Cross-Encoder ê¸°ë°˜ Reranking ëª¨ë¸"""

    MODELS = {
        'ms-marco': {
            'name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
            'description': 'MS MARCO í•™ìŠµ (ë¹ ë¦„, ì¼ë°˜ìš©)',
        },
        'ms-marco-large': {
            'name': 'cross-encoder/ms-marco-MiniLM-L-12-v2',
            'description': 'MS MARCO í•™ìŠµ (ì •í™•, ì¼ë°˜ìš©)',
        },
        'bge-reranker': {
            'name': 'BAAI/bge-reranker-base',
            'description': 'BGE Reranker (ë‹¤êµ­ì–´ ì§€ì›)',
        },
        'bge-reranker-large': {
            'name': 'BAAI/bge-reranker-large',
            'description': 'BGE Reranker Large (ê³ ì •í™•ë„)',
        },
    }

    def __init__(self, model_type: str = 'ms-marco', device: str = None):
        """
        Reranker ì´ˆê¸°í™”

        Args:
            model_type: ëª¨ë¸ íƒ€ì… ('ms-marco', 'ms-marco-large', 'bge-reranker', 'bge-reranker-large')
            device: 'cuda', 'mps', 'cpu' ë˜ëŠ” None (ìë™ ê°ì§€)
        """
        import torch
        from transformers import AutoModelForSequenceClassification, AutoTokenizer

        if device is None:
            if torch.cuda.is_available():
                device = 'cuda'
            elif torch.backends.mps.is_available():
                device = 'mps'
            else:
                device = 'cpu'

        self.device = device
        self.model_type = model_type

        model_info = self.MODELS.get(model_type, self.MODELS['ms-marco'])
        model_name = model_info['name']

        print(f"   ğŸ”„ Reranker ëª¨ë¸ ë¡œë”©: {model_name}")
        print(f"   ğŸ“ Device: {device}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()

        print(f"   âœ… Reranker ë¡œë“œ ì™„ë£Œ!")

    def compute_score(self, query: str, document: str) -> float:
        """ë‹¨ì¼ ì¿¼ë¦¬-ë¬¸ì„œ ìŒì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        import torch

        inputs = self.tokenizer(
            query, document,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model(**inputs)
            # Sigmoidë¥¼ ì ìš©í•˜ì—¬ 0-1 ë²”ìœ„ë¡œ ë³€í™˜
            score = torch.sigmoid(outputs.logits).squeeze().item()

        return score

    def rerank(
        self,
        query: str,
        documents: List[Dict],
        top_k: int = None,
        content_key: str = 'content',
        show_progress: bool = True
    ) -> List[Dict]:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ Reranking

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ê° ë¬¸ì„œëŠ” dict)
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ë°˜í™˜)
            content_key: ë¬¸ì„œ dictì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ í‚¤
            show_progress: ì§„í–‰ ë°” í‘œì‹œ ì—¬ë¶€

        Returns:
            rerank_scoreê°€ ì¶”ê°€ëœ ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        import torch

        if not documents:
            return []

        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
        pairs = [(query, doc.get(content_key, '')) for doc in documents]
        scores = []

        batch_size = 8

        if show_progress and len(documents) > batch_size:
            iterator = tqdm(
                range(0, len(pairs), batch_size),
                desc="   ğŸ¯ Reranking",
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
                colour='yellow',
                total=(len(pairs) + batch_size - 1) // batch_size
            )
        else:
            iterator = range(0, len(pairs), batch_size)

        for i in iterator:
            batch_pairs = pairs[i:i+batch_size]

            inputs = self.tokenizer(
                [p[0] for p in batch_pairs],
                [p[1] for p in batch_pairs],
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                batch_scores = torch.sigmoid(outputs.logits).squeeze(-1)

                if batch_scores.dim() == 0:
                    scores.append(batch_scores.item())
                else:
                    scores.extend(batch_scores.cpu().tolist())

        # ê²°ê³¼ì— rerank_score ì¶”ê°€
        reranked_docs = []
        for doc, score in zip(documents, scores):
            doc_copy = doc.copy()
            doc_copy['rerank_score'] = score
            doc_copy['original_score'] = doc.get('hybrid_score', doc.get('score', 0))
            reranked_docs.append(doc_copy)

        # rerank_scoreë¡œ ì •ë ¬
        reranked_docs.sort(key=lambda x: x['rerank_score'], reverse=True)

        if top_k:
            return reranked_docs[:top_k]
        return reranked_docs


# ==================== EmbeddingModelFactory í´ë˜ìŠ¤ ====================
class EmbeddingModelFactory:
    """ì„ë² ë”© ëª¨ë¸ íŒ©í† ë¦¬ - HuggingFace ë˜ëŠ” OpenAI ì„ íƒ ê°€ëŠ¥"""

    MODELS = {
        # PubMed/ì˜í•™ íŠ¹í™” ëª¨ë¸
        'pubmedbert': {
            'name': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',
            'description': 'PubMed ë…¼ë¬¸ íŠ¹í™” (PubMedBERT Full)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'pubmedbert-abs': {
            'name': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',
            'description': 'PubMed ì´ˆë¡ íŠ¹í™” (PubMedBERT Abstract)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'biobert': {
            'name': 'dmis-lab/biobert-base-cased-v1.2',
            'description': 'ì˜í•™/ìƒë¬¼í•™ íŠ¹í™” (BioBERT v1.2)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'scibert': {
            'name': 'allenai/scibert_scivocab_uncased',
            'description': 'ê³¼í•™ ë…¼ë¬¸ íŠ¹í™” (SciBERT)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'biolinkbert': {
            'name': 'michiyasunaga/BioLinkBERT-base',
            'description': 'ì˜í•™ ë¬¸í—Œ ë§í¬ í•™ìŠµ (BioLinkBERT)',
            'dimension': 768,
            'type': 'huggingface'
        },
        # ì¼ë°˜ ëª¨ë¸
        'bert-base': {
            'name': 'bert-base-uncased',
            'description': 'BERT ê¸°ë³¸ ëª¨ë¸',
            'dimension': 768,
            'type': 'huggingface'
        },
        # OpenAI API ëª¨ë¸
        'openai-small': {
            'name': 'text-embedding-3-small',
            'description': 'OpenAI ì„ë² ë”© (ë¹ ë¦„, ì €ë ´)',
            'dimension': 1536,
            'type': 'openai'
        },
        'openai-large': {
            'name': 'text-embedding-3-large',
            'description': 'OpenAI ì„ë² ë”© (ê³ ì„±ëŠ¥)',
            'dimension': 3072,
            'type': 'openai'
        }
    }

    @classmethod
    def create(cls, model_type: str = 'biobert', device: str = 'cpu', openai_api_key: str = None):
        if model_type not in cls.MODELS:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_type}. 'biobert' ì‚¬ìš©")
            model_type = 'biobert'

        model_info = cls.MODELS[model_type]

        print(f"\nğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"   ëª¨ë¸: {model_type}")
        print(f"   ì„¤ëª…: {model_info['description']}")
        print(f"   ì°¨ì›: {model_info['dimension']}")

        # OpenAI ëª¨ë¸ì¸ ê²½ìš°
        if model_info['type'] == 'openai':
            if not openai_api_key:
                print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. biobertë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return cls.create('biobert', device, None)

            try:
                embeddings = OpenAIEmbeddings(api_key=openai_api_key, model=model_info['name'])
                print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                return embeddings
            except Exception as e:
                print(f"âš ï¸ OpenAI ì„ë² ë”© ì‹¤íŒ¨: {str(e)[:50]}")
                print("   biobertë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
                return cls.create('biobert', device, None)

        # HuggingFace ëª¨ë¸ì¸ ê²½ìš°
        try:
            embeddings = HuggingFaceEmbeddings(model_name=model_info['name'], device=device)
            print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            return embeddings

        except Exception as e:
            print(f"âš ï¸ {model_type} ë¡œë“œ ì‹¤íŒ¨: {str(e)[:100]}")
            print("   bert-base ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")

            try:
                return HuggingFaceEmbeddings(
                    model_name='bert-base-uncased',
                    device=device
                )
            except Exception as e2:
                print(f"âš ï¸ bert-baseë„ ì‹¤íŒ¨: {str(e2)[:50]}")
                print("   OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
                if openai_api_key:
                    return OpenAIEmbeddings(api_key=openai_api_key)
                raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")


# ==================== RAGSystem í´ë˜ìŠ¤ ====================
class RAGSystem:
    def __init__(self, embeddings, chunk_size: int = 1000, chunk_overlap: int = 200, language: str = 'en'):
        self.embeddings = embeddings
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.vectorstore = None
        self.language = language
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def build_vectorstore(self, documents: List[Dict]) -> FAISS:
        print("\n" + "=" * 60)
        print("ğŸ”§ ë²¡í„° DB ìƒì„±")
        print("=" * 60)

        # Step 1: í…ìŠ¤íŠ¸ ì²­í‚¹
        print("\nâœ‚ï¸ Step 1: í…ìŠ¤íŠ¸ ì²­í‚¹")
        all_chunks = []
        all_metadata = []

        with tqdm(
            total=len(documents),
            desc="   ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='yellow'
        ) as pbar:
            for doc in documents:
                chunks = self.text_splitter.split_text(doc['text'])
                for i, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'source': doc['source'],
                        'chunk_id': i
                    })
                pbar.set_postfix_str(f"{len(chunks)} ì²­í¬")
                pbar.update(1)

        print(f"   ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}ê°œ")

        if not all_chunks:
            raise ValueError("ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")

        # Step 2: ì„ë² ë”© ìƒì„± ë° ë²¡í„° DB ìƒì„±
        print("\nğŸ§  Step 2: ì„ë² ë”© ìƒì„± ë° ë²¡í„° DB êµ¬ì¶•")

        self.vectorstore = FAISS.from_texts(
            texts=all_chunks,
            embedding=self.embeddings,
            metadatas=all_metadata
        )

        print("\nâœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ!")
        print("-" * 60)
        return self.vectorstore

    def save_vectorstore(self, path: str = VECTORSTORE_DIR):
        if self.vectorstore:
            self.vectorstore.save_local(path)
            print(f"ğŸ’¾ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {path}")

    def search(self, query: str, k: int = 3) -> List[Dict]:
        if not self.vectorstore:
            print("âŒ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)

        results = []
        for doc, score in docs_with_scores:
            results.append({
                'content': doc.page_content,
                'source': doc.metadata.get('source', 'Unknown'),
                'score': float(score)
            })

        return results

    def answer(self, question: str, k: int = 3) -> Dict:
        # ì§ˆë¬¸ ì–¸ì–´ ê°ì§€
        q_language = detect_language(question)

        results = self.search(question, k=k)

        return {
            'question': question,
            'contexts': results,
            'sources': list(set([r['source'] for r in results])),
            'language': q_language
        }

    def get_all_chunks(self) -> List[Dict]:
        """ì €ì¥ëœ ëª¨ë“  ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜"""
        if not self.vectorstore:
            return []

        # FAISSì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        docstore = self.vectorstore.docstore
        index_to_id = self.vectorstore.index_to_docstore_id

        chunks = []
        for idx, doc_id in index_to_id.items():
            doc = docstore.search(doc_id)
            if doc:
                chunks.append({
                    'content': doc.page_content,
                    'source': doc.metadata.get('source', 'Unknown'),
                    'chunk_id': doc.metadata.get('chunk_id', idx)
                })
        return chunks


# ==================== Qdrant Hybrid Search ì‹œìŠ¤í…œ ====================
class QdrantHybridSearch:
    """
    Qdrant ê¸°ë°˜ Hybrid Search ì‹œìŠ¤í…œ
    - Named Vectors: dense + sparse ë™ì‹œ ì €ì¥
    - Payload Filtering: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§
    - HNSW Index: ë¹ ë¥¸ ANN ê²€ìƒ‰
    """

    def __init__(
        self,
        embeddings,
        sparse_encoder=None,
        collection_name: str = "papers",
        use_memory: bool = True,
        qdrant_url: str = None,
        sparse_method: str = 'bm25',
        reranker: 'Reranker' = None,
        use_reranking: bool = False,
        reranker_model: str = 'ms-marco'
    ):
        from qdrant_client import QdrantClient
        from qdrant_client.models import (
            VectorParams, SparseVectorParams, Distance,
            HnswConfigDiff, OptimizersConfigDiff
        )

        self.embeddings = embeddings
        self.collection_name = collection_name
        self.sparse_method = sparse_method.lower()
        self.chunks = []
        self.splade_encoder = None
        self.reranker = reranker
        self.use_reranking = use_reranking

        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if use_memory:
            print("\nğŸ—„ï¸ Qdrant ì¸ë©”ëª¨ë¦¬ ëª¨ë“œë¡œ ì´ˆê¸°í™”...")
            self.client = QdrantClient(":memory:")
        else:
            print(f"\nğŸ—„ï¸ Qdrant ì„œë²„ ì—°ê²°: {qdrant_url}")
            self.client = QdrantClient(url=qdrant_url)

        # Reranker ì´ˆê¸°í™” (ìš”ì²­ ì‹œ)
        if use_reranking and self.reranker is None:
            print("\nğŸ¯ Reranker ì´ˆê¸°í™”...")
            self.reranker = Reranker(model_type=reranker_model)

        # Dense ë²¡í„° ì°¨ì› í™•ì¸
        test_embedding = self.embeddings.embed_query("test")
        self.dense_dim = len(test_embedding)
        print(f"   ğŸ“ Dense ë²¡í„° ì°¨ì›: {self.dense_dim}")

        # Sparse ì¸ì½”ë” ì´ˆê¸°í™”
        if self.sparse_method == 'splade':
            self._init_splade_encoder(sparse_encoder)
        else:
            # BM25ìš© ìŠ¤í…Œë¨¸ ì´ˆê¸°í™”
            self._init_stemmer()
            print(f"   ğŸ”¤ Sparse ë°©ì‹: BM25 (ìŠ¤í…Œë°)")

    def _init_splade_encoder(self, sparse_encoder=None):
        """SPLADE ì¸ì½”ë” ì´ˆê¸°í™”"""
        if sparse_encoder is not None:
            self.splade_encoder = sparse_encoder
            print(f"   ğŸ”¤ Sparse ë°©ì‹: SPLADE (ì™¸ë¶€ ì¸ì½”ë”)")
        else:
            try:
                print(f"   ğŸ”¤ SPLADE ì¸ì½”ë” ë¡œë”© ì¤‘...")
                self.splade_encoder = SPLADEEncoder(device='cpu')
                print(f"   âœ… SPLADE ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                print(f"   âš ï¸ SPLADE ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"   âš ï¸ BM25ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                self.sparse_method = 'bm25'
                self._init_stemmer()

    def _init_stemmer(self):
        """ìŠ¤í…Œë¨¸ ì´ˆê¸°í™”"""
        try:
            from nltk.stem import PorterStemmer
            self.stemmer = PorterStemmer()
            self.use_stemming = True
        except ImportError:
            self.stemmer = None
            self.use_stemming = False

    def _tokenize(self, text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì € + ìŠ¤í…Œë°"""
        import re
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)
        if self.use_stemming and self.stemmer:
            tokens = [self.stemmer.stem(token) for token in tokens]
        return tokens

    def _text_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """í…ìŠ¤íŠ¸ë¥¼ ìŠ¤íŒŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜ (BM25 ë˜ëŠ” SPLADE)"""
        if self.sparse_method == 'splade' and self.splade_encoder is not None:
            return self._splade_to_sparse_vector(text)
        else:
            return self._bm25_to_sparse_vector(text)

    def _bm25_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """BM25 ìŠ¤íƒ€ì¼ ìŠ¤íŒŒìŠ¤ ë²¡í„° ìƒì„±"""
        from collections import Counter
        import math

        tokens = self._tokenize(text)
        token_counts = Counter(tokens)

        sparse_vector = {}
        for token, count in token_counts.items():
            # í•´ì‹œ í•¨ìˆ˜ë¡œ ì¸ë±ìŠ¤ ìƒì„± (vocabulary ëŒ€ì‹ )
            idx = hash(token) % 30000  # 30000 ì°¨ì›ìœ¼ë¡œ ì œí•œ
            if idx < 0:
                idx = -idx
            tf = 1 + math.log(count) if count > 0 else 0
            sparse_vector[idx] = tf

        return sparse_vector

    def _splade_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """SPLADE ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŒŒìŠ¤ ë²¡í„° ìƒì„±"""
        # SPLADE ì¸ì½”ë”© (token -> weight ë”•ì…”ë„ˆë¦¬ ë°˜í™˜)
        splade_result = self.splade_encoder.encode([text])[0]

        # token ë¬¸ìì—´ì„ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        sparse_vector = {}
        for token, weight in splade_result.items():
            # í•´ì‹œ í•¨ìˆ˜ë¡œ ì¸ë±ìŠ¤ ìƒì„±
            idx = hash(token) % 30000
            if idx < 0:
                idx = -idx
            # ê°™ì€ ì¸ë±ìŠ¤ì— ì—¬ëŸ¬ í† í°ì´ ë§¤í•‘ë˜ë©´ ìµœëŒ€ê°’ ì‚¬ìš©
            if idx in sparse_vector:
                sparse_vector[idx] = max(sparse_vector[idx], weight)
            else:
                sparse_vector[idx] = weight

        return sparse_vector

    def create_collection(self):
        """ì»¬ë ‰ì…˜ ìƒì„± (Named Vectors + HNSW ì¸ë±ìŠ¤)"""
        from qdrant_client.models import (
            VectorParams, SparseVectorParams, Distance,
            HnswConfigDiff, OptimizersConfigDiff
        )

        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        try:
            self.client.delete_collection(self.collection_name)
        except:
            pass

        print(f"\nğŸ“¦ Qdrant ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
        print("   ğŸ”§ HNSW ì¸ë±ìŠ¤ ì„¤ì • ì¤‘...")

        # Named Vectorsë¡œ ì»¬ë ‰ì…˜ ìƒì„±
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config={
                # Dense ë²¡í„° (HNSW ì¸ë±ìŠ¤)
                "dense": VectorParams(
                    size=self.dense_dim,
                    distance=Distance.COSINE,
                    hnsw_config=HnswConfigDiff(
                        m=16,              # ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•, ëŠë¦¼)
                        ef_construct=100,  # êµ¬ì¶• ì‹œ íƒìƒ‰ ë²”ìœ„
                        full_scan_threshold=10000
                    )
                )
            },
            sparse_vectors_config={
                # Sparse ë²¡í„°
                "sparse": SparseVectorParams()
            },
            optimizers_config=OptimizersConfigDiff(
                indexing_threshold=20000  # ì¸ë±ì‹± ì„ê³„ê°’
            )
        )

        print("   âœ… Dense ë²¡í„°: HNSW ì¸ë±ìŠ¤ (COSINE)")
        print(f"   âœ… Sparse ë²¡í„°: {self.sparse_method.upper()} + Inverted Index")

    def add_documents(self, documents: List[Dict], text_splitter):
        """ë¬¸ì„œ ì¶”ê°€ (ì²­í‚¹ + ì„ë² ë”© + ì €ì¥)"""
        from qdrant_client.models import PointStruct, SparseVector
        import uuid

        print("\n" + "=" * 60)
        print("ğŸ”§ Qdrant ë²¡í„° DB êµ¬ì¶•")
        print("=" * 60)

        # Step 1: ì²­í‚¹
        print("\nâœ‚ï¸ Step 1: í…ìŠ¤íŠ¸ ì²­í‚¹")
        all_chunks = []
        all_metadata = []

        with tqdm(
            total=len(documents),
            desc="   ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='yellow'
        ) as pbar:
            for doc in documents:
                chunks = text_splitter.split_text(doc['text'])
                for i, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'source': doc['source'],
                        'filepath': doc.get('filepath', ''),
                        'chunk_id': i,
                        'chunk_total': len(chunks),
                        'text_length': len(chunk)
                    })
                pbar.set_postfix_str(f"{len(chunks)} ì²­í¬")
                pbar.update(1)

        print(f"   ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}ê°œ")

        if not all_chunks:
            raise ValueError("ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")

        self.chunks = [{'content': c, **m} for c, m in zip(all_chunks, all_metadata)]

        # Step 2: Dense ì„ë² ë”© ìƒì„±
        print("\nğŸ§  Step 2: Dense ì„ë² ë”© ìƒì„±")
        dense_vectors = self.embeddings.embed_documents(all_chunks)

        # Step 3: Sparse ë²¡í„° ìƒì„±
        sparse_method_name = self.sparse_method.upper()
        print(f"\nğŸ”¤ Step 3: Sparse ë²¡í„° ìƒì„± ({sparse_method_name})")
        sparse_vectors = []

        with tqdm(
            total=len(all_chunks),
            desc=f"   ğŸ“Š {sparse_method_name}",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='magenta'
        ) as pbar:
            for chunk in all_chunks:
                sv = self._text_to_sparse_vector(chunk)
                sparse_vectors.append(sv)
                pbar.update(1)

        # Step 4: Qdrantì— ì €ì¥
        print("\nğŸ’¾ Step 4: Qdrant ì €ì¥")
        points = []

        with tqdm(
            total=len(all_chunks),
            desc="   ğŸ”§ í¬ì¸íŠ¸ ìƒì„±",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='blue'
        ) as pbar:
            for i, (chunk, dense_vec, sparse_vec, metadata) in enumerate(
                zip(all_chunks, dense_vectors, sparse_vectors, all_metadata)
            ):
                sparse_indices = list(sparse_vec.keys())
                sparse_values = list(sparse_vec.values())

                point = PointStruct(
                    id=str(uuid.uuid4()),
                    vector={
                        "dense": dense_vec,
                        "sparse": SparseVector(
                            indices=sparse_indices,
                            values=sparse_values
                        )
                    },
                    payload={
                        "text": chunk,
                        **metadata
                    }
                )
                points.append(point)
                pbar.update(1)

        # ë°°ì¹˜ ì—…ë¡œë“œ
        batch_size = 100
        total_batches = (len(points) + batch_size - 1) // batch_size

        with tqdm(
            total=total_batches,
            desc="   ğŸ“¤ Qdrant ì—…ë¡œë“œ",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='green'
        ) as pbar:
            for i in range(0, len(points), batch_size):
                batch = points[i:i + batch_size]
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=batch
                )
                pbar.update(1)

        print(f"\nâœ… Qdrant ì €ì¥ ì™„ë£Œ! ({len(points)}ê°œ ë²¡í„°)")
        print("-" * 60)

        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        info = self.client.get_collection(self.collection_name)
        print(f"   ğŸ“Š ì»¬ë ‰ì…˜ ìƒíƒœ: {info.status}")
        print(f"   ğŸ“Š í¬ì¸íŠ¸ ìˆ˜: {info.points_count}")

    def dense_search(self, query: str, k: int = 5, filter_conditions: Dict = None) -> List[Dict]:
        """Dense ë²¡í„° ê²€ìƒ‰ (HNSW ANN)"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, SearchParams

        query_vector = self.embeddings.embed_query(query)

        # í•„í„° ì¡°ê±´ êµ¬ì„±
        query_filter = None
        if filter_conditions:
            conditions = []
            for field, value in filter_conditions.items():
                conditions.append(
                    FieldCondition(key=field, match=MatchValue(value=value))
                )
            query_filter = Filter(must=conditions)

        # ìƒˆë¡œìš´ Qdrant API ì‚¬ìš©
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=query_vector,
            using="dense",
            query_filter=query_filter,
            limit=k,
            with_payload=True,
            search_params=SearchParams(hnsw_ef=128)
        )

        return [
            {
                'content': r.payload.get('text', ''),
                'source': r.payload.get('source', 'Unknown'),
                'score': r.score,
                'chunk_id': r.payload.get('chunk_id', 0),
                'method': 'dense'
            }
            for r in results.points
        ]

    def sparse_search(self, query: str, k: int = 5, filter_conditions: Dict = None) -> List[Dict]:
        """Sparse ë²¡í„° ê²€ìƒ‰"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, SparseVector

        sparse_vec = self._text_to_sparse_vector(query)
        sparse_indices = list(sparse_vec.keys())
        sparse_values = list(sparse_vec.values())

        # í•„í„° ì¡°ê±´ êµ¬ì„±
        query_filter = None
        if filter_conditions:
            conditions = []
            for field, value in filter_conditions.items():
                conditions.append(
                    FieldCondition(key=field, match=MatchValue(value=value))
                )
            query_filter = Filter(must=conditions)

        # ìƒˆë¡œìš´ Qdrant API ì‚¬ìš©
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=SparseVector(
                indices=sparse_indices,
                values=sparse_values
            ),
            using="sparse",
            query_filter=query_filter,
            limit=k,
            with_payload=True
        )

        return [
            {
                'content': r.payload.get('text', ''),
                'source': r.payload.get('source', 'Unknown'),
                'score': r.score,
                'chunk_id': r.payload.get('chunk_id', 0),
                'method': 'sparse'
            }
            for r in results.points
        ]

    def hybrid_search(
        self,
        query: str,
        k: int = 5,
        alpha: float = 0.5,
        filter_conditions: Dict = None,
        use_reranking: bool = None,
        rerank_top_k: int = None
    ) -> List[Dict]:
        """
        Hybrid ê²€ìƒ‰ (Dense + Sparse) + ì„ íƒì  Reranking

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: 0.0 = ìˆœìˆ˜ Sparse, 1.0 = ìˆœìˆ˜ Dense
            filter_conditions: í•„í„° ì¡°ê±´
            use_reranking: Reranking ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ self.use_reranking ì‚¬ìš©)
            rerank_top_k: Rerankingí•  í›„ë³´ ìˆ˜ (Noneì´ë©´ k*3)
        """
        import numpy as np

        # Reranking ì„¤ì •
        apply_reranking = use_reranking if use_reranking is not None else self.use_reranking
        rerank_candidates = rerank_top_k if rerank_top_k else max(k * 3, 20)

        # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰ (reranking ì‹œ ë” ë§ì´)
        num_candidates = rerank_candidates if apply_reranking else max(k * 3, 20)

        dense_results = self.dense_search(query, k=num_candidates, filter_conditions=filter_conditions)
        sparse_results = self.sparse_search(query, k=num_candidates, filter_conditions=filter_conditions)

        # ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’
        dense_scores = [r['score'] for r in dense_results] if dense_results else [0]
        sparse_scores = [r['score'] for r in sparse_results] if sparse_results else [0]

        dense_max, dense_min = max(dense_scores), min(dense_scores)
        sparse_max, sparse_min = max(sparse_scores), min(sparse_scores)

        dense_range = dense_max - dense_min if dense_max > dense_min else 1.0
        sparse_range = sparse_max - sparse_min if sparse_max > sparse_min else 1.0

        # ê²°ê³¼ í†µí•©
        doc_scores = {}

        for r in dense_results:
            key = r['content'][:100]
            dense_norm = (r['score'] - dense_min) / dense_range
            doc_scores[key] = {
                'content': r['content'],
                'source': r['source'],
                'chunk_id': r['chunk_id'],
                'dense_score': r['score'],
                'dense_norm': dense_norm,
                'sparse_score': 0,
                'sparse_norm': 0
            }

        for r in sparse_results:
            key = r['content'][:100]
            sparse_norm = (r['score'] - sparse_min) / sparse_range

            if key in doc_scores:
                doc_scores[key]['sparse_score'] = r['score']
                doc_scores[key]['sparse_norm'] = sparse_norm
            else:
                doc_scores[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'chunk_id': r['chunk_id'],
                    'dense_score': 0,
                    'dense_norm': 0,
                    'sparse_score': r['score'],
                    'sparse_norm': sparse_norm
                }

        # Hybrid ì ìˆ˜ ê³„ì‚°
        results = []
        for key, data in doc_scores.items():
            hybrid_score = alpha * data['dense_norm'] + (1 - alpha) * data['sparse_norm']
            results.append({
                'content': data['content'],
                'source': data['source'],
                'chunk_id': data['chunk_id'],
                'hybrid_score': hybrid_score,
                'dense_score': data['dense_score'],
                'dense_norm': data['dense_norm'],
                'sparse_score': data['sparse_score'],
                'sparse_norm': data['sparse_norm'],
                'method': 'hybrid'
            })

        # ì •ë ¬
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)

        # Reranking ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if apply_reranking and self.reranker is not None:
            # ìƒìœ„ í›„ë³´ë“¤ë§Œ reranking
            candidates = results[:rerank_candidates]
            reranked = self.reranker.rerank(
                query=query,
                documents=candidates,
                top_k=k,
                content_key='content',
                show_progress=len(candidates) > 10
            )
            # reranked ê²°ê³¼ì— method ì—…ë°ì´íŠ¸
            for r in reranked:
                r['method'] = 'hybrid+rerank'
            return reranked

        return results[:k]

    def search_with_filter(
        self,
        query: str,
        source_filter: str = None,
        min_chunk_length: int = None,
        k: int = 5,
        search_type: str = 'hybrid'
    ) -> List[Dict]:
        """Payload í•„í„°ë§ì„ ì ìš©í•œ ê²€ìƒ‰"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, Range

        conditions = []

        if source_filter:
            conditions.append(
                FieldCondition(
                    key="source",
                    match=MatchValue(value=source_filter)
                )
            )

        if min_chunk_length:
            conditions.append(
                FieldCondition(
                    key="text_length",
                    range=Range(gte=min_chunk_length)
                )
            )

        query_filter = Filter(must=conditions) if conditions else None

        filter_dict = {}
        if source_filter:
            filter_dict['source'] = source_filter

        if search_type == 'dense':
            return self.dense_search(query, k, filter_conditions=filter_dict if filter_dict else None)
        elif search_type == 'sparse':
            return self.sparse_search(query, k, filter_conditions=filter_dict if filter_dict else None)
        else:
            return self.hybrid_search(query, k, filter_conditions=filter_dict if filter_dict else None)

    def get_collection_info(self) -> Dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        info = self.client.get_collection(self.collection_name)
        return {
            'name': self.collection_name,
            'status': str(info.status),
            'points_count': info.points_count,
            'vectors_count': info.vectors_count,
            'indexed_vectors_count': info.indexed_vectors_count
        }

    def get_sparse_method_name(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ sparse ë°©ì‹ ì´ë¦„ ë°˜í™˜"""
        return self.sparse_method.upper()

    def visualize_comparison(
        self,
        query: str,
        sparse_results: List[Dict],
        dense_results: List[Dict],
        hybrid_results: List[Dict],
        alpha: float = 0.7,
        sparse_method: str = 'BM25',
        dense_model: str = 'Dense',
        save_path: str = None,
        show_plot: bool = False
    ) -> str:
        """Qdrant ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™” (íŒŒì¼ ì €ì¥)"""
        import matplotlib
        matplotlib.use('Agg')  # ë¹„ëŒ€í™”í˜• ë°±ì—”ë“œ (ë¹ ë¥´ê³  ì•ˆì •ì )
        import matplotlib.pyplot as plt

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            plt.rcParams['font.family'] = 'AppleGothic'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(15, 11))
        fig.suptitle(f'ğŸ” Qdrant Hybrid Search Analysis\nQuery: "{query[:60]}..."',
                    fontsize=14, fontweight='bold')

        # 1. Sparse ì ìˆ˜ (ì™¼ìª½ ìƒë‹¨)
        ax1 = axes[0, 0]
        if sparse_results:
            sparse_labels = [f"Doc {i+1}\n{r['source'][:20]}..." for i, r in enumerate(sparse_results[:5])]
            sparse_scores = [r['score'] for r in sparse_results[:5]]
            colors1 = plt.cm.Blues([(0.4 + 0.12*i) for i in range(len(sparse_scores))])
            bars1 = ax1.barh(sparse_labels, sparse_scores, color=colors1, edgecolor='navy', alpha=0.85)
            ax1.set_xlabel(f'{sparse_method} Score', fontweight='bold')
            ax1.set_title(f'ğŸ”µ Sparse Search ({sparse_method})', fontweight='bold', fontsize=11)
            ax1.invert_yaxis()
            for bar, score in zip(bars1, sparse_scores):
                ax1.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,
                        f'{score:.3f}', va='center', fontsize=9, fontweight='bold')
            ax1.set_xlim(0, max(sparse_scores) * 1.3 if sparse_scores else 1)

        # 2. Dense ì ìˆ˜ (ì˜¤ë¥¸ìª½ ìƒë‹¨)
        ax2 = axes[0, 1]
        if dense_results:
            dense_labels = [f"Doc {i+1}\n{r['source'][:20]}..." for i, r in enumerate(dense_results[:5])]
            dense_scores = [r['score'] for r in dense_results[:5]]
            colors2 = plt.cm.Reds([(0.4 + 0.12*i) for i in range(len(dense_scores))])
            bars2 = ax2.barh(dense_labels, dense_scores, color=colors2, edgecolor='darkred', alpha=0.85)
            ax2.set_xlabel('Cosine Similarity (higher = better)', fontweight='bold')
            ax2.set_title(f'ğŸ”´ Dense Search (HNSW, {dense_model})', fontweight='bold', fontsize=11)
            ax2.invert_yaxis()
            for bar, score in zip(bars2, dense_scores):
                ax2.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                        f'{score:.4f}', va='center', fontsize=9, fontweight='bold')
            ax2.set_xlim(0, max(dense_scores) * 1.2 if dense_scores else 1)

        # 3. Hybrid ì ìˆ˜ ë¹„êµ (ì™¼ìª½ í•˜ë‹¨)
        ax3 = axes[1, 0]
        if hybrid_results:
            hybrid_labels = [f"Doc {i+1}" for i in range(len(hybrid_results[:5]))]
            x = range(len(hybrid_labels))
            width = 0.25

            sparse_norm = [r.get('sparse_norm', 0) for r in hybrid_results[:5]]
            dense_norm = [r.get('dense_norm', 0) for r in hybrid_results[:5]]
            hybrid_scores = [r['hybrid_score'] for r in hybrid_results[:5]]

            bars_sparse = ax3.bar([i - width for i in x], sparse_norm, width,
                                 label=f'{sparse_method} (norm)', color='#3498db', alpha=0.85, edgecolor='navy')
            bars_dense = ax3.bar(x, dense_norm, width,
                                label='Dense (norm)', color='#e74c3c', alpha=0.85, edgecolor='darkred')
            bars_hybrid = ax3.bar([i + width for i in x], hybrid_scores, width,
                                 label='Hybrid', color='#2ecc71', alpha=0.85, edgecolor='darkgreen')

            ax3.set_xlabel('Document', fontweight='bold')
            ax3.set_ylabel('Normalized Score (0-1)', fontweight='bold')
            ax3.set_title(f'ğŸŸ¢ Hybrid Score Fusion (Î±={alpha})\n{sparse_method}Ã—{1-alpha:.1f} + DenseÃ—{alpha:.1f}',
                         fontweight='bold', fontsize=11)
            ax3.set_xticks(x)
            ax3.set_xticklabels(hybrid_labels)
            ax3.set_ylim(0, 1.15)
            ax3.legend(loc='upper right', fontsize=9)
            ax3.grid(axis='y', alpha=0.3)

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ í‘œì‹œ
            for bar, score in zip(bars_hybrid, hybrid_scores):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                        f'{score:.2f}', ha='center', va='bottom', fontsize=8, fontweight='bold')

        # 4. ìƒì„¸ ê²°ê³¼ ìš”ì•½ (ì˜¤ë¥¸ìª½ í•˜ë‹¨)
        ax4 = axes[1, 1]
        ax4.axis('off')

        info_text = "ğŸ“Š Qdrant Search Results Summary\n" + "="*45 + "\n\n"

        info_text += f"ğŸ”µ Sparse ({sparse_method}) - Top 3:\n"
        for i, r in enumerate(sparse_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     Score: {r['score']:.4f}\n"

        info_text += f"\nğŸ”´ Dense (HNSW) - Top 3:\n"
        for i, r in enumerate(dense_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     Cosine: {r['score']:.4f}\n"

        info_text += f"\nğŸŸ¢ Hybrid (Î±={alpha}) - Top 3:\n"
        for i, r in enumerate(hybrid_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            sparse_s = r.get('sparse_score', 0)
            dense_s = r.get('dense_score', 0)
            hybrid_s = r.get('hybrid_score', 0)
            info_text += f"  {i}. {source}\n"
            info_text += f"     Hybrid: {hybrid_s:.3f} (S:{sparse_s:.3f} D:{dense_s:.3f})\n"

        info_text += f"\nğŸ“ˆ Statistics:\n"
        info_text += f"  â€¢ Collection: {self.collection_name}\n"
        info_text += f"  â€¢ Total chunks: {len(self.chunks)}\n"
        info_text += f"  â€¢ Dense dim: {self.dense_dim}\n"

        ax4.text(0.02, 0.98, info_text, transform=ax4.transAxes, fontsize=9,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow',
                         alpha=0.9, edgecolor='orange'))

        plt.tight_layout()

        # ì €ì¥
        if save_path is None:
            save_path = f"qdrant_hybrid_search_{query[:20].replace(' ', '_')}.png"

        plt.savefig(save_path, dpi=100, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close(fig)  # ëª…ì‹œì ìœ¼ë¡œ figure ë‹«ê¸°

        print(f"   ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
        return save_path


# ==================== SPLADE Encoder ====================
class SPLADEEncoder:
    """SPLADE (Sparse Lexical and Expansion) ì¸ì½”ë”"""

    def __init__(self, model_name: str = "naver/splade-cocondenser-ensembledistil", device: str = 'cpu'):
        import torch
        from transformers import AutoTokenizer, AutoModelForMaskedLM

        self.device = device
        self.model_name = model_name

        print(f"   ğŸ“¥ SPLADE ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()
        self.torch = torch

    def encode(self, texts: List[str], max_length: int = 256) -> List[Dict[str, float]]:
        """í…ìŠ¤íŠ¸ë¥¼ SPLADE ìŠ¤íŒŒìŠ¤ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
        sparse_vectors = []

        for text in texts:
            # í† í°í™”
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                max_length=max_length,
                truncation=True,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # SPLADE ì¸ì½”ë”©
            with self.torch.no_grad():
                outputs = self.model(**inputs)
                # SPLADE: log(1 + ReLU(logits)) * attention_mask
                logits = outputs.logits
                relu_log = self.torch.log1p(self.torch.relu(logits))
                # Max pooling over sequence
                weighted = relu_log * inputs['attention_mask'].unsqueeze(-1)
                sparse_vec = self.torch.max(weighted, dim=1).values.squeeze()

            # 0ì´ ì•„ë‹Œ ê°’ë§Œ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
            sparse_dict = {}
            indices = self.torch.nonzero(sparse_vec).squeeze(-1)
            for idx in indices:
                idx = idx.item()
                token = self.tokenizer.decode([idx])
                weight = sparse_vec[idx].item()
                if weight > 0.1:  # ì„ê³„ê°’ ì´ìƒë§Œ ì €ì¥
                    sparse_dict[token] = weight

            sparse_vectors.append(sparse_dict)

        return sparse_vectors

    def compute_similarity(self, query_vec: Dict[str, float], doc_vec: Dict[str, float]) -> float:
        """ì¿¼ë¦¬ì™€ ë¬¸ì„œ ìŠ¤íŒŒìŠ¤ ë²¡í„°ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ë‚´ì )"""
        score = 0.0
        for token, weight in query_vec.items():
            if token in doc_vec:
                score += weight * doc_vec[token]
        return score


# ==================== Hybrid Search ì‹œìŠ¤í…œ ====================
class HybridSearchSystem:
    """Sparse (BM25/SPLADE) + Dense (Semantic) + Hybrid ê²€ìƒ‰ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        rag_system: RAGSystem,
        sparse_method: str = 'bm25',
        use_reranking: bool = False,
        reranker_model: str = 'ms-marco'
    ):
        """
        Args:
            rag_system: RAG ì‹œìŠ¤í…œ
            sparse_method: 'bm25' ë˜ëŠ” 'splade'
            use_reranking: Reranking ì‚¬ìš© ì—¬ë¶€
            reranker_model: Reranker ëª¨ë¸ íƒ€ì…
        """
        from rank_bm25 import BM25Okapi
        import numpy as np

        self.rag = rag_system
        self.chunks = rag_system.get_all_chunks()
        self.np = np
        self.sparse_method = sparse_method.lower()
        self.use_reranking = use_reranking
        self.reranker = None

        # Reranker ì´ˆê¸°í™” (ìš”ì²­ ì‹œ)
        if use_reranking:
            print("\nğŸ¯ Reranker ì´ˆê¸°í™”...")
            self.reranker = Reranker(model_type=reranker_model)

        # ìŠ¤í…Œë¨¸ ì´ˆê¸°í™” (BM25ìš©)
        self._init_stemmer()

        # Sparse ì¸ë±ìŠ¤ êµ¬ì¶•
        if self.sparse_method == 'splade':
            self._init_splade()
        else:
            self._init_bm25()

    def _init_bm25(self):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        from rank_bm25 import BM25Okapi

        print("\nğŸ” BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (ìŠ¤í…Œë° ì ìš©)...")
        tokenized_corpus = [self._tokenize(chunk['content']) for chunk in self.chunks]
        self.bm25 = BM25Okapi(tokenized_corpus)
        self.splade = None
        self.splade_vectors = None
        print(f"âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ({len(self.chunks)} ì²­í¬)")

    def _init_splade(self):
        """SPLADE ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("\nğŸ” SPLADE ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

        try:
            self.splade = SPLADEEncoder()
            self.bm25 = None

            # ëª¨ë“  ë¬¸ì„œ ì¸ì½”ë”©
            print(f"   ğŸ“„ {len(self.chunks)}ê°œ ë¬¸ì„œ ì¸ì½”ë”© ì¤‘...")
            doc_texts = [chunk['content'] for chunk in self.chunks]

            # ë°°ì¹˜ ì²˜ë¦¬
            batch_size = 8
            self.splade_vectors = []
            for i in range(0, len(doc_texts), batch_size):
                batch = doc_texts[i:i+batch_size]
                vectors = self.splade.encode(batch)
                self.splade_vectors.extend(vectors)
                print(f"   ì§„í–‰: {min(i+batch_size, len(doc_texts))}/{len(doc_texts)}", end='\r')

            print(f"\nâœ… SPLADE ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ({len(self.chunks)} ì²­í¬)")

        except Exception as e:
            print(f"âš ï¸ SPLADE ë¡œë“œ ì‹¤íŒ¨: {str(e)[:50]}")
            print("   BM25ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            self.sparse_method = 'bm25'
            self._init_bm25()

    def _init_stemmer(self):
        """ìŠ¤í…Œë¨¸ ì´ˆê¸°í™” - Porter Stemmer ì‚¬ìš©"""
        try:
            from nltk.stem import PorterStemmer
            self.stemmer = PorterStemmer()
            self.use_stemming = True
        except ImportError:
            self.stemmer = None
            self.use_stemming = False

    def _tokenize(self, text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì € + ìŠ¤í…Œë°"""
        import re
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)

        # ìŠ¤í…Œë° ì ìš© (diabete -> diabet, diabetes -> diabet)
        if self.use_stemming and self.stemmer:
            tokens = [self.stemmer.stem(token) for token in tokens]

        return tokens

    def sparse_search(self, query: str, k: int = 5) -> List[Dict]:
        """Sparse ê²€ìƒ‰ (BM25 ë˜ëŠ” SPLADE)"""
        if self.sparse_method == 'splade' and self.splade is not None:
            return self._splade_search(query, k)
        else:
            return self._bm25_search(query, k)

    def _bm25_search(self, query: str, k: int = 5) -> List[Dict]:
        """BM25 ê¸°ë°˜ ê²€ìƒ‰"""
        tokenized_query = self._tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = self.np.argsort(scores)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                'content': self.chunks[idx]['content'],
                'source': self.chunks[idx]['source'],
                'score': float(scores[idx]),
                'method': 'sparse (BM25)'
            })
        return results

    def _splade_search(self, query: str, k: int = 5) -> List[Dict]:
        """SPLADE ê¸°ë°˜ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì¸ì½”ë”©
        query_vec = self.splade.encode([query])[0]

        # ëª¨ë“  ë¬¸ì„œì™€ ìœ ì‚¬ë„ ê³„ì‚°
        scores = []
        for doc_vec in self.splade_vectors:
            score = self.splade.compute_similarity(query_vec, doc_vec)
            scores.append(score)

        scores = self.np.array(scores)

        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = self.np.argsort(scores)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                'content': self.chunks[idx]['content'],
                'source': self.chunks[idx]['source'],
                'score': float(scores[idx]),
                'method': 'sparse (SPLADE)'
            })
        return results

    def dense_search(self, query: str, k: int = 5) -> List[Dict]:
        """FAISS ê¸°ë°˜ Dense ê²€ìƒ‰ (ì˜ë¯¸ì  ìœ ì‚¬ë„)"""
        results = self.rag.search(query, k=k)
        for r in results:
            r['method'] = 'dense'
        return results

    def hybrid_search(
        self,
        query: str,
        k: int = 5,
        alpha: float = 0.5,
        rrf_k: int = 10,
        use_reranking: bool = None,
        rerank_top_k: int = None
    ) -> List[Dict]:
        """
        Hybrid ê²€ìƒ‰ (Sparse + Dense ê²°í•©) - RRF (Reciprocal Rank Fusion) ì‚¬ìš© + Reranking

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: 0.0 = ìˆœìˆ˜ Sparse, 1.0 = ìˆœìˆ˜ Dense
            rrf_k: RRF ìƒìˆ˜ (ê¸°ë³¸ê°’ 10)
            use_reranking: Reranking ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ self.use_reranking)
            rerank_top_k: Rerankingí•  í›„ë³´ ìˆ˜
        """
        # Reranking ì„¤ì •
        apply_reranking = use_reranking if use_reranking is not None else self.use_reranking
        rerank_candidates = rerank_top_k if rerank_top_k else max(k * 3, 20)

        # ì¶©ë¶„í•œ í›„ë³´ ê²€ìƒ‰
        num_candidates = rerank_candidates if apply_reranking else max(k * 3, 20)
        sparse_results = self.sparse_search(query, k=num_candidates)
        dense_results = self.dense_search(query, k=num_candidates)

        # BM25 ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’ ê³„ì‚°
        bm25_scores = [r['score'] for r in sparse_results]
        bm25_max = max(bm25_scores) if bm25_scores else 1.0
        bm25_min = min(bm25_scores) if bm25_scores else 0.0
        bm25_range = bm25_max - bm25_min if bm25_max > bm25_min else 1.0

        # Dense ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’ ê³„ì‚° (L2 ê±°ë¦¬ - ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        dense_scores = [r['score'] for r in dense_results]
        dense_max = max(dense_scores) if dense_scores else 1.0
        dense_min = min(dense_scores) if dense_scores else 0.0
        dense_range = dense_max - dense_min if dense_max > dense_min else 1.0

        # ë¬¸ì„œ í†µí•©ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        doc_data = {}

        # Sparse ê²°ê³¼ ì²˜ë¦¬ - ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ + BM25 ì •ê·œí™”
        for rank, r in enumerate(sparse_results):
            key = r['content'][:100]
            sparse_rrf = 1.0 / (rrf_k + rank + 1)  # RRF ì ìˆ˜
            # BM25 ì ìˆ˜ë¥¼ 0-1ë¡œ ì •ê·œí™”
            bm25_norm = (r['score'] - bm25_min) / bm25_range if bm25_range > 0 else 1.0

            if key not in doc_data:
                doc_data[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'sparse_rank': rank + 1,
                    'sparse_score': r['score'],  # ì›ë³¸ BM25 ì ìˆ˜ (0~30+ ë²”ìœ„)
                    'sparse_score_norm': bm25_norm,  # ì •ê·œí™”ëœ BM25 (0-1)
                    'sparse_rrf': sparse_rrf,
                    'dense_rank': 0,
                    'dense_score': 0,
                    'dense_score_norm': 0,
                    'dense_rrf': 0
                }
            else:
                doc_data[key]['sparse_rank'] = rank + 1
                doc_data[key]['sparse_score'] = r['score']
                doc_data[key]['sparse_score_norm'] = bm25_norm
                doc_data[key]['sparse_rrf'] = sparse_rrf

        # Dense ê²°ê³¼ ì²˜ë¦¬ - ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ + ê±°ë¦¬ ì •ê·œí™”
        for rank, r in enumerate(dense_results):
            key = r['content'][:100]
            dense_rrf = 1.0 / (rrf_k + rank + 1)  # RRF ì ìˆ˜
            # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (1 - ì •ê·œí™”ëœ ê±°ë¦¬)
            dense_norm = 1.0 - ((r['score'] - dense_min) / dense_range) if dense_range > 0 else 1.0

            if key not in doc_data:
                doc_data[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'sparse_rank': 0,
                    'sparse_score': 0,
                    'sparse_score_norm': 0,
                    'sparse_rrf': 0,
                    'dense_rank': rank + 1,
                    'dense_score': r['score'],  # ì›ë³¸ L2 ê±°ë¦¬
                    'dense_score_norm': dense_norm,  # ì •ê·œí™”ëœ ìœ ì‚¬ë„ (0-1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                    'dense_rrf': dense_rrf
                }
            else:
                doc_data[key]['dense_rank'] = rank + 1
                doc_data[key]['dense_score'] = r['score']
                doc_data[key]['dense_score_norm'] = dense_norm
                doc_data[key]['dense_rrf'] = dense_rrf

        # Hybrid ìŠ¤ì½”ì–´ ê³„ì‚° (ì •ê·œí™”ëœ ì ìˆ˜ ê¸°ë°˜)
        results = []
        for key, data in doc_data.items():
            # ë°©ë²• 1: RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ
            hybrid_rrf = (1 - alpha) * data['sparse_rrf'] + alpha * data['dense_rrf']

            # ë°©ë²• 2: ì •ê·œí™”ëœ ì ìˆ˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ (ë” ì§ê´€ì )
            hybrid_norm = (1 - alpha) * data['sparse_score_norm'] + alpha * data['dense_score_norm']

            results.append({
                'content': data['content'],
                'source': data['source'],
                # ì›ë³¸ ì ìˆ˜ë“¤
                'sparse_score': data['sparse_score'],      # ì›ë³¸ BM25 (0~30+)
                'dense_score': data['dense_score'],        # ì›ë³¸ L2 ê±°ë¦¬
                # ì •ê·œí™”ëœ ì ìˆ˜ë“¤ (0-1)
                'sparse_score_norm': data['sparse_score_norm'],  # BM25 ì •ê·œí™”
                'dense_score_norm': data['dense_score_norm'],    # ìœ ì‚¬ë„ ì •ê·œí™”
                # ìˆœìœ„ ì •ë³´
                'sparse_rank': data['sparse_rank'],
                'dense_rank': data['dense_rank'],
                # RRF ì ìˆ˜
                'sparse_rrf': data['sparse_rrf'],
                'dense_rrf': data['dense_rrf'],
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
                'hybrid_score': hybrid_norm,        # ì •ê·œí™” ê¸°ë°˜ (0-1)
                'hybrid_score_rrf': hybrid_rrf,     # RRF ê¸°ë°˜
                'method': 'hybrid'
            })

        # Hybrid ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)

        # Reranking ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if apply_reranking and self.reranker is not None:
            candidates = results[:rerank_candidates]
            reranked = self.reranker.rerank(
                query=query,
                documents=candidates,
                top_k=k,
                content_key='content',
                show_progress=len(candidates) > 10
            )
            for r in reranked:
                r['method'] = 'hybrid+rerank'
            return reranked

        return results[:k]

    def compare_all(self, query: str, k: int = 5, alpha: float = 0.5) -> Dict:
        """ì„¸ ê°€ì§€ ê²€ìƒ‰ ë°©ë²• ë¹„êµ"""
        sparse = self.sparse_search(query, k)
        dense = self.dense_search(query, k)
        hybrid = self.hybrid_search(query, k, alpha)

        return {
            'query': query,
            'sparse': sparse,
            'dense': dense,
            'hybrid': hybrid
        }

    def visualize_comparison(self, query: str, k: int = 5, alpha: float = 0.5,
                            save_path: str = None) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™” (íŒŒì¼ ì €ì¥)"""
        import matplotlib
        matplotlib.use('Agg')  # ë¹„ëŒ€í™”í˜• ë°±ì—”ë“œ (ë¹ ë¥´ê³  ì•ˆì •ì )
        import matplotlib.pyplot as plt

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            plt.rcParams['font.family'] = 'AppleGothic'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False

        results = self.compare_all(query, k, alpha)

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'Hybrid Search Comparison\nQuery: "{query[:50]}..."', fontsize=14, fontweight='bold')

        # 1. Sparse (BM25) ì ìˆ˜
        ax1 = axes[0, 0]
        sparse_labels = [f"Doc {i+1}" for i in range(len(results['sparse']))]
        sparse_scores = [r['score'] for r in results['sparse']]
        bars1 = ax1.barh(sparse_labels, sparse_scores, color='#3498db', alpha=0.8)
        ax1.set_xlabel('BM25 Score')
        ax1.set_title('Sparse Search (BM25)', fontweight='bold')
        ax1.invert_yaxis()
        for bar, score in zip(bars1, sparse_scores):
            ax1.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                    f'{score:.2f}', va='center', fontsize=9)

        # 2. Dense (Semantic) ì ìˆ˜
        ax2 = axes[0, 1]
        dense_labels = [f"Doc {i+1}" for i in range(len(results['dense']))]
        dense_scores = [r['score'] for r in results['dense']]
        bars2 = ax2.barh(dense_labels, dense_scores, color='#e74c3c', alpha=0.8)
        ax2.set_xlabel('L2 Distance (lower = better)')
        ax2.set_title('Dense Search (Semantic)', fontweight='bold')
        ax2.invert_yaxis()
        for bar, score in zip(bars2, dense_scores):
            ax2.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                    f'{score:.3f}', va='center', fontsize=9)

        # 3. Hybrid ì •ê·œí™” ì ìˆ˜ ë¹„êµ
        ax3 = axes[1, 0]
        hybrid_labels = [f"Doc {i+1}" for i in range(len(results['hybrid']))]
        x = range(len(hybrid_labels))
        width = 0.25

        # ì •ê·œí™”ëœ ì ìˆ˜ ì‚¬ìš© (0-1 ë²”ìœ„)
        sparse_norm = [r.get('sparse_score_norm', 0) for r in results['hybrid']]
        dense_norm = [r.get('dense_score_norm', 0) for r in results['hybrid']]
        hybrid_scores = [r['hybrid_score'] for r in results['hybrid']]

        ax3.bar([i - width for i in x], sparse_norm, width, label='BM25 (norm)', color='#3498db', alpha=0.8)
        ax3.bar(x, dense_norm, width, label='Semantic (norm)', color='#e74c3c', alpha=0.8)
        ax3.bar([i + width for i in x], hybrid_scores, width, label='Hybrid', color='#2ecc71', alpha=0.8)
        ax3.set_xlabel('Document')
        ax3.set_ylabel('Normalized Score (0-1)')
        ax3.set_title(f'Hybrid Search - Score Fusion (Î±={alpha})', fontweight='bold')
        ax3.set_xticks(x)
        ax3.set_xticklabels(hybrid_labels)
        ax3.set_ylim(0, 1.1)  # 0-1 ë²”ìœ„ ëª…í™•íˆ í‘œì‹œ
        ax3.legend()

        # 4. ë¬¸ì„œ ì¶œì²˜ ì •ë³´ (ìˆœìœ„ í¬í•¨)
        ax4 = axes[1, 1]
        ax4.axis('off')

        info_text = "ğŸ“Š Search Results Summary\n" + "="*40 + "\n\n"

        info_text += "ğŸ”µ Sparse (BM25) - Top Results:\n"
        for i, r in enumerate(results['sparse'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     BM25: {r['score']:.2f}\n"

        info_text += "\nğŸ”´ Dense (Semantic) - Top Results:\n"
        for i, r in enumerate(results['dense'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     L2 Dist: {r['score']:.4f}\n"

        info_text += "\nğŸŸ¢ Hybrid - Top Results:\n"
        for i, r in enumerate(results['hybrid'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            s_rank = r.get('sparse_rank', 0)
            d_rank = r.get('dense_rank', 0)
            bm25 = r.get('sparse_score', 0)
            s_norm = r.get('sparse_score_norm', 0)
            d_norm = r.get('dense_score_norm', 0)
            info_text += f"  {i}. {source}\n"
            info_text += f"     Hybrid: {r['hybrid_score']:.2f}\n"
            info_text += f"     BM25={bm25:.1f}({s_norm:.2f}) Sem={d_norm:.2f}\n"

        ax4.text(0.05, 0.95, info_text, transform=ax4.transAxes, fontsize=9,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.tight_layout()

        # ì €ì¥
        if save_path is None:
            save_path = f"./hybrid_search_comparison.png"

        plt.savefig(save_path, dpi=100, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close(fig)  # ëª…ì‹œì ìœ¼ë¡œ figure ë‹«ê¸°

        print(f"   ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
        return results


# ==================== ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ====================
def interactive_qa(rag: RAGSystem, openai_api_key: str = None):
    """ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ëª¨ë“œ"""

    # ì–¸ì–´ë³„ ë©”ì‹œì§€
    if rag.language == 'ko':
        print("\n" + "=" * 60)
        print("ğŸ’¬ ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ëª¨ë“œ")
        print("=" * 60)
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        prompt_text = "â“ ì§ˆë¬¸: "
        exit_msg = "ğŸ‘‹ ì§ˆì˜ì‘ë‹µì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
    else:
        print("\n" + "=" * 60)
        print("ğŸ’¬ Interactive Q&A Mode")
        print("=" * 60)
        print("Enter your question. Type 'quit', 'exit', or 'q' to exit.")
        prompt_text = "â“ Question: "
        exit_msg = "ğŸ‘‹ Exiting Q&A mode."

    print("-" * 60)

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìˆëŠ” ê²½ìš°)
    client = None
    if openai_api_key:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai_api_key)
        except:
            pass

    while True:
        try:
            question = input(f"\n{prompt_text}").strip()

            if not question:
                continue

            if question.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ', 'ë']:
                print(f"\n{exit_msg}")
                break

            # ê²°ê³¼ ê°œìˆ˜ ì¡°ì ˆ
            k = 3
            if 'k=' in question:
                try:
                    k_part = question.split('k=')[1].split()[0]
                    k = int(k_part)
                    question = question.replace(f'k={k_part}', '').strip()
                except:
                    pass

            # ì§ˆë¬¸ ì–¸ì–´ ê°ì§€
            q_lang = detect_language(question)

            # í•œêµ­ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ê²€ìƒ‰
            search_question = question
            if q_lang == 'ko':
                search_question = translate_to_english(question, openai_api_key)
                if search_question != question:
                    print(f"   ğŸ”„ ê²€ìƒ‰ì–´: '{search_question}'")

            result = rag.answer(search_question, k=k)
            result['language'] = q_lang  # ì›ë³¸ ì§ˆë¬¸ì˜ ì–¸ì–´ ìœ ì§€

            # ì–¸ì–´ë³„ ì¶œë ¥
            if q_lang == 'ko':
                print(f"\nğŸ“š ê´€ë ¨ ë¬¸ì„œ {len(result['contexts'])}ê°œ ê²€ìƒ‰ë¨:\n")
            else:
                print(f"\nğŸ“š Found {len(result['contexts'])} relevant documents:\n")

            for i, ctx in enumerate(result['contexts'], 1):
                similarity = 1 / (1 + ctx['score'])

                if q_lang == 'ko':
                    print(f"[{i}] ì¶œì²˜: {ctx['source'][:50]}")
                    print(f"    ìœ ì‚¬ë„: {similarity:.2%}")
                else:
                    print(f"[{i}] Source: {ctx['source'][:50]}")
                    print(f"    Similarity: {similarity:.2%}")

                content_preview = ctx['content'].replace('\n', ' ')[:300]
                print(f"    {content_preview}...")
                print("-" * 40)

            # OpenAIë¡œ ë‹µë³€ ìƒì„± (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            if client:
                try:
                    context_text = "\n\n".join([ctx['content'] for ctx in result['contexts']])

                    if q_lang == 'ko':
                        system_msg = "ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                    else:
                        system_msg = "You are an expert answering questions based on medical/scientific papers. Answer based on the provided context."

                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": f"Context:\n{context_text}\n\nQuestion: {question}"}
                        ],
                        max_tokens=500,
                        temperature=0.3
                    )

                    answer = response.choices[0].message.content

                    if q_lang == 'ko':
                        print(f"\nğŸ¤– AI ë‹µë³€:\n{answer}")
                    else:
                        print(f"\nğŸ¤– AI Answer:\n{answer}")

                except Exception as e:
                    pass

            if q_lang == 'ko':
                print(f"\nğŸ“– ì°¸ê³  ë¬¸ì„œ: {', '.join(result['sources'])}")
            else:
                print(f"\nğŸ“– References: {', '.join(result['sources'])}")

        except KeyboardInterrupt:
            print(f"\n\n{exit_msg}")
            break
        except Exception as e:
            print(f"âŒ Error: {str(e)}")


# ==================== íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ ====================
def run_trend_analysis():
    """íŠ¸ë Œë“œ ë¶„ì„ ëª¨ë“œ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„")
    print("=" * 60)

    # .envì—ì„œ API í‚¤ ë¡œë“œ
    pubmed_api_key = os.getenv('PUBMED_API_KEY')
    pubmed_email = os.getenv('PUBMED_EMAIL')
    openai_api_key = os.getenv('OPENAI_API_KEY')

    if openai_api_key:
        print("   âœ… OpenAI API ì—°ê²°ë¨ - AI ìš”ì•½ ê¸°ëŠ¥ í™œì„±í™”")
    else:
        print("   âš ï¸ OpenAI API ë¯¸ì„¤ì • - ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©")

    # ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ
    print("\nğŸ“– ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ:")
    print("   1. PubMed (ì˜í•™/ìƒë¬¼í•™) [ê¸°ë³¸ê°’]")
    print("   2. arXiv (CS/ë¬¼ë¦¬/ìˆ˜í•™)")
    print("   3. ë‘˜ ë‹¤")
    source_choice = input("ì„ íƒ [1]: ").strip() or "1"
    source = {'1': 'pubmed', '2': 'arxiv', '3': 'both'}.get(source_choice, 'pubmed')

    # í‚¤ì›Œë“œ ì…ë ¥
    keyword = input("\nğŸ” ë¶„ì„í•  í‚¤ì›Œë“œ ì…ë ¥: ").strip()
    if not keyword:
        keyword = "diabetes treatment"
        print(f"   ê¸°ë³¸ê°’ ì‚¬ìš©: {keyword}")

    # ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
    if detect_language(keyword) == 'ko':
        openai_api_key = os.getenv('OPENAI_API_KEY')
        print("\nğŸ”„ í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘...")
        keyword_en = translate_to_english(keyword, openai_api_key)
        print(f"   ğŸ‡°ğŸ‡· ì›ë³¸: {keyword}")
        print(f"   ğŸ‡ºğŸ‡¸ ë²ˆì—­: {keyword_en}")
        keyword = keyword_en

    # ë…¼ë¬¸ ìˆ˜ ì„¤ì •
    max_results = input("\nğŸ“„ ë¶„ì„í•  ìµœëŒ€ ë…¼ë¬¸ ìˆ˜ [50]: ").strip()
    max_results = int(max_results) if max_results.isdigit() else 50

    print("\n" + "-" * 60)
    print("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì„¤ì • ì™„ë£Œ!")
    print(f"   ğŸ” í‚¤ì›Œë“œ: {keyword}")
    print(f"   ğŸ“– ì†ŒìŠ¤: {source}")
    print(f"   ğŸ“„ ìµœëŒ€ ë…¼ë¬¸: {max_results}")
    print("-" * 60)

    # íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰
    analyzer = TrendAnalyzer(api_key=pubmed_api_key, email=pubmed_email, openai_api_key=openai_api_key)
    analyzer.analyze(keyword, max_results=max_results, source=source)

    # ì¶”ê°€ ë¶„ì„ ì—¬ë¶€
    while True:
        another = input("\nğŸ”„ ë‹¤ë¥¸ í‚¤ì›Œë“œ ë¶„ì„? (y/n) [n]: ").strip().lower()
        if another == 'y':
            keyword = input("ğŸ” ìƒˆ í‚¤ì›Œë“œ ì…ë ¥: ").strip()
            if keyword:
                if detect_language(keyword) == 'ko':
                    keyword = translate_to_english(keyword, os.getenv('OPENAI_API_KEY'))
                    print(f"   ğŸ‡ºğŸ‡¸ ë²ˆì—­: {keyword}")
                analyzer.analyze(keyword, max_results=max_results, source=source)
        else:
            break

    print("\n" + "=" * 60)
    print("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì¢…ë£Œ!")
    print("=" * 60)


# ==================== ë©”ì¸ ì‹¤í–‰ ====================
def main():
    print("=" * 60)
    print("ğŸš€ Medical/Scientific Paper RAG System")
    print("   with Paper Summarization & Multilingual Support")
    print("=" * 60)

    # ë©”ì¸ ë©”ë‰´
    print("\nğŸ“‹ ê¸°ëŠ¥ ì„ íƒ:")
    print("   1. RAG ì‹œìŠ¤í…œ (ë…¼ë¬¸ ê²€ìƒ‰ + ì§ˆì˜ì‘ë‹µ)")
    print("   2. íŠ¸ë Œë“œ ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜ ì—°êµ¬ ë™í–¥)")
    mode = input("ì„ íƒ [1]: ").strip() or "1"

    if mode == "2":
        # íŠ¸ë Œë“œ ë¶„ì„ ëª¨ë“œ
        run_trend_analysis()
        return

    # 1. ëŒ€í™”í˜• ì„¤ì •
    config = Config().interactive_setup()

    # 2. ë…¼ë¬¸ ê²€ìƒ‰ (ì˜ì–´ë¡œ ê²€ìƒ‰)
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: ë…¼ë¬¸ ê²€ìƒ‰")
    print("=" * 60)

    # í•œêµ­ì–´ì¸ ê²½ìš° ë²ˆì—­ëœ ì˜ì–´ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    search_query = config.search_query_en if config.search_query_en else config.search_query
    if config.language == 'ko' and config.search_query_en != config.search_query:
        print(f"   ğŸ‡°ğŸ‡· â†’ ğŸ‡ºğŸ‡¸ '{search_query}' (ìœ¼)ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤...")

    searcher = PaperSearcher(
        api_key=config.pubmed_api_key,
        email=config.pubmed_email
    )
    papers = searcher.search(
        query=search_query,
        source=config.search_source,
        max_results=config.max_results
    )

    print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ì´ {len(papers)}ê°œ ë…¼ë¬¸")

    if papers:
        print("\nğŸ“‹ ê²€ìƒ‰ëœ ë…¼ë¬¸ ëª©ë¡:\n")
        for i, paper in enumerate(papers, 1):
            print(f"[{i}] {paper['source']} | {paper['title'][:65]}...")
            print(f"    ì €ì: {', '.join(paper['authors'][:3])}")
            print(f"    ë°œí–‰: {paper['published']}")
            print()

    if not papers:
        print("âŒ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. PDF ë‹¤ìš´ë¡œë“œ
    print("\n" + "=" * 60)
    print("ğŸ“¥ Step 2: PDF ë‹¤ìš´ë¡œë“œ")
    print("=" * 60)

    downloader = PDFDownloader(PAPERS_DIR)
    downloaded_files = downloader.download_all(papers)

    if not downloaded_files:
        print("âŒ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 4. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    print("\n" + "=" * 60)
    print("ğŸ“„ Step 3: í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    print("=" * 60)

    documents = TextExtractor.extract_all(downloaded_files)

    if not documents:
        print("âŒ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 5. ë…¼ë¬¸ ìš”ì•½ (OpenAI API ìˆëŠ” ê²½ìš°)
    if config.openai_api_key:
        summarizer = PaperSummarizer(
            api_key=config.openai_api_key,
            language=config.language
        )
        papers = summarizer.summarize(papers, documents)

    # 6. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("\n" + "=" * 60)
    print("ğŸ§  Step 4: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ")
    print("=" * 60)

    embeddings = EmbeddingModelFactory.create(
        model_type=config.embedding_model,
        device='cpu',
        openai_api_key=config.openai_api_key
    )

    # 7. RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    print("\n" + "=" * 60)
    print("ğŸ’¾ Step 5: RAG ì‹œìŠ¤í…œ êµ¬ì¶•")
    print("=" * 60)

    # Text Splitter ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap,
        length_function=len
    )

    # Vector DB ì„ íƒì— ë”°ë¼ ë¶„ê¸°
    if config.vector_db == 'qdrant':
        # Qdrant ê¸°ë°˜ ì‹œìŠ¤í…œ
        print(f"\nğŸ—„ï¸ Qdrant Vector DB ì‚¬ìš©")

        qdrant_search = QdrantHybridSearch(
            embeddings=embeddings,
            sparse_method=config.sparse_method,
            collection_name="medical_papers",
            use_memory=True  # ì¸ë©”ëª¨ë¦¬ ëª¨ë“œ
        )
        qdrant_search.create_collection()
        qdrant_search.add_documents(documents, text_splitter)

        # RAG ì‹œìŠ¤í…œë„ ìƒì„± (interactive_qaìš©)
        rag = RAGSystem(
            embeddings=embeddings,
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            language=config.language
        )
        rag.build_vectorstore(documents)

        # 8. Hybrid Search ë¶„ì„ ë° ì‹œê°í™” (Qdrant)
        print("\n" + "=" * 60)
        print("ğŸ”€ Step 6: Qdrant Hybrid Search ë¶„ì„")
        print("=" * 60)

        test_query = search_query
        print(f"\nğŸ” ê²€ìƒ‰ì–´: '{test_query}'")
        print("-" * 40)

        # Qdrant ê²€ìƒ‰ ì‹¤í–‰
        sparse_results = qdrant_search.sparse_search(test_query, k=5)
        dense_results = qdrant_search.dense_search(test_query, k=5)
        hybrid_results = qdrant_search.hybrid_search(test_query, k=5, alpha=0.7)

        # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ sparse ë°©ì‹ (SPLADE ë¡œë“œ ì‹¤íŒ¨ ì‹œ BM25ë¡œ í´ë°±ë  ìˆ˜ ìˆìŒ)
        sparse_name = qdrant_search.get_sparse_method_name()

        # Sparse ê²°ê³¼
        print(f"\nğŸ”µ Qdrant Sparse Search ({sparse_name}) ê²°ê³¼:")
        for i, r in enumerate(sparse_results[:3], 1):
            score = r['score']
            source = r['source'][:50]
            print(f"   [{i}] {sparse_name}: {score:.4f} | {source}...")

        # Dense ê²°ê³¼ (HNSW)
        print(f"\nğŸ”´ Qdrant Dense Search (HNSW, {config.embedding_model}) ê²°ê³¼:")
        for i, r in enumerate(dense_results[:3], 1):
            score = r['score']
            source = r['source'][:50]
            print(f"   [{i}] Cosine: {score:.4f} | {source}...")

        # Hybrid ê²°ê³¼
        print(f"\nğŸŸ¢ Qdrant Hybrid Search ê²°ê³¼ ({sparse_name} + Dense, Î±=0.7):")
        for i, r in enumerate(hybrid_results[:3], 1):
            hybrid_score = r.get('hybrid_score', 0)
            sparse_score = r.get('sparse_score', 0)
            dense_score = r.get('dense_score', 0)
            source = r['source'][:50]
            print(f"   [{i}] Hybrid: {hybrid_score:.3f} | Sparse={sparse_score:.3f} + Dense={dense_score:.3f}")
            print(f"       {source}...")

        # Payload í•„í„°ë§ ì˜ˆì‹œ
        print(f"\nğŸ” Payload í•„í„°ë§ í…ŒìŠ¤íŠ¸:")
        filtered_results = qdrant_search.search_with_filter(
            test_query,
            min_chunk_length=100,
            k=3
        )
        print(f"   (ìµœì†Œ ì²­í¬ ê¸¸ì´ 100ì ì´ìƒ í•„í„°)")
        for i, r in enumerate(filtered_results[:3], 1):
            print(f"   [{i}] {r['source'][:50]}...")

        # ì‹œê°í™” ìƒì„±
        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        qdrant_search.visualize_comparison(
            query=test_query,
            sparse_results=sparse_results,
            dense_results=dense_results,
            hybrid_results=hybrid_results,
            alpha=0.7,
            sparse_method=sparse_name,
            dense_model=config.embedding_model
        )

    else:
        # FAISS ê¸°ë°˜ ì‹œìŠ¤í…œ (ê¸°ì¡´)
        print(f"\nğŸ—„ï¸ FAISS Vector DB ì‚¬ìš©")

        rag = RAGSystem(
            embeddings=embeddings,
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            language=config.language
        )

        vectorstore = rag.build_vectorstore(documents)
        rag.save_vectorstore(VECTORSTORE_DIR)

        # 8. Hybrid Search ë¶„ì„ ë° ì‹œê°í™” (FAISS)
        print("\n" + "=" * 60)
        print("ğŸ”€ Step 6: Hybrid Search ë¶„ì„")
        print("=" * 60)

        hybrid_searcher = HybridSearchSystem(rag, sparse_method=config.sparse_method)

        test_query = search_query
        print(f"\nğŸ” ê²€ìƒ‰ì–´: '{test_query}'")
        print("-" * 40)

        search_results = hybrid_searcher.compare_all(test_query, k=5, alpha=0.5)

        sparse_name = config.sparse_method.upper()

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ”µ Sparse Search ({sparse_name}) ê²°ê³¼:")
        for i, r in enumerate(search_results['sparse'][:3], 1):
            sparse_score = r['score']
            source = r['source'][:50]
            print(f"   [{i}] {sparse_name}: {sparse_score:.2f} | {source}...")

        print(f"\nğŸ”´ Dense Search ({config.embedding_model}) ê²°ê³¼:")
        for i, r in enumerate(search_results['dense'][:3], 1):
            l2_dist = r['score']
            source = r['source'][:50]
            print(f"   [{i}] L2 Dist: {l2_dist:.4f} | {source}...")

        print(f"\nğŸŸ¢ Hybrid Search ê²°ê³¼ ({sparse_name} + {config.embedding_model}, Î±=0.5):")
        for i, r in enumerate(search_results['hybrid'][:3], 1):
            sparse_raw = r.get('sparse_score', 0)
            sparse_norm = r.get('sparse_score_norm', 0)
            sem_norm = r.get('dense_score_norm', 0)
            hybrid_score = r.get('hybrid_score', 0)
            source = r['source'][:50]
            print(f"   [{i}] Hybrid: {hybrid_score:.2f} | {sparse_name}={sparse_raw:.1f}({sparse_norm:.2f}) + Semantic({sem_norm:.2f})")
            print(f"       {source}...")

        # ì‹œê°í™” ì €ì¥ ë° í‘œì‹œ
        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        hybrid_searcher.visualize_comparison(test_query, k=5, alpha=0.5)

    # 9. ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ
    interactive_qa(rag, config.openai_api_key)

    print("\n" + "=" * 60)
    print("âœ… RAG ì‹œìŠ¤í…œ ì¢…ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
