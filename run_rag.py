#!/usr/bin/env python3
"""
Medical/Scientific Paper RAG System
- ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ì§€ì›
- ë…¼ë¬¸ ìš”ì•½ ê¸°ëŠ¥ (OpenAI API)
- ë‹¤êµ­ì–´ ì§€ì› (ì˜ì–´/í•œêµ­ì–´)
- .env íŒŒì¼ì„ í†µí•œ API í‚¤ ê´€ë¦¬
"""
#!pip install langchain langchain-community langchain-text-splitters langchain-openai faiss-cpu

import os
import requests
import time
import re
from pathlib import Path
from typing import List, Dict, Optional

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ë…¼ë¬¸ ê²€ìƒ‰
import arxiv
import xmltodict
# PDF ì²˜ë¦¬
from PyPDF2 import PdfReader
import pdfplumber

# LangChain ê´€ë ¨
from langchain_text_splitters import RecursiveCharacterTextSplitter
#from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
import warnings
warnings.filterwarnings('ignore')

# ë””ë ‰í† ë¦¬ ì„¤ì •
PAPERS_DIR = "./papers"
VECTORSTORE_DIR = "./vectorstore"
os.makedirs(PAPERS_DIR, exist_ok=True)
os.makedirs(VECTORSTORE_DIR, exist_ok=True)


# ==================== ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ====================
def detect_language(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€ (í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´)"""
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    total_chars = len(re.findall(r'[a-zA-Zê°€-í£]', text))

    if total_chars == 0:
        return 'en'

    korean_ratio = korean_chars / total_chars
    return 'ko' if korean_ratio > 0.3 else 'en'


def translate_to_english(text: str, openai_api_key: str = None) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ (ê²€ìƒ‰ìš©)"""
    if not openai_api_key:
        # API í‚¤ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì˜í•™ ìš©ì–´ ë§¤í•‘ ì‚¬ìš©
        medical_terms = {
            'ë‹¹ë‡¨ë³‘': 'diabetes mellitus',
            'ë‹¹ë‡¨': 'diabetes',
            'ì¹˜ë£Œ': 'treatment',
            'ì¹˜ë£Œë²•': 'treatment therapy',
            'ì•”': 'cancer',
            'íì•”': 'lung cancer',
            'ìœ ë°©ì•”': 'breast cancer',
            'ìœ„ì•”': 'gastric cancer stomach cancer',
            'ê°„ì•”': 'liver cancer hepatocellular carcinoma',
            'ëŒ€ì¥ì•”': 'colon cancer colorectal cancer',
            'ê³ í˜ˆì••': 'hypertension',
            'ì‹¬ì¥ë³‘': 'heart disease cardiovascular disease',
            'ë‡Œì¡¸ì¤‘': 'stroke cerebrovascular accident',
            'ì¹˜ë§¤': 'dementia alzheimer',
            'ìš°ìš¸ì¦': 'depression',
            'ë¹„ë§Œ': 'obesity',
            'ê³¨ë‹¤ê³µì¦': 'osteoporosis',
            'ê´€ì ˆì—¼': 'arthritis',
            'ì²œì‹': 'asthma',
            'ì•Œë ˆë¥´ê¸°': 'allergy',
            'ê°ì—¼': 'infection',
            'ë°”ì´ëŸ¬ìŠ¤': 'virus viral',
            'ë°±ì‹ ': 'vaccine vaccination',
            'í•­ìƒì œ': 'antibiotic',
            'ë©´ì—­': 'immunity immune',
            'ì§„ë‹¨': 'diagnosis diagnostic',
            'ì˜ˆë°©': 'prevention preventive',
            'ì¦ìƒ': 'symptoms',
            'ë¶€ì‘ìš©': 'side effects adverse effects',
            'ì„ìƒì‹œí—˜': 'clinical trial',
            'ì•½ë¬¼': 'drug medication',
            'ìˆ˜ìˆ ': 'surgery surgical',
            'ë°©ì‚¬ì„ ': 'radiation radiotherapy',
            'í™”í•™ìš”ë²•': 'chemotherapy',
        }

        translated = text
        for ko, en in medical_terms.items():
            if ko in translated:
                translated = translated.replace(ko, en)

        # ë‚¨ì€ í•œê¸€ ì œê±°
        translated = re.sub(r'[ê°€-í£]+', '', translated).strip()
        return translated if translated else text

    try:
        from openai import OpenAI
        client = OpenAI(api_key=openai_api_key)

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a medical/scientific translator. Translate the following Korean medical query to English. Only output the English translation, nothing else."},
                {"role": "user", "content": text}
            ],
            max_tokens=100,
            temperature=0.1
        )

        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"   âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©: {str(e)[:30]}")
        return translate_to_english(text, None)  # API ì—†ì´ ì¬ì‹œë„


# ==================== ì„¤ì • í´ë˜ìŠ¤ ====================
class Config:
    """RAG ì‹œìŠ¤í…œ ì„¤ì •"""
    def __init__(self):
        self.search_source = 'pubmed'
        self.search_query = ''  # ì›ë³¸ ê²€ìƒ‰ì–´ (í•œêµ­ì–´ ê°€ëŠ¥)
        self.search_query_en = ''  # ì˜ì–´ë¡œ ë²ˆì—­ëœ ê²€ìƒ‰ì–´ (ì‹¤ì œ ê²€ìƒ‰ìš©)
        self.max_results = 5
        self.embedding_model = 'pubmedbert'  # ê¸°ë³¸ê°’ì„ PubMedBERTë¡œ ë³€ê²½
        self.sparse_method = 'bm25'  # 'bm25' ë˜ëŠ” 'splade'
        self.vector_db = 'faiss'  # 'faiss' ë˜ëŠ” 'qdrant'
        self.chunk_size = 1000
        self.chunk_overlap = 200
        # .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
        self.pubmed_api_key = os.getenv('PUBMED_API_KEY') or None
        self.pubmed_email = os.getenv('PUBMED_EMAIL') or None
        self.openai_api_key = os.getenv('OPENAI_API_KEY') or None
        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY') or None
        self.language = 'en'  # ê°ì§€ëœ ì–¸ì–´

    def interactive_setup(self):
        """ëŒ€í™”í˜• ì„¤ì •"""
        print("\n" + "=" * 60)
        print("âš™ï¸  RAG ì‹œìŠ¤í…œ ì„¤ì •")
        print("=" * 60)

        # ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ
        print("\nğŸ“– ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ:")
        print("   1. PubMed (ì˜í•™/ìƒë¬¼í•™)")
        print("   2. arXiv (CS/ë¬¼ë¦¬/ìˆ˜í•™)")
        print("   3. ë‘˜ ë‹¤")
        choice = input("ì„ íƒ [1]: ").strip() or "1"
        self.search_source = {'1': 'pubmed', '2': 'arxiv', '3': 'both'}.get(choice, 'pubmed')

        # ê²€ìƒ‰ì–´ ì…ë ¥
        self.search_query = input("\nğŸ” ê²€ìƒ‰ì–´ ì…ë ¥: ").strip()
        if not self.search_query:
            self.search_query = "COVID-19 vaccine efficacy"
            print(f"   ê¸°ë³¸ê°’ ì‚¬ìš©: {self.search_query}")

        # ì–¸ì–´ ê°ì§€
        self.language = detect_language(self.search_query)
        lang_name = "í•œêµ­ì–´" if self.language == 'ko' else "English"
        print(f"   ğŸŒ ê°ì§€ëœ ì–¸ì–´: {lang_name}")

        # ìµœëŒ€ ê²°ê³¼ ìˆ˜
        max_res = input(f"\nğŸ“„ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜ [{self.max_results}]: ").strip()
        if max_res.isdigit():
            self.max_results = int(max_res)

        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
        print("\nğŸ§  Dense ì„ë² ë”© ëª¨ë¸ ì„ íƒ:")
        print("   [PubMed/ì˜í•™ íŠ¹í™” - ë¡œì»¬ ì‹¤í–‰]")
        print("   1. PubMedBERT Full (PubMed ì „ë¬¸ í•™ìŠµ) [ê¶Œì¥]")
        print("   2. PubMedBERT Abstract (PubMed ì´ˆë¡ íŠ¹í™”)")
        print("   3. BioBERT (ì˜í•™/ìƒë¬¼í•™)")
        print("   4. BioLinkBERT (ì˜í•™ ë¬¸í—Œ ë§í¬ í•™ìŠµ)")
        print("   5. SciBERT (ê³¼í•™ ë…¼ë¬¸ ì „ë°˜)")
        print("   [ì¼ë°˜ ëª¨ë¸]")
        print("   6. BERT-base (ì¼ë°˜)")
        print("   [OpenAI API - ë¹ ë¥´ê³  ì •í™•]")
        print("   7. OpenAI Small (ë¹ ë¦„, ì €ë ´)")
        print("   8. OpenAI Large (ê³ ì„±ëŠ¥)")
        model_choice = input("ì„ íƒ [1 - PubMedBERT]: ").strip() or "1"
        self.embedding_model = {
            '1': 'pubmedbert',
            '2': 'pubmedbert-abs',
            '3': 'biobert',
            '4': 'biolinkbert',
            '5': 'scibert',
            '6': 'bert-base',
            '7': 'openai-small',
            '8': 'openai-large'
        }.get(model_choice, 'pubmedbert')

        # Sparse ê²€ìƒ‰ ë°©ì‹ ì„ íƒ
        print("\nğŸ” Sparse ê²€ìƒ‰ ë°©ì‹ ì„ íƒ:")
        print("   1. BM25 (ì „í†µì  í‚¤ì›Œë“œ ë§¤ì¹­) [ê¸°ë³¸ê°’]")
        print("   2. SPLADE (ì‹ ê²½ë§ ê¸°ë°˜ í™•ì¥ ê²€ìƒ‰)")
        sparse_choice = input("ì„ íƒ [1 - BM25]: ").strip() or "1"
        self.sparse_method = {
            '1': 'bm25',
            '2': 'splade'
        }.get(sparse_choice, 'bm25')

        # Vector DB ì„ íƒ
        print("\nğŸ—„ï¸ Vector DB ì„ íƒ:")
        print("   1. FAISS (Facebook AI, ë¡œì»¬ íŒŒì¼ ì €ì¥) [ê¸°ë³¸ê°’]")
        print("   2. Qdrant (Named Vectors, HNSW ì¸ë±ìŠ¤, Payload í•„í„°ë§)")
        db_choice = input("ì„ íƒ [1 - FAISS]: ").strip() or "1"
        self.vector_db = {
            '1': 'faiss',
            '2': 'qdrant'
        }.get(db_choice, 'faiss')

        # PubMed API ì„¤ì •
        if self.search_source in ['pubmed', 'both']:
            print("\nğŸ”‘ PubMed API ì„¤ì • (ì„ íƒì‚¬í•­ - ì†ë„ í–¥ìƒ):")
            if self.pubmed_api_key:
                print(f"   âœ… .envì—ì„œ ë¡œë“œë¨ (Key: {self.pubmed_api_key[:8]}...)")
            else:
                print("   API í‚¤ê°€ ì—†ìœ¼ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
                print("   ë°œê¸‰: https://www.ncbi.nlm.nih.gov/account/settings/")
                self.pubmed_api_key = input("   API Key: ").strip() or None
                if self.pubmed_api_key:
                    self.pubmed_email = input("   Email: ").strip() or None

        # OpenAI API ì„¤ì • (ë…¼ë¬¸ ìš”ì•½ ë° ë²ˆì—­ìš©)
        print("\nğŸ¤– OpenAI API ì„¤ì • (ë…¼ë¬¸ ìš”ì•½ ë° í•œâ†’ì˜ ë²ˆì—­ìš©):")
        if self.openai_api_key:
            print(f"   âœ… .envì—ì„œ ë¡œë“œë¨ (Key: {self.openai_api_key[:12]}...)")
        else:
            print("   API í‚¤ê°€ ì—†ìœ¼ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            print("   ë°œê¸‰: https://platform.openai.com/api-keys")
            self.openai_api_key = input("   OpenAI API Key: ").strip() or None

        # í•œêµ­ì–´ ê²€ìƒ‰ì–´ì¸ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­
        if self.language == 'ko':
            print("\nğŸ”„ í•œêµ­ì–´ ê²€ìƒ‰ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘...")
            self.search_query_en = translate_to_english(self.search_query, self.openai_api_key)
            print(f"   ğŸ‡°ğŸ‡· ì›ë³¸: {self.search_query}")
            print(f"   ğŸ‡ºğŸ‡¸ ë²ˆì—­: {self.search_query_en}")
        else:
            self.search_query_en = self.search_query

        print("\n" + "-" * 60)
        print("âœ… ì„¤ì • ì™„ë£Œ!")
        print(f"   ğŸ“– ì†ŒìŠ¤: {self.search_source}")
        print(f"   ğŸ” ê²€ìƒ‰ì–´: {self.search_query}")
        if self.language == 'ko':
            print(f"   ğŸ” ê²€ìƒ‰ì–´(ì˜ë¬¸): {self.search_query_en}")
        print(f"   ğŸŒ ì–¸ì–´: {lang_name} (ì‘ë‹µë„ {lang_name}ë¡œ)")
        print(f"   ğŸ“„ ìµœëŒ€ ë…¼ë¬¸: {self.max_results}")
        print(f"   ğŸ§  Dense ëª¨ë¸: {self.embedding_model}")
        print(f"   ğŸ”¤ Sparse ë°©ì‹: {self.sparse_method.upper()}")
        print(f"   ğŸ—„ï¸ Vector DB: {self.vector_db.upper()}")
        if self.pubmed_api_key:
            print(f"   ğŸ”‘ PubMed API: ì„¤ì •ë¨")
        if self.openai_api_key:
            print(f"   ğŸ¤– OpenAI API: ì„¤ì •ë¨ (ìš”ì•½/ë²ˆì—­ í™œì„±í™”)")
        print("-" * 60)

        return self


# ==================== ë…¼ë¬¸ ìš”ì•½ í´ë˜ìŠ¤ ====================
class PaperSummarizer:
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ìš”ì•½"""

    def __init__(self, api_key: str = None, language: str = 'en'):
        self.api_key = api_key
        self.language = language

    def summarize(self, papers: List[Dict], documents: List[Dict]) -> List[Dict]:
        """ë…¼ë¬¸ë“¤ì„ ìš”ì•½"""
        if not self.api_key:
            print("\nâš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return papers

        print("\n" + "=" * 60)
        print("ğŸ“ ë…¼ë¬¸ ìš”ì•½ ì¤‘... (OpenAI API ì‚¬ìš©)")
        print("=" * 60)

        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.api_key)
        except ImportError:
            print("âš ï¸ OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai")
            return papers
        except Exception as e:
            print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:50]}")
            return papers

        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸
        if self.language == 'ko':
            system_prompt = """ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì„ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë…¼ë¬¸ì˜ ì œëª©, ì €ì, ì´ˆë¡, ë³¸ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:
- ì—°êµ¬ ëª©ì 
- ì£¼ìš” ë°©ë²•
- í•µì‹¬ ê²°ê³¼
- ê²°ë¡  ë° ì˜ì˜"""
        else:
            system_prompt = """You are an expert at summarizing medical/scientific papers.
Based on the title, authors, abstract, and content, provide a concise summary.
Follow this format:
- Research Objective
- Key Methods
- Main Results
- Conclusion & Significance"""

        summarized_papers = []

        for i, paper in enumerate(papers):
            print(f"\n   [{i+1}/{len(papers)}] {paper['title'][:50]}...")

            # í•´ë‹¹ ë…¼ë¬¸ì˜ ë³¸ë¬¸ ì°¾ê¸°
            paper_content = ""
            for doc in documents:
                if paper['id'] in doc['source']:
                    paper_content = doc['text'][:3000]  # í† í° ì œí•œ
                    break

            # ìš”ì•½í•  ë‚´ìš© êµ¬ì„±
            content_to_summarize = f"""
Title: {paper['title']}
Authors: {', '.join(paper['authors'][:5])}
Published: {paper['published']}
Source: {paper['source']}

Abstract:
{paper['abstract']}

Content:
{paper_content}
"""

            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": content_to_summarize}
                    ],
                    max_tokens=500,
                    temperature=0.3
                )

                summary = response.choices[0].message.content
                paper['summary'] = summary
                print(f"      âœ… ìš”ì•½ ì™„ë£Œ")

            except Exception as e:
                print(f"      âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {str(e)[:50]}")
                paper['summary'] = paper['abstract'][:500] + "..."

            summarized_papers.append(paper)
            time.sleep(0.5)  # API ì†ë„ ì œí•œ

        # ìš”ì•½ ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“‹ ë…¼ë¬¸ ìš”ì•½ ê²°ê³¼")
        print("=" * 60)

        for i, paper in enumerate(summarized_papers):
            print(f"\n[{i+1}] {paper['title'][:60]}...")
            print("-" * 40)
            print(paper.get('summary', 'No summary available'))
            print("-" * 40)

        input("\nâ Enterë¥¼ ëˆŒëŸ¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰...")

        return summarized_papers


# ==================== PaperSearcher í´ë˜ìŠ¤ ====================
class PaperSearcher:
    def __init__(self, api_key: str = None, email: str = None):
        self.papers = []
        self.api_key = api_key
        self.email = email

    def search_arxiv(self, query: str, max_results: int = 5) -> List[Dict]:
        print(f"\nğŸ” arXivì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")

        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance
        )

        papers = []
        for result in search.results():
            paper = {
                'source': 'arXiv',
                'title': result.title,
                'authors': [author.name for author in result.authors],
                'abstract': result.summary,
                'pdf_url': result.pdf_url,
                'published': result.published.strftime('%Y-%m-%d'),
                'id': result.entry_id.split('/')[-1]
            }
            papers.append(paper)
            print(f"   ğŸ“„ {paper['title'][:60]}...")

        print(f"   âœ… arXiv: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
        return papers

    def search_pubmed(self, query: str, max_results: int = 5) -> List[Dict]:
        print(f"\nğŸ” PubMedì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")

        search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        search_params = {
            'db': 'pubmed',
            'term': query,
            'retmax': max_results,
            'retmode': 'json',
            'sort': 'relevance'
        }

        if self.api_key:
            search_params['api_key'] = self.api_key
            print("   ğŸ”‘ API í‚¤ ì‚¬ìš© ì¤‘...")
        if self.email:
            search_params['email'] = self.email

        try:
            response = requests.get(search_url, params=search_params, timeout=30)
            response.raise_for_status()
            search_data = response.json()
            pmids = search_data.get('esearchresult', {}).get('idlist', [])

            if not pmids:
                print("   âš ï¸ PubMed: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return []

            fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
            fetch_params = {
                'db': 'pubmed',
                'id': ','.join(pmids),
                'retmode': 'xml'
            }
            if self.api_key:
                fetch_params['api_key'] = self.api_key

            if not self.api_key:
                time.sleep(0.35)

            response = requests.get(fetch_url, params=fetch_params, timeout=30)
            response.raise_for_status()

            data = xmltodict.parse(response.content)
            articles = data.get('PubmedArticleSet', {}).get('PubmedArticle', [])

            if isinstance(articles, dict):
                articles = [articles]

            papers = []
            for article in articles:
                try:
                    medline = article.get('MedlineCitation', {})
                    article_data = medline.get('Article', {})

                    title = article_data.get('ArticleTitle', 'No Title')
                    if isinstance(title, dict):
                        title = title.get('#text', 'No Title')

                    abstract_data = article_data.get('Abstract', {}).get('AbstractText', '')
                    if isinstance(abstract_data, list):
                        abstract = ' '.join([a.get('#text', str(a)) if isinstance(a, dict) else str(a) for a in abstract_data])
                    elif isinstance(abstract_data, dict):
                        abstract = abstract_data.get('#text', str(abstract_data))
                    else:
                        abstract = str(abstract_data) if abstract_data else 'No abstract available'

                    author_list = article_data.get('AuthorList', {}).get('Author', [])
                    if isinstance(author_list, dict):
                        author_list = [author_list]
                    authors = []
                    for author in author_list[:5]:
                        if isinstance(author, dict):
                            last = author.get('LastName', '')
                            first = author.get('ForeName', '')
                            if last:
                                authors.append(f"{last} {first}".strip())

                    pmid = medline.get('PMID', {}).get('#text', 'Unknown')
                    pub_date = article_data.get('Journal', {}).get('JournalIssue', {}).get('PubDate', {})
                    year = pub_date.get('Year', 'Unknown')

                    paper = {
                        'source': 'PubMed',
                        'title': title,
                        'authors': authors if authors else ['Unknown'],
                        'abstract': abstract,
                        'pdf_url': None,
                        'pubmed_url': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                        'pmc_url': None,
                        'published': str(year),
                        'id': f"PMID_{pmid}"
                    }
                    papers.append(paper)
                    print(f"   ğŸ“„ {paper['title'][:60]}...")

                except Exception as e:
                    continue

            print(f"   âœ… PubMed: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
            return papers

        except Exception as e:
            print(f"   âŒ PubMed ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []

    def search(self, query: str, source: str = 'both', max_results: int = 5) -> List[Dict]:
        papers = []

        if source in ['arxiv', 'both']:
            papers.extend(self.search_arxiv(query, max_results))

        if source in ['pubmed', 'both']:
            papers.extend(self.search_pubmed(query, max_results))

        self.papers = papers
        return papers


# ==================== PDFDownloader í´ë˜ìŠ¤ ====================
class PDFDownloader:
    def __init__(self, save_dir: str = PAPERS_DIR):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def download(self, paper: Dict) -> Optional[str]:
        safe_title = re.sub(r'[^\w\s-]', '', paper['title'])[:50]
        filename = f"{paper['id']}_{safe_title}.pdf"
        filepath = os.path.join(self.save_dir, filename)
        txt_filepath = filepath.replace('.pdf', '.txt')

        if os.path.exists(txt_filepath):
            print(f"   â­ï¸ ì´ë¯¸ ì¡´ì¬: {os.path.basename(txt_filepath)[:40]}...")
            return txt_filepath
        if os.path.exists(filepath):
            print(f"   â­ï¸ ì´ë¯¸ ì¡´ì¬: {filename[:40]}...")
            return filepath

        pdf_url = paper.get('pdf_url') or paper.get('pmc_url')

        if not pdf_url:
            if paper['source'] == 'PubMed':
                return self._save_abstract_as_text(paper, filename)
            print(f"   âš ï¸ PDF URL ì—†ìŒ: {paper['title'][:40]}...")
            return None

        try:
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; ResearchBot/1.0)'}
            response = requests.get(pdf_url, headers=headers, timeout=60)
            response.raise_for_status()

            if response.content[:5] == b'<html' or response.content[:5] == b'<!DOC':
                print(f"   âš ï¸ PDF ì ‘ê·¼ ë¶ˆê°€, ì´ˆë¡ ì €ì¥: {paper['title'][:30]}...")
                return self._save_abstract_as_text(paper, filename)

            with open(filepath, 'wb') as f:
                f.write(response.content)

            print(f"   âœ… ë‹¤ìš´ë¡œë“œ: {filename[:40]}...")
            return filepath

        except Exception as e:
            print(f"   âš ï¸ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ì´ˆë¡ ì €ì¥: {paper['title'][:30]}...")
            return self._save_abstract_as_text(paper, filename)

    def _save_abstract_as_text(self, paper: Dict, filename: str) -> Optional[str]:
        txt_filename = filename.replace('.pdf', '.txt')
        filepath = os.path.join(self.save_dir, txt_filename)

        content = f"""Title: {paper['title']}

Authors: {', '.join(paper['authors'])}

Source: {paper['source']}

Published: {paper['published']}

Abstract:
{paper['abstract']}

URL: {paper.get('pubmed_url', paper.get('pdf_url', 'N/A'))}
"""

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"   ğŸ“ ì´ˆë¡ ì €ì¥: {txt_filename[:40]}...")
        return filepath

    def download_all(self, papers: List[Dict]) -> List[str]:
        print("\nğŸ“¥ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\n")

        downloaded = []
        for paper in papers:
            filepath = self.download(paper)
            if filepath:
                downloaded.append(filepath)
            time.sleep(0.3)

        print(f"\nğŸ“ ì´ {len(downloaded)}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        return downloaded


# ==================== TextExtractor í´ë˜ìŠ¤ ====================
class TextExtractor:
    @staticmethod
    def extract(filepath: str) -> str:
        if filepath.endswith('.txt'):
            return TextExtractor._extract_from_txt(filepath)
        elif filepath.endswith('.pdf'):
            return TextExtractor._extract_from_pdf(filepath)
        return ""

    @staticmethod
    def _extract_from_txt(filepath: str) -> str:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            return ""

    @staticmethod
    def _extract_from_pdf(filepath: str) -> str:
        text = ""
        try:
            reader = PdfReader(filepath)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"

            if len(text) < 100:
                text = ""
                with pdfplumber.open(filepath) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
        except Exception as e:
            print(f"      âš ï¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)[:50]}")

        return text.strip()

    @staticmethod
    def extract_all(filepaths: List[str]) -> List[Dict]:
        print("\nğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\n")

        documents = []
        for filepath in filepaths:
            filename = os.path.basename(filepath)
            print(f"   ğŸ“– {filename[:50]}...")

            text = TextExtractor.extract(filepath)
            if text:
                documents.append({
                    'text': text,
                    'source': filename,
                    'filepath': filepath
                })
                print(f"      âœ… {len(text):,} ê¸€ì ì¶”ì¶œ")
            else:
                print(f"      âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ")

        print(f"\nğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
        return documents


# ==================== ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤ (sentence-transformers ì—†ì´) ====================
from langchain_core.embeddings import Embeddings

os.environ["SAFETENSORS_FAST_GPU"] = "1"


class HuggingFaceEmbeddings(Embeddings):
    """HuggingFace Transformersë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© í´ë˜ìŠ¤ (sentence-transformers ë¶ˆí•„ìš”)"""

    def __init__(self, model_name: str, device: str = 'cpu'):
        import torch
        from transformers import AutoTokenizer, AutoModel

        self.device = device
        self.model_name = model_name

        print(f"   ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()

    def _mean_pooling(self, model_output, attention_mask):
        """Mean pooling - attention maskë¥¼ ê³ ë ¤í•œ í‰ê· """
        import torch
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def _encode(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        import torch
        import torch.nn.functional as F

        # í† í°í™”
        encoded_input = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            model_output = self.model(**encoded_input)

        # Mean pooling
        embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])

        # ì •ê·œí™”
        embeddings = F.normalize(embeddings, p=2, dim=1)

        return embeddings.cpu().numpy().tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”©"""
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = 8
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            embeddings = self._encode(batch)
            all_embeddings.extend(embeddings)
            if len(texts) > batch_size:
                print(f"   ì§„í–‰: {min(i+batch_size, len(texts))}/{len(texts)}", end='\r')

        if len(texts) > batch_size:
            print()
        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        return self._encode([text])[0]


class OpenAIEmbeddings(Embeddings):
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© í´ë˜ìŠ¤"""

    def __init__(self, api_key: str, model: str = "text-embedding-3-small"):
        from openai import OpenAI
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.dimension = 1536 if "small" in model else 3072

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”©"""
        all_embeddings = []
        batch_size = 100  # OpenAI ë°°ì¹˜ ì œí•œ

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            response = self.client.embeddings.create(
                model=self.model,
                input=batch
            )
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)
            if len(texts) > batch_size:
                print(f"   ì§„í–‰: {min(i+batch_size, len(texts))}/{len(texts)}", end='\r')

        if len(texts) > batch_size:
            print()
        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        response = self.client.embeddings.create(
            model=self.model,
            input=[text]
        )
        return response.data[0].embedding


# ==================== EmbeddingModelFactory í´ë˜ìŠ¤ ====================
class EmbeddingModelFactory:
    """ì„ë² ë”© ëª¨ë¸ íŒ©í† ë¦¬ - HuggingFace ë˜ëŠ” OpenAI ì„ íƒ ê°€ëŠ¥"""

    MODELS = {
        # PubMed/ì˜í•™ íŠ¹í™” ëª¨ë¸
        'pubmedbert': {
            'name': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',
            'description': 'PubMed ë…¼ë¬¸ íŠ¹í™” (PubMedBERT Full)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'pubmedbert-abs': {
            'name': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',
            'description': 'PubMed ì´ˆë¡ íŠ¹í™” (PubMedBERT Abstract)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'biobert': {
            'name': 'dmis-lab/biobert-base-cased-v1.2',
            'description': 'ì˜í•™/ìƒë¬¼í•™ íŠ¹í™” (BioBERT v1.2)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'scibert': {
            'name': 'allenai/scibert_scivocab_uncased',
            'description': 'ê³¼í•™ ë…¼ë¬¸ íŠ¹í™” (SciBERT)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'biolinkbert': {
            'name': 'michiyasunaga/BioLinkBERT-base',
            'description': 'ì˜í•™ ë¬¸í—Œ ë§í¬ í•™ìŠµ (BioLinkBERT)',
            'dimension': 768,
            'type': 'huggingface'
        },
        # ì¼ë°˜ ëª¨ë¸
        'bert-base': {
            'name': 'bert-base-uncased',
            'description': 'BERT ê¸°ë³¸ ëª¨ë¸',
            'dimension': 768,
            'type': 'huggingface'
        },
        # OpenAI API ëª¨ë¸
        'openai-small': {
            'name': 'text-embedding-3-small',
            'description': 'OpenAI ì„ë² ë”© (ë¹ ë¦„, ì €ë ´)',
            'dimension': 1536,
            'type': 'openai'
        },
        'openai-large': {
            'name': 'text-embedding-3-large',
            'description': 'OpenAI ì„ë² ë”© (ê³ ì„±ëŠ¥)',
            'dimension': 3072,
            'type': 'openai'
        }
    }

    @classmethod
    def create(cls, model_type: str = 'biobert', device: str = 'cpu', openai_api_key: str = None):
        if model_type not in cls.MODELS:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_type}. 'biobert' ì‚¬ìš©")
            model_type = 'biobert'

        model_info = cls.MODELS[model_type]

        print(f"\nğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"   ëª¨ë¸: {model_type}")
        print(f"   ì„¤ëª…: {model_info['description']}")
        print(f"   ì°¨ì›: {model_info['dimension']}")

        # OpenAI ëª¨ë¸ì¸ ê²½ìš°
        if model_info['type'] == 'openai':
            if not openai_api_key:
                print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. biobertë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return cls.create('biobert', device, None)

            try:
                embeddings = OpenAIEmbeddings(api_key=openai_api_key, model=model_info['name'])
                print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                return embeddings
            except Exception as e:
                print(f"âš ï¸ OpenAI ì„ë² ë”© ì‹¤íŒ¨: {str(e)[:50]}")
                print("   biobertë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
                return cls.create('biobert', device, None)

        # HuggingFace ëª¨ë¸ì¸ ê²½ìš°
        try:
            embeddings = HuggingFaceEmbeddings(model_name=model_info['name'], device=device)
            print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            return embeddings

        except Exception as e:
            print(f"âš ï¸ {model_type} ë¡œë“œ ì‹¤íŒ¨: {str(e)[:100]}")
            print("   bert-base ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")

            try:
                return HuggingFaceEmbeddings(
                    model_name='bert-base-uncased',
                    device=device
                )
            except Exception as e2:
                print(f"âš ï¸ bert-baseë„ ì‹¤íŒ¨: {str(e2)[:50]}")
                print("   OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
                if openai_api_key:
                    return OpenAIEmbeddings(api_key=openai_api_key)
                raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")


# ==================== RAGSystem í´ë˜ìŠ¤ ====================
class RAGSystem:
    def __init__(self, embeddings, chunk_size: int = 1000, chunk_overlap: int = 200, language: str = 'en'):
        self.embeddings = embeddings
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.vectorstore = None
        self.language = language
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def build_vectorstore(self, documents: List[Dict]) -> FAISS:
        print("\nâœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")

        all_chunks = []
        all_metadata = []

        for doc in documents:
            chunks = self.text_splitter.split_text(doc['text'])
            for i, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                all_metadata.append({
                    'source': doc['source'],
                    'chunk_id': i
                })
            print(f"   ğŸ“„ {doc['source'][:40]}...: {len(chunks)} ì²­í¬")

        print(f"\nğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}ê°œ")

        if not all_chunks:
            raise ValueError("ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")

        print("\nğŸ’¾ ë²¡í„° DB ìƒì„± ì¤‘...")

        self.vectorstore = FAISS.from_texts(
            texts=all_chunks,
            embedding=self.embeddings,
            metadatas=all_metadata
        )

        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ!")
        return self.vectorstore

    def save_vectorstore(self, path: str = VECTORSTORE_DIR):
        if self.vectorstore:
            self.vectorstore.save_local(path)
            print(f"ğŸ’¾ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {path}")

    def search(self, query: str, k: int = 3) -> List[Dict]:
        if not self.vectorstore:
            print("âŒ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)

        results = []
        for doc, score in docs_with_scores:
            results.append({
                'content': doc.page_content,
                'source': doc.metadata.get('source', 'Unknown'),
                'score': float(score)
            })

        return results

    def answer(self, question: str, k: int = 3) -> Dict:
        # ì§ˆë¬¸ ì–¸ì–´ ê°ì§€
        q_language = detect_language(question)

        results = self.search(question, k=k)

        return {
            'question': question,
            'contexts': results,
            'sources': list(set([r['source'] for r in results])),
            'language': q_language
        }

    def get_all_chunks(self) -> List[Dict]:
        """ì €ì¥ëœ ëª¨ë“  ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜"""
        if not self.vectorstore:
            return []

        # FAISSì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        docstore = self.vectorstore.docstore
        index_to_id = self.vectorstore.index_to_docstore_id

        chunks = []
        for idx, doc_id in index_to_id.items():
            doc = docstore.search(doc_id)
            if doc:
                chunks.append({
                    'content': doc.page_content,
                    'source': doc.metadata.get('source', 'Unknown'),
                    'chunk_id': doc.metadata.get('chunk_id', idx)
                })
        return chunks


# ==================== Qdrant Hybrid Search ì‹œìŠ¤í…œ ====================
class QdrantHybridSearch:
    """
    Qdrant ê¸°ë°˜ Hybrid Search ì‹œìŠ¤í…œ
    - Named Vectors: dense + sparse ë™ì‹œ ì €ì¥
    - Payload Filtering: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§
    - HNSW Index: ë¹ ë¥¸ ANN ê²€ìƒ‰
    """

    def __init__(
        self,
        embeddings,
        sparse_encoder=None,
        collection_name: str = "papers",
        use_memory: bool = True,
        qdrant_url: str = None,
        sparse_method: str = 'bm25'
    ):
        from qdrant_client import QdrantClient
        from qdrant_client.models import (
            VectorParams, SparseVectorParams, Distance,
            HnswConfigDiff, OptimizersConfigDiff
        )

        self.embeddings = embeddings
        self.collection_name = collection_name
        self.sparse_method = sparse_method.lower()
        self.chunks = []
        self.splade_encoder = None

        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if use_memory:
            print("\nğŸ—„ï¸ Qdrant ì¸ë©”ëª¨ë¦¬ ëª¨ë“œë¡œ ì´ˆê¸°í™”...")
            self.client = QdrantClient(":memory:")
        else:
            print(f"\nğŸ—„ï¸ Qdrant ì„œë²„ ì—°ê²°: {qdrant_url}")
            self.client = QdrantClient(url=qdrant_url)

        # Dense ë²¡í„° ì°¨ì› í™•ì¸
        test_embedding = self.embeddings.embed_query("test")
        self.dense_dim = len(test_embedding)
        print(f"   ğŸ“ Dense ë²¡í„° ì°¨ì›: {self.dense_dim}")

        # Sparse ì¸ì½”ë” ì´ˆê¸°í™”
        if self.sparse_method == 'splade':
            self._init_splade_encoder(sparse_encoder)
        else:
            # BM25ìš© ìŠ¤í…Œë¨¸ ì´ˆê¸°í™”
            self._init_stemmer()
            print(f"   ğŸ”¤ Sparse ë°©ì‹: BM25 (ìŠ¤í…Œë°)")

    def _init_splade_encoder(self, sparse_encoder=None):
        """SPLADE ì¸ì½”ë” ì´ˆê¸°í™”"""
        if sparse_encoder is not None:
            self.splade_encoder = sparse_encoder
            print(f"   ğŸ”¤ Sparse ë°©ì‹: SPLADE (ì™¸ë¶€ ì¸ì½”ë”)")
        else:
            try:
                print(f"   ğŸ”¤ SPLADE ì¸ì½”ë” ë¡œë”© ì¤‘...")
                self.splade_encoder = SPLADEEncoder(device='cpu')
                print(f"   âœ… SPLADE ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                print(f"   âš ï¸ SPLADE ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"   âš ï¸ BM25ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                self.sparse_method = 'bm25'
                self._init_stemmer()

    def _init_stemmer(self):
        """ìŠ¤í…Œë¨¸ ì´ˆê¸°í™”"""
        try:
            from nltk.stem import PorterStemmer
            self.stemmer = PorterStemmer()
            self.use_stemming = True
        except ImportError:
            self.stemmer = None
            self.use_stemming = False

    def _tokenize(self, text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì € + ìŠ¤í…Œë°"""
        import re
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)
        if self.use_stemming and self.stemmer:
            tokens = [self.stemmer.stem(token) for token in tokens]
        return tokens

    def _text_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """í…ìŠ¤íŠ¸ë¥¼ ìŠ¤íŒŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜ (BM25 ë˜ëŠ” SPLADE)"""
        if self.sparse_method == 'splade' and self.splade_encoder is not None:
            return self._splade_to_sparse_vector(text)
        else:
            return self._bm25_to_sparse_vector(text)

    def _bm25_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """BM25 ìŠ¤íƒ€ì¼ ìŠ¤íŒŒìŠ¤ ë²¡í„° ìƒì„±"""
        from collections import Counter
        import math

        tokens = self._tokenize(text)
        token_counts = Counter(tokens)

        sparse_vector = {}
        for token, count in token_counts.items():
            # í•´ì‹œ í•¨ìˆ˜ë¡œ ì¸ë±ìŠ¤ ìƒì„± (vocabulary ëŒ€ì‹ )
            idx = hash(token) % 30000  # 30000 ì°¨ì›ìœ¼ë¡œ ì œí•œ
            if idx < 0:
                idx = -idx
            tf = 1 + math.log(count) if count > 0 else 0
            sparse_vector[idx] = tf

        return sparse_vector

    def _splade_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """SPLADE ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŒŒìŠ¤ ë²¡í„° ìƒì„±"""
        # SPLADE ì¸ì½”ë”© (token -> weight ë”•ì…”ë„ˆë¦¬ ë°˜í™˜)
        splade_result = self.splade_encoder.encode([text])[0]

        # token ë¬¸ìì—´ì„ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        sparse_vector = {}
        for token, weight in splade_result.items():
            # í•´ì‹œ í•¨ìˆ˜ë¡œ ì¸ë±ìŠ¤ ìƒì„±
            idx = hash(token) % 30000
            if idx < 0:
                idx = -idx
            # ê°™ì€ ì¸ë±ìŠ¤ì— ì—¬ëŸ¬ í† í°ì´ ë§¤í•‘ë˜ë©´ ìµœëŒ€ê°’ ì‚¬ìš©
            if idx in sparse_vector:
                sparse_vector[idx] = max(sparse_vector[idx], weight)
            else:
                sparse_vector[idx] = weight

        return sparse_vector

    def create_collection(self):
        """ì»¬ë ‰ì…˜ ìƒì„± (Named Vectors + HNSW ì¸ë±ìŠ¤)"""
        from qdrant_client.models import (
            VectorParams, SparseVectorParams, Distance,
            HnswConfigDiff, OptimizersConfigDiff
        )

        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        try:
            self.client.delete_collection(self.collection_name)
        except:
            pass

        print(f"\nğŸ“¦ Qdrant ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
        print("   ğŸ”§ HNSW ì¸ë±ìŠ¤ ì„¤ì • ì¤‘...")

        # Named Vectorsë¡œ ì»¬ë ‰ì…˜ ìƒì„±
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config={
                # Dense ë²¡í„° (HNSW ì¸ë±ìŠ¤)
                "dense": VectorParams(
                    size=self.dense_dim,
                    distance=Distance.COSINE,
                    hnsw_config=HnswConfigDiff(
                        m=16,              # ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•, ëŠë¦¼)
                        ef_construct=100,  # êµ¬ì¶• ì‹œ íƒìƒ‰ ë²”ìœ„
                        full_scan_threshold=10000
                    )
                )
            },
            sparse_vectors_config={
                # Sparse ë²¡í„°
                "sparse": SparseVectorParams()
            },
            optimizers_config=OptimizersConfigDiff(
                indexing_threshold=20000  # ì¸ë±ì‹± ì„ê³„ê°’
            )
        )

        print("   âœ… Dense ë²¡í„°: HNSW ì¸ë±ìŠ¤ (COSINE)")
        print(f"   âœ… Sparse ë²¡í„°: {self.sparse_method.upper()} + Inverted Index")

    def add_documents(self, documents: List[Dict], text_splitter):
        """ë¬¸ì„œ ì¶”ê°€ (ì²­í‚¹ + ì„ë² ë”© + ì €ì¥)"""
        from qdrant_client.models import PointStruct, SparseVector
        import uuid

        print("\nğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")

        all_chunks = []
        all_metadata = []

        # ì²­í‚¹
        for doc in documents:
            chunks = text_splitter.split_text(doc['text'])
            for i, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                # í’ë¶€í•œ ë©”íƒ€ë°ì´í„°
                all_metadata.append({
                    'source': doc['source'],
                    'filepath': doc.get('filepath', ''),
                    'chunk_id': i,
                    'chunk_total': len(chunks),
                    'text_length': len(chunk)
                })
            print(f"   ğŸ“„ {doc['source'][:40]}...: {len(chunks)} ì²­í¬")

        print(f"\nğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}ê°œ")

        if not all_chunks:
            raise ValueError("ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")

        self.chunks = [{'content': c, **m} for c, m in zip(all_chunks, all_metadata)]

        # Dense ì„ë² ë”© ìƒì„±
        print("\nğŸ§  Dense ì„ë² ë”© ìƒì„± ì¤‘...")
        dense_vectors = self.embeddings.embed_documents(all_chunks)

        # Sparse ë²¡í„° ìƒì„±
        sparse_method_name = self.sparse_method.upper()
        print(f"ğŸ”¤ Sparse ë²¡í„° ìƒì„± ì¤‘... ({sparse_method_name})")
        sparse_vectors = []
        for chunk in all_chunks:
            sv = self._text_to_sparse_vector(chunk)
            sparse_vectors.append(sv)

        # Qdrantì— ì €ì¥
        print("\nğŸ’¾ Qdrantì— ì €ì¥ ì¤‘...")
        points = []
        for i, (chunk, dense_vec, sparse_vec, metadata) in enumerate(
            zip(all_chunks, dense_vectors, sparse_vectors, all_metadata)
        ):
            # Sparse ë²¡í„°ë¥¼ Qdrant í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            sparse_indices = list(sparse_vec.keys())
            sparse_values = list(sparse_vec.values())

            point = PointStruct(
                id=str(uuid.uuid4()),
                vector={
                    "dense": dense_vec,
                    "sparse": SparseVector(
                        indices=sparse_indices,
                        values=sparse_values
                    )
                },
                payload={
                    "text": chunk,
                    **metadata
                }
            )
            points.append(point)

            if (i + 1) % 50 == 0:
                print(f"   ì§„í–‰: {i + 1}/{len(all_chunks)}", end='\r')

        # ë°°ì¹˜ ì—…ë¡œë“œ
        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i:i + batch_size]
            self.client.upsert(
                collection_name=self.collection_name,
                points=batch
            )

        print(f"\nâœ… Qdrant ì €ì¥ ì™„ë£Œ! ({len(points)}ê°œ ë²¡í„°)")

        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        info = self.client.get_collection(self.collection_name)
        print(f"   ğŸ“Š ì»¬ë ‰ì…˜ ìƒíƒœ: {info.status}")
        print(f"   ğŸ“Š í¬ì¸íŠ¸ ìˆ˜: {info.points_count}")

    def dense_search(self, query: str, k: int = 5, filter_conditions: Dict = None) -> List[Dict]:
        """Dense ë²¡í„° ê²€ìƒ‰ (HNSW ANN)"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, SearchParams

        query_vector = self.embeddings.embed_query(query)

        # í•„í„° ì¡°ê±´ êµ¬ì„±
        query_filter = None
        if filter_conditions:
            conditions = []
            for field, value in filter_conditions.items():
                conditions.append(
                    FieldCondition(key=field, match=MatchValue(value=value))
                )
            query_filter = Filter(must=conditions)

        # ìƒˆë¡œìš´ Qdrant API ì‚¬ìš©
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=query_vector,
            using="dense",
            query_filter=query_filter,
            limit=k,
            with_payload=True,
            search_params=SearchParams(hnsw_ef=128)
        )

        return [
            {
                'content': r.payload.get('text', ''),
                'source': r.payload.get('source', 'Unknown'),
                'score': r.score,
                'chunk_id': r.payload.get('chunk_id', 0),
                'method': 'dense'
            }
            for r in results.points
        ]

    def sparse_search(self, query: str, k: int = 5, filter_conditions: Dict = None) -> List[Dict]:
        """Sparse ë²¡í„° ê²€ìƒ‰"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, SparseVector

        sparse_vec = self._text_to_sparse_vector(query)
        sparse_indices = list(sparse_vec.keys())
        sparse_values = list(sparse_vec.values())

        # í•„í„° ì¡°ê±´ êµ¬ì„±
        query_filter = None
        if filter_conditions:
            conditions = []
            for field, value in filter_conditions.items():
                conditions.append(
                    FieldCondition(key=field, match=MatchValue(value=value))
                )
            query_filter = Filter(must=conditions)

        # ìƒˆë¡œìš´ Qdrant API ì‚¬ìš©
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=SparseVector(
                indices=sparse_indices,
                values=sparse_values
            ),
            using="sparse",
            query_filter=query_filter,
            limit=k,
            with_payload=True
        )

        return [
            {
                'content': r.payload.get('text', ''),
                'source': r.payload.get('source', 'Unknown'),
                'score': r.score,
                'chunk_id': r.payload.get('chunk_id', 0),
                'method': 'sparse'
            }
            for r in results.points
        ]

    def hybrid_search(
        self,
        query: str,
        k: int = 5,
        alpha: float = 0.5,
        filter_conditions: Dict = None
    ) -> List[Dict]:
        """
        Hybrid ê²€ìƒ‰ (Dense + Sparse)
        alpha: 0.0 = ìˆœìˆ˜ Sparse, 1.0 = ìˆœìˆ˜ Dense
        """
        import numpy as np

        # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
        num_candidates = max(k * 3, 20)

        dense_results = self.dense_search(query, k=num_candidates, filter_conditions=filter_conditions)
        sparse_results = self.sparse_search(query, k=num_candidates, filter_conditions=filter_conditions)

        # ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’
        dense_scores = [r['score'] for r in dense_results] if dense_results else [0]
        sparse_scores = [r['score'] for r in sparse_results] if sparse_results else [0]

        dense_max, dense_min = max(dense_scores), min(dense_scores)
        sparse_max, sparse_min = max(sparse_scores), min(sparse_scores)

        dense_range = dense_max - dense_min if dense_max > dense_min else 1.0
        sparse_range = sparse_max - sparse_min if sparse_max > sparse_min else 1.0

        # ê²°ê³¼ í†µí•©
        doc_scores = {}

        for r in dense_results:
            key = r['content'][:100]
            dense_norm = (r['score'] - dense_min) / dense_range
            doc_scores[key] = {
                'content': r['content'],
                'source': r['source'],
                'chunk_id': r['chunk_id'],
                'dense_score': r['score'],
                'dense_norm': dense_norm,
                'sparse_score': 0,
                'sparse_norm': 0
            }

        for r in sparse_results:
            key = r['content'][:100]
            sparse_norm = (r['score'] - sparse_min) / sparse_range

            if key in doc_scores:
                doc_scores[key]['sparse_score'] = r['score']
                doc_scores[key]['sparse_norm'] = sparse_norm
            else:
                doc_scores[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'chunk_id': r['chunk_id'],
                    'dense_score': 0,
                    'dense_norm': 0,
                    'sparse_score': r['score'],
                    'sparse_norm': sparse_norm
                }

        # Hybrid ì ìˆ˜ ê³„ì‚°
        results = []
        for key, data in doc_scores.items():
            hybrid_score = alpha * data['dense_norm'] + (1 - alpha) * data['sparse_norm']
            results.append({
                'content': data['content'],
                'source': data['source'],
                'chunk_id': data['chunk_id'],
                'hybrid_score': hybrid_score,
                'dense_score': data['dense_score'],
                'dense_norm': data['dense_norm'],
                'sparse_score': data['sparse_score'],
                'sparse_norm': data['sparse_norm'],
                'method': 'hybrid'
            })

        # ì •ë ¬ ë° ë°˜í™˜
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        return results[:k]

    def search_with_filter(
        self,
        query: str,
        source_filter: str = None,
        min_chunk_length: int = None,
        k: int = 5,
        search_type: str = 'hybrid'
    ) -> List[Dict]:
        """Payload í•„í„°ë§ì„ ì ìš©í•œ ê²€ìƒ‰"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, Range

        conditions = []

        if source_filter:
            conditions.append(
                FieldCondition(
                    key="source",
                    match=MatchValue(value=source_filter)
                )
            )

        if min_chunk_length:
            conditions.append(
                FieldCondition(
                    key="text_length",
                    range=Range(gte=min_chunk_length)
                )
            )

        query_filter = Filter(must=conditions) if conditions else None

        filter_dict = {}
        if source_filter:
            filter_dict['source'] = source_filter

        if search_type == 'dense':
            return self.dense_search(query, k, filter_conditions=filter_dict if filter_dict else None)
        elif search_type == 'sparse':
            return self.sparse_search(query, k, filter_conditions=filter_dict if filter_dict else None)
        else:
            return self.hybrid_search(query, k, filter_conditions=filter_dict if filter_dict else None)

    def get_collection_info(self) -> Dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        info = self.client.get_collection(self.collection_name)
        return {
            'name': self.collection_name,
            'status': str(info.status),
            'points_count': info.points_count,
            'vectors_count': info.vectors_count,
            'indexed_vectors_count': info.indexed_vectors_count
        }

    def get_sparse_method_name(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ sparse ë°©ì‹ ì´ë¦„ ë°˜í™˜"""
        return self.sparse_method.upper()

    def visualize_comparison(
        self,
        query: str,
        sparse_results: List[Dict],
        dense_results: List[Dict],
        hybrid_results: List[Dict],
        alpha: float = 0.7,
        sparse_method: str = 'BM25',
        dense_model: str = 'Dense',
        save_path: str = None,
        show_plot: bool = False
    ) -> str:
        """Qdrant ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™” (íŒŒì¼ ì €ì¥)"""
        import matplotlib
        matplotlib.use('Agg')  # ë¹„ëŒ€í™”í˜• ë°±ì—”ë“œ (ë¹ ë¥´ê³  ì•ˆì •ì )
        import matplotlib.pyplot as plt

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            plt.rcParams['font.family'] = 'AppleGothic'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(15, 11))
        fig.suptitle(f'ğŸ” Qdrant Hybrid Search Analysis\nQuery: "{query[:60]}..."',
                    fontsize=14, fontweight='bold')

        # 1. Sparse ì ìˆ˜ (ì™¼ìª½ ìƒë‹¨)
        ax1 = axes[0, 0]
        if sparse_results:
            sparse_labels = [f"Doc {i+1}\n{r['source'][:20]}..." for i, r in enumerate(sparse_results[:5])]
            sparse_scores = [r['score'] for r in sparse_results[:5]]
            colors1 = plt.cm.Blues([(0.4 + 0.12*i) for i in range(len(sparse_scores))])
            bars1 = ax1.barh(sparse_labels, sparse_scores, color=colors1, edgecolor='navy', alpha=0.85)
            ax1.set_xlabel(f'{sparse_method} Score', fontweight='bold')
            ax1.set_title(f'ğŸ”µ Sparse Search ({sparse_method})', fontweight='bold', fontsize=11)
            ax1.invert_yaxis()
            for bar, score in zip(bars1, sparse_scores):
                ax1.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,
                        f'{score:.3f}', va='center', fontsize=9, fontweight='bold')
            ax1.set_xlim(0, max(sparse_scores) * 1.3 if sparse_scores else 1)

        # 2. Dense ì ìˆ˜ (ì˜¤ë¥¸ìª½ ìƒë‹¨)
        ax2 = axes[0, 1]
        if dense_results:
            dense_labels = [f"Doc {i+1}\n{r['source'][:20]}..." for i, r in enumerate(dense_results[:5])]
            dense_scores = [r['score'] for r in dense_results[:5]]
            colors2 = plt.cm.Reds([(0.4 + 0.12*i) for i in range(len(dense_scores))])
            bars2 = ax2.barh(dense_labels, dense_scores, color=colors2, edgecolor='darkred', alpha=0.85)
            ax2.set_xlabel('Cosine Similarity (higher = better)', fontweight='bold')
            ax2.set_title(f'ğŸ”´ Dense Search (HNSW, {dense_model})', fontweight='bold', fontsize=11)
            ax2.invert_yaxis()
            for bar, score in zip(bars2, dense_scores):
                ax2.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                        f'{score:.4f}', va='center', fontsize=9, fontweight='bold')
            ax2.set_xlim(0, max(dense_scores) * 1.2 if dense_scores else 1)

        # 3. Hybrid ì ìˆ˜ ë¹„êµ (ì™¼ìª½ í•˜ë‹¨)
        ax3 = axes[1, 0]
        if hybrid_results:
            hybrid_labels = [f"Doc {i+1}" for i in range(len(hybrid_results[:5]))]
            x = range(len(hybrid_labels))
            width = 0.25

            sparse_norm = [r.get('sparse_norm', 0) for r in hybrid_results[:5]]
            dense_norm = [r.get('dense_norm', 0) for r in hybrid_results[:5]]
            hybrid_scores = [r['hybrid_score'] for r in hybrid_results[:5]]

            bars_sparse = ax3.bar([i - width for i in x], sparse_norm, width,
                                 label=f'{sparse_method} (norm)', color='#3498db', alpha=0.85, edgecolor='navy')
            bars_dense = ax3.bar(x, dense_norm, width,
                                label='Dense (norm)', color='#e74c3c', alpha=0.85, edgecolor='darkred')
            bars_hybrid = ax3.bar([i + width for i in x], hybrid_scores, width,
                                 label='Hybrid', color='#2ecc71', alpha=0.85, edgecolor='darkgreen')

            ax3.set_xlabel('Document', fontweight='bold')
            ax3.set_ylabel('Normalized Score (0-1)', fontweight='bold')
            ax3.set_title(f'ğŸŸ¢ Hybrid Score Fusion (Î±={alpha})\n{sparse_method}Ã—{1-alpha:.1f} + DenseÃ—{alpha:.1f}',
                         fontweight='bold', fontsize=11)
            ax3.set_xticks(x)
            ax3.set_xticklabels(hybrid_labels)
            ax3.set_ylim(0, 1.15)
            ax3.legend(loc='upper right', fontsize=9)
            ax3.grid(axis='y', alpha=0.3)

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ í‘œì‹œ
            for bar, score in zip(bars_hybrid, hybrid_scores):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                        f'{score:.2f}', ha='center', va='bottom', fontsize=8, fontweight='bold')

        # 4. ìƒì„¸ ê²°ê³¼ ìš”ì•½ (ì˜¤ë¥¸ìª½ í•˜ë‹¨)
        ax4 = axes[1, 1]
        ax4.axis('off')

        info_text = "ğŸ“Š Qdrant Search Results Summary\n" + "="*45 + "\n\n"

        info_text += f"ğŸ”µ Sparse ({sparse_method}) - Top 3:\n"
        for i, r in enumerate(sparse_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     Score: {r['score']:.4f}\n"

        info_text += f"\nğŸ”´ Dense (HNSW) - Top 3:\n"
        for i, r in enumerate(dense_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     Cosine: {r['score']:.4f}\n"

        info_text += f"\nğŸŸ¢ Hybrid (Î±={alpha}) - Top 3:\n"
        for i, r in enumerate(hybrid_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            sparse_s = r.get('sparse_score', 0)
            dense_s = r.get('dense_score', 0)
            hybrid_s = r.get('hybrid_score', 0)
            info_text += f"  {i}. {source}\n"
            info_text += f"     Hybrid: {hybrid_s:.3f} (S:{sparse_s:.3f} D:{dense_s:.3f})\n"

        info_text += f"\nğŸ“ˆ Statistics:\n"
        info_text += f"  â€¢ Collection: {self.collection_name}\n"
        info_text += f"  â€¢ Total chunks: {len(self.chunks)}\n"
        info_text += f"  â€¢ Dense dim: {self.dense_dim}\n"

        ax4.text(0.02, 0.98, info_text, transform=ax4.transAxes, fontsize=9,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow',
                         alpha=0.9, edgecolor='orange'))

        plt.tight_layout()

        # ì €ì¥
        if save_path is None:
            save_path = f"qdrant_hybrid_search_{query[:20].replace(' ', '_')}.png"

        plt.savefig(save_path, dpi=100, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close(fig)  # ëª…ì‹œì ìœ¼ë¡œ figure ë‹«ê¸°

        print(f"   ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
        return save_path


# ==================== SPLADE Encoder ====================
class SPLADEEncoder:
    """SPLADE (Sparse Lexical and Expansion) ì¸ì½”ë”"""

    def __init__(self, model_name: str = "naver/splade-cocondenser-ensembledistil", device: str = 'cpu'):
        import torch
        from transformers import AutoTokenizer, AutoModelForMaskedLM

        self.device = device
        self.model_name = model_name

        print(f"   ğŸ“¥ SPLADE ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()
        self.torch = torch

    def encode(self, texts: List[str], max_length: int = 256) -> List[Dict[str, float]]:
        """í…ìŠ¤íŠ¸ë¥¼ SPLADE ìŠ¤íŒŒìŠ¤ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
        sparse_vectors = []

        for text in texts:
            # í† í°í™”
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                max_length=max_length,
                truncation=True,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # SPLADE ì¸ì½”ë”©
            with self.torch.no_grad():
                outputs = self.model(**inputs)
                # SPLADE: log(1 + ReLU(logits)) * attention_mask
                logits = outputs.logits
                relu_log = self.torch.log1p(self.torch.relu(logits))
                # Max pooling over sequence
                weighted = relu_log * inputs['attention_mask'].unsqueeze(-1)
                sparse_vec = self.torch.max(weighted, dim=1).values.squeeze()

            # 0ì´ ì•„ë‹Œ ê°’ë§Œ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
            sparse_dict = {}
            indices = self.torch.nonzero(sparse_vec).squeeze(-1)
            for idx in indices:
                idx = idx.item()
                token = self.tokenizer.decode([idx])
                weight = sparse_vec[idx].item()
                if weight > 0.1:  # ì„ê³„ê°’ ì´ìƒë§Œ ì €ì¥
                    sparse_dict[token] = weight

            sparse_vectors.append(sparse_dict)

        return sparse_vectors

    def compute_similarity(self, query_vec: Dict[str, float], doc_vec: Dict[str, float]) -> float:
        """ì¿¼ë¦¬ì™€ ë¬¸ì„œ ìŠ¤íŒŒìŠ¤ ë²¡í„°ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ë‚´ì )"""
        score = 0.0
        for token, weight in query_vec.items():
            if token in doc_vec:
                score += weight * doc_vec[token]
        return score


# ==================== Hybrid Search ì‹œìŠ¤í…œ ====================
class HybridSearchSystem:
    """Sparse (BM25/SPLADE) + Dense (Semantic) + Hybrid ê²€ìƒ‰ ì‹œìŠ¤í…œ"""

    def __init__(self, rag_system: RAGSystem, sparse_method: str = 'bm25'):
        """
        Args:
            rag_system: RAG ì‹œìŠ¤í…œ
            sparse_method: 'bm25' ë˜ëŠ” 'splade'
        """
        from rank_bm25 import BM25Okapi
        import numpy as np

        self.rag = rag_system
        self.chunks = rag_system.get_all_chunks()
        self.np = np
        self.sparse_method = sparse_method.lower()

        # ìŠ¤í…Œë¨¸ ì´ˆê¸°í™” (BM25ìš©)
        self._init_stemmer()

        # Sparse ì¸ë±ìŠ¤ êµ¬ì¶•
        if self.sparse_method == 'splade':
            self._init_splade()
        else:
            self._init_bm25()

    def _init_bm25(self):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        from rank_bm25 import BM25Okapi

        print("\nğŸ” BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (ìŠ¤í…Œë° ì ìš©)...")
        tokenized_corpus = [self._tokenize(chunk['content']) for chunk in self.chunks]
        self.bm25 = BM25Okapi(tokenized_corpus)
        self.splade = None
        self.splade_vectors = None
        print(f"âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ({len(self.chunks)} ì²­í¬)")

    def _init_splade(self):
        """SPLADE ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("\nğŸ” SPLADE ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

        try:
            self.splade = SPLADEEncoder()
            self.bm25 = None

            # ëª¨ë“  ë¬¸ì„œ ì¸ì½”ë”©
            print(f"   ğŸ“„ {len(self.chunks)}ê°œ ë¬¸ì„œ ì¸ì½”ë”© ì¤‘...")
            doc_texts = [chunk['content'] for chunk in self.chunks]

            # ë°°ì¹˜ ì²˜ë¦¬
            batch_size = 8
            self.splade_vectors = []
            for i in range(0, len(doc_texts), batch_size):
                batch = doc_texts[i:i+batch_size]
                vectors = self.splade.encode(batch)
                self.splade_vectors.extend(vectors)
                print(f"   ì§„í–‰: {min(i+batch_size, len(doc_texts))}/{len(doc_texts)}", end='\r')

            print(f"\nâœ… SPLADE ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ({len(self.chunks)} ì²­í¬)")

        except Exception as e:
            print(f"âš ï¸ SPLADE ë¡œë“œ ì‹¤íŒ¨: {str(e)[:50]}")
            print("   BM25ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            self.sparse_method = 'bm25'
            self._init_bm25()

    def _init_stemmer(self):
        """ìŠ¤í…Œë¨¸ ì´ˆê¸°í™” - Porter Stemmer ì‚¬ìš©"""
        try:
            from nltk.stem import PorterStemmer
            self.stemmer = PorterStemmer()
            self.use_stemming = True
        except ImportError:
            self.stemmer = None
            self.use_stemming = False

    def _tokenize(self, text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì € + ìŠ¤í…Œë°"""
        import re
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)

        # ìŠ¤í…Œë° ì ìš© (diabete -> diabet, diabetes -> diabet)
        if self.use_stemming and self.stemmer:
            tokens = [self.stemmer.stem(token) for token in tokens]

        return tokens

    def sparse_search(self, query: str, k: int = 5) -> List[Dict]:
        """Sparse ê²€ìƒ‰ (BM25 ë˜ëŠ” SPLADE)"""
        if self.sparse_method == 'splade' and self.splade is not None:
            return self._splade_search(query, k)
        else:
            return self._bm25_search(query, k)

    def _bm25_search(self, query: str, k: int = 5) -> List[Dict]:
        """BM25 ê¸°ë°˜ ê²€ìƒ‰"""
        tokenized_query = self._tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = self.np.argsort(scores)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                'content': self.chunks[idx]['content'],
                'source': self.chunks[idx]['source'],
                'score': float(scores[idx]),
                'method': 'sparse (BM25)'
            })
        return results

    def _splade_search(self, query: str, k: int = 5) -> List[Dict]:
        """SPLADE ê¸°ë°˜ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì¸ì½”ë”©
        query_vec = self.splade.encode([query])[0]

        # ëª¨ë“  ë¬¸ì„œì™€ ìœ ì‚¬ë„ ê³„ì‚°
        scores = []
        for doc_vec in self.splade_vectors:
            score = self.splade.compute_similarity(query_vec, doc_vec)
            scores.append(score)

        scores = self.np.array(scores)

        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = self.np.argsort(scores)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                'content': self.chunks[idx]['content'],
                'source': self.chunks[idx]['source'],
                'score': float(scores[idx]),
                'method': 'sparse (SPLADE)'
            })
        return results

    def dense_search(self, query: str, k: int = 5) -> List[Dict]:
        """FAISS ê¸°ë°˜ Dense ê²€ìƒ‰ (ì˜ë¯¸ì  ìœ ì‚¬ë„)"""
        results = self.rag.search(query, k=k)
        for r in results:
            r['method'] = 'dense'
        return results

    def hybrid_search(self, query: str, k: int = 5, alpha: float = 0.5, rrf_k: int = 10) -> List[Dict]:
        """
        Hybrid ê²€ìƒ‰ (Sparse + Dense ê²°í•©) - RRF (Reciprocal Rank Fusion) ì‚¬ìš©
        alpha: 0.0 = ìˆœìˆ˜ Sparse, 1.0 = ìˆœìˆ˜ Dense
        rrf_k: RRF ìƒìˆ˜ (ê¸°ë³¸ê°’ 10 - ì ìˆ˜ ë²”ìœ„ í–¥ìƒ)
        """
        # ì¶©ë¶„í•œ í›„ë³´ ê²€ìƒ‰
        num_candidates = max(k * 3, 20)
        sparse_results = self.sparse_search(query, k=num_candidates)
        dense_results = self.dense_search(query, k=num_candidates)

        # BM25 ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’ ê³„ì‚°
        bm25_scores = [r['score'] for r in sparse_results]
        bm25_max = max(bm25_scores) if bm25_scores else 1.0
        bm25_min = min(bm25_scores) if bm25_scores else 0.0
        bm25_range = bm25_max - bm25_min if bm25_max > bm25_min else 1.0

        # Dense ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’ ê³„ì‚° (L2 ê±°ë¦¬ - ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        dense_scores = [r['score'] for r in dense_results]
        dense_max = max(dense_scores) if dense_scores else 1.0
        dense_min = min(dense_scores) if dense_scores else 0.0
        dense_range = dense_max - dense_min if dense_max > dense_min else 1.0

        # ë¬¸ì„œ í†µí•©ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        doc_data = {}

        # Sparse ê²°ê³¼ ì²˜ë¦¬ - ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ + BM25 ì •ê·œí™”
        for rank, r in enumerate(sparse_results):
            key = r['content'][:100]
            sparse_rrf = 1.0 / (rrf_k + rank + 1)  # RRF ì ìˆ˜
            # BM25 ì ìˆ˜ë¥¼ 0-1ë¡œ ì •ê·œí™”
            bm25_norm = (r['score'] - bm25_min) / bm25_range if bm25_range > 0 else 1.0

            if key not in doc_data:
                doc_data[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'sparse_rank': rank + 1,
                    'sparse_score': r['score'],  # ì›ë³¸ BM25 ì ìˆ˜ (0~30+ ë²”ìœ„)
                    'sparse_score_norm': bm25_norm,  # ì •ê·œí™”ëœ BM25 (0-1)
                    'sparse_rrf': sparse_rrf,
                    'dense_rank': 0,
                    'dense_score': 0,
                    'dense_score_norm': 0,
                    'dense_rrf': 0
                }
            else:
                doc_data[key]['sparse_rank'] = rank + 1
                doc_data[key]['sparse_score'] = r['score']
                doc_data[key]['sparse_score_norm'] = bm25_norm
                doc_data[key]['sparse_rrf'] = sparse_rrf

        # Dense ê²°ê³¼ ì²˜ë¦¬ - ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ + ê±°ë¦¬ ì •ê·œí™”
        for rank, r in enumerate(dense_results):
            key = r['content'][:100]
            dense_rrf = 1.0 / (rrf_k + rank + 1)  # RRF ì ìˆ˜
            # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (1 - ì •ê·œí™”ëœ ê±°ë¦¬)
            dense_norm = 1.0 - ((r['score'] - dense_min) / dense_range) if dense_range > 0 else 1.0

            if key not in doc_data:
                doc_data[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'sparse_rank': 0,
                    'sparse_score': 0,
                    'sparse_score_norm': 0,
                    'sparse_rrf': 0,
                    'dense_rank': rank + 1,
                    'dense_score': r['score'],  # ì›ë³¸ L2 ê±°ë¦¬
                    'dense_score_norm': dense_norm,  # ì •ê·œí™”ëœ ìœ ì‚¬ë„ (0-1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                    'dense_rrf': dense_rrf
                }
            else:
                doc_data[key]['dense_rank'] = rank + 1
                doc_data[key]['dense_score'] = r['score']
                doc_data[key]['dense_score_norm'] = dense_norm
                doc_data[key]['dense_rrf'] = dense_rrf

        # Hybrid ìŠ¤ì½”ì–´ ê³„ì‚° (ì •ê·œí™”ëœ ì ìˆ˜ ê¸°ë°˜)
        results = []
        for key, data in doc_data.items():
            # ë°©ë²• 1: RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ
            hybrid_rrf = (1 - alpha) * data['sparse_rrf'] + alpha * data['dense_rrf']

            # ë°©ë²• 2: ì •ê·œí™”ëœ ì ìˆ˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ (ë” ì§ê´€ì )
            hybrid_norm = (1 - alpha) * data['sparse_score_norm'] + alpha * data['dense_score_norm']

            results.append({
                'content': data['content'],
                'source': data['source'],
                # ì›ë³¸ ì ìˆ˜ë“¤
                'sparse_score': data['sparse_score'],      # ì›ë³¸ BM25 (0~30+)
                'dense_score': data['dense_score'],        # ì›ë³¸ L2 ê±°ë¦¬
                # ì •ê·œí™”ëœ ì ìˆ˜ë“¤ (0-1)
                'sparse_score_norm': data['sparse_score_norm'],  # BM25 ì •ê·œí™”
                'dense_score_norm': data['dense_score_norm'],    # ìœ ì‚¬ë„ ì •ê·œí™”
                # ìˆœìœ„ ì •ë³´
                'sparse_rank': data['sparse_rank'],
                'dense_rank': data['dense_rank'],
                # RRF ì ìˆ˜
                'sparse_rrf': data['sparse_rrf'],
                'dense_rrf': data['dense_rrf'],
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
                'hybrid_score': hybrid_norm,        # ì •ê·œí™” ê¸°ë°˜ (0-1)
                'hybrid_score_rrf': hybrid_rrf,     # RRF ê¸°ë°˜
                'method': 'hybrid'
            })

        # Hybrid ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)

        return results[:k]

    def compare_all(self, query: str, k: int = 5, alpha: float = 0.5) -> Dict:
        """ì„¸ ê°€ì§€ ê²€ìƒ‰ ë°©ë²• ë¹„êµ"""
        sparse = self.sparse_search(query, k)
        dense = self.dense_search(query, k)
        hybrid = self.hybrid_search(query, k, alpha)

        return {
            'query': query,
            'sparse': sparse,
            'dense': dense,
            'hybrid': hybrid
        }

    def visualize_comparison(self, query: str, k: int = 5, alpha: float = 0.5,
                            save_path: str = None) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™” (íŒŒì¼ ì €ì¥)"""
        import matplotlib
        matplotlib.use('Agg')  # ë¹„ëŒ€í™”í˜• ë°±ì—”ë“œ (ë¹ ë¥´ê³  ì•ˆì •ì )
        import matplotlib.pyplot as plt

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            plt.rcParams['font.family'] = 'AppleGothic'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False

        results = self.compare_all(query, k, alpha)

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'Hybrid Search Comparison\nQuery: "{query[:50]}..."', fontsize=14, fontweight='bold')

        # 1. Sparse (BM25) ì ìˆ˜
        ax1 = axes[0, 0]
        sparse_labels = [f"Doc {i+1}" for i in range(len(results['sparse']))]
        sparse_scores = [r['score'] for r in results['sparse']]
        bars1 = ax1.barh(sparse_labels, sparse_scores, color='#3498db', alpha=0.8)
        ax1.set_xlabel('BM25 Score')
        ax1.set_title('Sparse Search (BM25)', fontweight='bold')
        ax1.invert_yaxis()
        for bar, score in zip(bars1, sparse_scores):
            ax1.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                    f'{score:.2f}', va='center', fontsize=9)

        # 2. Dense (Semantic) ì ìˆ˜
        ax2 = axes[0, 1]
        dense_labels = [f"Doc {i+1}" for i in range(len(results['dense']))]
        dense_scores = [r['score'] for r in results['dense']]
        bars2 = ax2.barh(dense_labels, dense_scores, color='#e74c3c', alpha=0.8)
        ax2.set_xlabel('L2 Distance (lower = better)')
        ax2.set_title('Dense Search (Semantic)', fontweight='bold')
        ax2.invert_yaxis()
        for bar, score in zip(bars2, dense_scores):
            ax2.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                    f'{score:.3f}', va='center', fontsize=9)

        # 3. Hybrid ì •ê·œí™” ì ìˆ˜ ë¹„êµ
        ax3 = axes[1, 0]
        hybrid_labels = [f"Doc {i+1}" for i in range(len(results['hybrid']))]
        x = range(len(hybrid_labels))
        width = 0.25

        # ì •ê·œí™”ëœ ì ìˆ˜ ì‚¬ìš© (0-1 ë²”ìœ„)
        sparse_norm = [r.get('sparse_score_norm', 0) for r in results['hybrid']]
        dense_norm = [r.get('dense_score_norm', 0) for r in results['hybrid']]
        hybrid_scores = [r['hybrid_score'] for r in results['hybrid']]

        ax3.bar([i - width for i in x], sparse_norm, width, label='BM25 (norm)', color='#3498db', alpha=0.8)
        ax3.bar(x, dense_norm, width, label='Semantic (norm)', color='#e74c3c', alpha=0.8)
        ax3.bar([i + width for i in x], hybrid_scores, width, label='Hybrid', color='#2ecc71', alpha=0.8)
        ax3.set_xlabel('Document')
        ax3.set_ylabel('Normalized Score (0-1)')
        ax3.set_title(f'Hybrid Search - Score Fusion (Î±={alpha})', fontweight='bold')
        ax3.set_xticks(x)
        ax3.set_xticklabels(hybrid_labels)
        ax3.set_ylim(0, 1.1)  # 0-1 ë²”ìœ„ ëª…í™•íˆ í‘œì‹œ
        ax3.legend()

        # 4. ë¬¸ì„œ ì¶œì²˜ ì •ë³´ (ìˆœìœ„ í¬í•¨)
        ax4 = axes[1, 1]
        ax4.axis('off')

        info_text = "ğŸ“Š Search Results Summary\n" + "="*40 + "\n\n"

        info_text += "ğŸ”µ Sparse (BM25) - Top Results:\n"
        for i, r in enumerate(results['sparse'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     BM25: {r['score']:.2f}\n"

        info_text += "\nğŸ”´ Dense (Semantic) - Top Results:\n"
        for i, r in enumerate(results['dense'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     L2 Dist: {r['score']:.4f}\n"

        info_text += "\nğŸŸ¢ Hybrid - Top Results:\n"
        for i, r in enumerate(results['hybrid'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            s_rank = r.get('sparse_rank', 0)
            d_rank = r.get('dense_rank', 0)
            bm25 = r.get('sparse_score', 0)
            s_norm = r.get('sparse_score_norm', 0)
            d_norm = r.get('dense_score_norm', 0)
            info_text += f"  {i}. {source}\n"
            info_text += f"     Hybrid: {r['hybrid_score']:.2f}\n"
            info_text += f"     BM25={bm25:.1f}({s_norm:.2f}) Sem={d_norm:.2f}\n"

        ax4.text(0.05, 0.95, info_text, transform=ax4.transAxes, fontsize=9,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.tight_layout()

        # ì €ì¥
        if save_path is None:
            save_path = f"./hybrid_search_comparison.png"

        plt.savefig(save_path, dpi=100, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close(fig)  # ëª…ì‹œì ìœ¼ë¡œ figure ë‹«ê¸°

        print(f"   ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
        return results


# ==================== ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ====================
def interactive_qa(rag: RAGSystem, openai_api_key: str = None):
    """ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ëª¨ë“œ"""

    # ì–¸ì–´ë³„ ë©”ì‹œì§€
    if rag.language == 'ko':
        print("\n" + "=" * 60)
        print("ğŸ’¬ ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ëª¨ë“œ")
        print("=" * 60)
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        prompt_text = "â“ ì§ˆë¬¸: "
        exit_msg = "ğŸ‘‹ ì§ˆì˜ì‘ë‹µì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
    else:
        print("\n" + "=" * 60)
        print("ğŸ’¬ Interactive Q&A Mode")
        print("=" * 60)
        print("Enter your question. Type 'quit', 'exit', or 'q' to exit.")
        prompt_text = "â“ Question: "
        exit_msg = "ğŸ‘‹ Exiting Q&A mode."

    print("-" * 60)

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìˆëŠ” ê²½ìš°)
    client = None
    if openai_api_key:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai_api_key)
        except:
            pass

    while True:
        try:
            question = input(f"\n{prompt_text}").strip()

            if not question:
                continue

            if question.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ', 'ë']:
                print(f"\n{exit_msg}")
                break

            # ê²°ê³¼ ê°œìˆ˜ ì¡°ì ˆ
            k = 3
            if 'k=' in question:
                try:
                    k_part = question.split('k=')[1].split()[0]
                    k = int(k_part)
                    question = question.replace(f'k={k_part}', '').strip()
                except:
                    pass

            # ì§ˆë¬¸ ì–¸ì–´ ê°ì§€
            q_lang = detect_language(question)

            # í•œêµ­ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ê²€ìƒ‰
            search_question = question
            if q_lang == 'ko':
                search_question = translate_to_english(question, openai_api_key)
                if search_question != question:
                    print(f"   ğŸ”„ ê²€ìƒ‰ì–´: '{search_question}'")

            result = rag.answer(search_question, k=k)
            result['language'] = q_lang  # ì›ë³¸ ì§ˆë¬¸ì˜ ì–¸ì–´ ìœ ì§€

            # ì–¸ì–´ë³„ ì¶œë ¥
            if q_lang == 'ko':
                print(f"\nğŸ“š ê´€ë ¨ ë¬¸ì„œ {len(result['contexts'])}ê°œ ê²€ìƒ‰ë¨:\n")
            else:
                print(f"\nğŸ“š Found {len(result['contexts'])} relevant documents:\n")

            for i, ctx in enumerate(result['contexts'], 1):
                similarity = 1 / (1 + ctx['score'])

                if q_lang == 'ko':
                    print(f"[{i}] ì¶œì²˜: {ctx['source'][:50]}")
                    print(f"    ìœ ì‚¬ë„: {similarity:.2%}")
                else:
                    print(f"[{i}] Source: {ctx['source'][:50]}")
                    print(f"    Similarity: {similarity:.2%}")

                content_preview = ctx['content'].replace('\n', ' ')[:300]
                print(f"    {content_preview}...")
                print("-" * 40)

            # OpenAIë¡œ ë‹µë³€ ìƒì„± (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            if client:
                try:
                    context_text = "\n\n".join([ctx['content'] for ctx in result['contexts']])

                    if q_lang == 'ko':
                        system_msg = "ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                    else:
                        system_msg = "You are an expert answering questions based on medical/scientific papers. Answer based on the provided context."

                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": f"Context:\n{context_text}\n\nQuestion: {question}"}
                        ],
                        max_tokens=500,
                        temperature=0.3
                    )

                    answer = response.choices[0].message.content

                    if q_lang == 'ko':
                        print(f"\nğŸ¤– AI ë‹µë³€:\n{answer}")
                    else:
                        print(f"\nğŸ¤– AI Answer:\n{answer}")

                except Exception as e:
                    pass

            if q_lang == 'ko':
                print(f"\nğŸ“– ì°¸ê³  ë¬¸ì„œ: {', '.join(result['sources'])}")
            else:
                print(f"\nğŸ“– References: {', '.join(result['sources'])}")

        except KeyboardInterrupt:
            print(f"\n\n{exit_msg}")
            break
        except Exception as e:
            print(f"âŒ Error: {str(e)}")


# ==================== ë©”ì¸ ì‹¤í–‰ ====================
def main():
    print("=" * 60)
    print("ğŸš€ Medical/Scientific Paper RAG System")
    print("   with Paper Summarization & Multilingual Support")
    print("=" * 60)

    # 1. ëŒ€í™”í˜• ì„¤ì •
    config = Config().interactive_setup()

    # 2. ë…¼ë¬¸ ê²€ìƒ‰ (ì˜ì–´ë¡œ ê²€ìƒ‰)
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: ë…¼ë¬¸ ê²€ìƒ‰")
    print("=" * 60)

    # í•œêµ­ì–´ì¸ ê²½ìš° ë²ˆì—­ëœ ì˜ì–´ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    search_query = config.search_query_en if config.search_query_en else config.search_query
    if config.language == 'ko' and config.search_query_en != config.search_query:
        print(f"   ğŸ‡°ğŸ‡· â†’ ğŸ‡ºğŸ‡¸ '{search_query}' (ìœ¼)ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤...")

    searcher = PaperSearcher(
        api_key=config.pubmed_api_key,
        email=config.pubmed_email
    )
    papers = searcher.search(
        query=search_query,
        source=config.search_source,
        max_results=config.max_results
    )

    print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ì´ {len(papers)}ê°œ ë…¼ë¬¸")

    if papers:
        print("\nğŸ“‹ ê²€ìƒ‰ëœ ë…¼ë¬¸ ëª©ë¡:\n")
        for i, paper in enumerate(papers, 1):
            print(f"[{i}] {paper['source']} | {paper['title'][:65]}...")
            print(f"    ì €ì: {', '.join(paper['authors'][:3])}")
            print(f"    ë°œí–‰: {paper['published']}")
            print()

    if not papers:
        print("âŒ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. PDF ë‹¤ìš´ë¡œë“œ
    print("\n" + "=" * 60)
    print("ğŸ“¥ Step 2: PDF ë‹¤ìš´ë¡œë“œ")
    print("=" * 60)

    downloader = PDFDownloader(PAPERS_DIR)
    downloaded_files = downloader.download_all(papers)

    if not downloaded_files:
        print("âŒ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 4. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    print("\n" + "=" * 60)
    print("ğŸ“„ Step 3: í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    print("=" * 60)

    documents = TextExtractor.extract_all(downloaded_files)

    if not documents:
        print("âŒ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 5. ë…¼ë¬¸ ìš”ì•½ (OpenAI API ìˆëŠ” ê²½ìš°)
    if config.openai_api_key:
        summarizer = PaperSummarizer(
            api_key=config.openai_api_key,
            language=config.language
        )
        papers = summarizer.summarize(papers, documents)

    # 6. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("\n" + "=" * 60)
    print("ğŸ§  Step 4: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ")
    print("=" * 60)

    embeddings = EmbeddingModelFactory.create(
        model_type=config.embedding_model,
        device='cpu',
        openai_api_key=config.openai_api_key
    )

    # 7. RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    print("\n" + "=" * 60)
    print("ğŸ’¾ Step 5: RAG ì‹œìŠ¤í…œ êµ¬ì¶•")
    print("=" * 60)

    # Text Splitter ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap,
        length_function=len
    )

    # Vector DB ì„ íƒì— ë”°ë¼ ë¶„ê¸°
    if config.vector_db == 'qdrant':
        # Qdrant ê¸°ë°˜ ì‹œìŠ¤í…œ
        print(f"\nğŸ—„ï¸ Qdrant Vector DB ì‚¬ìš©")

        qdrant_search = QdrantHybridSearch(
            embeddings=embeddings,
            sparse_method=config.sparse_method,
            collection_name="medical_papers",
            use_memory=True  # ì¸ë©”ëª¨ë¦¬ ëª¨ë“œ
        )
        qdrant_search.create_collection()
        qdrant_search.add_documents(documents, text_splitter)

        # RAG ì‹œìŠ¤í…œë„ ìƒì„± (interactive_qaìš©)
        rag = RAGSystem(
            embeddings=embeddings,
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            language=config.language
        )
        rag.build_vectorstore(documents)

        # 8. Hybrid Search ë¶„ì„ ë° ì‹œê°í™” (Qdrant)
        print("\n" + "=" * 60)
        print("ğŸ”€ Step 6: Qdrant Hybrid Search ë¶„ì„")
        print("=" * 60)

        test_query = search_query
        print(f"\nğŸ” ê²€ìƒ‰ì–´: '{test_query}'")
        print("-" * 40)

        # Qdrant ê²€ìƒ‰ ì‹¤í–‰
        sparse_results = qdrant_search.sparse_search(test_query, k=5)
        dense_results = qdrant_search.dense_search(test_query, k=5)
        hybrid_results = qdrant_search.hybrid_search(test_query, k=5, alpha=0.7)

        # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ sparse ë°©ì‹ (SPLADE ë¡œë“œ ì‹¤íŒ¨ ì‹œ BM25ë¡œ í´ë°±ë  ìˆ˜ ìˆìŒ)
        sparse_name = qdrant_search.get_sparse_method_name()

        # Sparse ê²°ê³¼
        print(f"\nğŸ”µ Qdrant Sparse Search ({sparse_name}) ê²°ê³¼:")
        for i, r in enumerate(sparse_results[:3], 1):
            score = r['score']
            source = r['source'][:50]
            print(f"   [{i}] {sparse_name}: {score:.4f} | {source}...")

        # Dense ê²°ê³¼ (HNSW)
        print(f"\nğŸ”´ Qdrant Dense Search (HNSW, {config.embedding_model}) ê²°ê³¼:")
        for i, r in enumerate(dense_results[:3], 1):
            score = r['score']
            source = r['source'][:50]
            print(f"   [{i}] Cosine: {score:.4f} | {source}...")

        # Hybrid ê²°ê³¼
        print(f"\nğŸŸ¢ Qdrant Hybrid Search ê²°ê³¼ ({sparse_name} + Dense, Î±=0.7):")
        for i, r in enumerate(hybrid_results[:3], 1):
            hybrid_score = r.get('hybrid_score', 0)
            sparse_score = r.get('sparse_score', 0)
            dense_score = r.get('dense_score', 0)
            source = r['source'][:50]
            print(f"   [{i}] Hybrid: {hybrid_score:.3f} | Sparse={sparse_score:.3f} + Dense={dense_score:.3f}")
            print(f"       {source}...")

        # Payload í•„í„°ë§ ì˜ˆì‹œ
        print(f"\nğŸ” Payload í•„í„°ë§ í…ŒìŠ¤íŠ¸:")
        filtered_results = qdrant_search.search_with_filter(
            test_query,
            min_chunk_length=100,
            k=3
        )
        print(f"   (ìµœì†Œ ì²­í¬ ê¸¸ì´ 100ì ì´ìƒ í•„í„°)")
        for i, r in enumerate(filtered_results[:3], 1):
            print(f"   [{i}] {r['source'][:50]}...")

        # ì‹œê°í™” ìƒì„±
        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        qdrant_search.visualize_comparison(
            query=test_query,
            sparse_results=sparse_results,
            dense_results=dense_results,
            hybrid_results=hybrid_results,
            alpha=0.7,
            sparse_method=sparse_name,
            dense_model=config.embedding_model
        )

    else:
        # FAISS ê¸°ë°˜ ì‹œìŠ¤í…œ (ê¸°ì¡´)
        print(f"\nğŸ—„ï¸ FAISS Vector DB ì‚¬ìš©")

        rag = RAGSystem(
            embeddings=embeddings,
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            language=config.language
        )

        vectorstore = rag.build_vectorstore(documents)
        rag.save_vectorstore(VECTORSTORE_DIR)

        # 8. Hybrid Search ë¶„ì„ ë° ì‹œê°í™” (FAISS)
        print("\n" + "=" * 60)
        print("ğŸ”€ Step 6: Hybrid Search ë¶„ì„")
        print("=" * 60)

        hybrid_searcher = HybridSearchSystem(rag, sparse_method=config.sparse_method)

        test_query = search_query
        print(f"\nğŸ” ê²€ìƒ‰ì–´: '{test_query}'")
        print("-" * 40)

        search_results = hybrid_searcher.compare_all(test_query, k=5, alpha=0.5)

        sparse_name = config.sparse_method.upper()

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ”µ Sparse Search ({sparse_name}) ê²°ê³¼:")
        for i, r in enumerate(search_results['sparse'][:3], 1):
            sparse_score = r['score']
            source = r['source'][:50]
            print(f"   [{i}] {sparse_name}: {sparse_score:.2f} | {source}...")

        print(f"\nğŸ”´ Dense Search ({config.embedding_model}) ê²°ê³¼:")
        for i, r in enumerate(search_results['dense'][:3], 1):
            l2_dist = r['score']
            source = r['source'][:50]
            print(f"   [{i}] L2 Dist: {l2_dist:.4f} | {source}...")

        print(f"\nğŸŸ¢ Hybrid Search ê²°ê³¼ ({sparse_name} + {config.embedding_model}, Î±=0.5):")
        for i, r in enumerate(search_results['hybrid'][:3], 1):
            sparse_raw = r.get('sparse_score', 0)
            sparse_norm = r.get('sparse_score_norm', 0)
            sem_norm = r.get('dense_score_norm', 0)
            hybrid_score = r.get('hybrid_score', 0)
            source = r['source'][:50]
            print(f"   [{i}] Hybrid: {hybrid_score:.2f} | {sparse_name}={sparse_raw:.1f}({sparse_norm:.2f}) + Semantic({sem_norm:.2f})")
            print(f"       {source}...")

        # ì‹œê°í™” ì €ì¥ ë° í‘œì‹œ
        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        hybrid_searcher.visualize_comparison(test_query, k=5, alpha=0.5)

    # 9. ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ
    interactive_qa(rag, config.openai_api_key)

    print("\n" + "=" * 60)
    print("âœ… RAG ì‹œìŠ¤í…œ ì¢…ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
