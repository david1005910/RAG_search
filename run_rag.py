#!/usr/bin/env python3
"""
Medical/Scientific Paper RAG System
- ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ì§€ì›
- ë…¼ë¬¸ ìš”ì•½ ê¸°ëŠ¥ (OpenAI API)
- ë‹¤êµ­ì–´ ì§€ì› (ì˜ì–´/í•œêµ­ì–´)
- .env íŒŒì¼ì„ í†µí•œ API í‚¤ ê´€ë¦¬
"""
#!pip install langchain langchain-community langchain-text-splitters langchain-openai faiss-cpu

import os
import requests
import time
import re
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ë…¼ë¬¸ ê²€ìƒ‰
import arxiv
import xmltodict
# PDF ì²˜ë¦¬
from PyPDF2 import PdfReader
import pdfplumber

# LangChain ê´€ë ¨
from langchain_text_splitters import RecursiveCharacterTextSplitter
#from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
import warnings
warnings.filterwarnings('ignore')

# ë””ë ‰í† ë¦¬ ì„¤ì •
PAPERS_DIR = "./papers"
VECTORSTORE_DIR = "./vectorstore"
os.makedirs(PAPERS_DIR, exist_ok=True)
os.makedirs(VECTORSTORE_DIR, exist_ok=True)


# ==================== ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ====================
def detect_language(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€ (í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´)"""
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    total_chars = len(re.findall(r'[a-zA-Zê°€-í£]', text))

    if total_chars == 0:
        return 'en'

    korean_ratio = korean_chars / total_chars
    return 'ko' if korean_ratio > 0.3 else 'en'


def translate_to_english(text: str, openai_api_key: str = None) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ (ê²€ìƒ‰ìš©)"""
    if not openai_api_key:
        # API í‚¤ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì˜í•™ ìš©ì–´ ë§¤í•‘ ì‚¬ìš©
        medical_terms = {
            'ë‹¹ë‡¨ë³‘': 'diabetes mellitus',
            'ë‹¹ë‡¨': 'diabetes',
            'ì¹˜ë£Œ': 'treatment',
            'ì¹˜ë£Œë²•': 'treatment therapy',
            'ì•”': 'cancer',
            'íì•”': 'lung cancer',
            'ìœ ë°©ì•”': 'breast cancer',
            'ìœ„ì•”': 'gastric cancer stomach cancer',
            'ê°„ì•”': 'liver cancer hepatocellular carcinoma',
            'ëŒ€ì¥ì•”': 'colon cancer colorectal cancer',
            'ê³ í˜ˆì••': 'hypertension',
            'ì‹¬ì¥ë³‘': 'heart disease cardiovascular disease',
            'ë‡Œì¡¸ì¤‘': 'stroke cerebrovascular accident',
            'ì¹˜ë§¤': 'dementia alzheimer',
            'ìš°ìš¸ì¦': 'depression',
            'ë¹„ë§Œ': 'obesity',
            'ê³¨ë‹¤ê³µì¦': 'osteoporosis',
            'ê´€ì ˆì—¼': 'arthritis',
            'ì²œì‹': 'asthma',
            'ì•Œë ˆë¥´ê¸°': 'allergy',
            'ê°ì—¼': 'infection',
            'ë°”ì´ëŸ¬ìŠ¤': 'virus viral',
            'ë°±ì‹ ': 'vaccine vaccination',
            'í•­ìƒì œ': 'antibiotic',
            'ë©´ì—­': 'immunity immune',
            'ì§„ë‹¨': 'diagnosis diagnostic',
            'ì˜ˆë°©': 'prevention preventive',
            'ì¦ìƒ': 'symptoms',
            'ë¶€ì‘ìš©': 'side effects adverse effects',
            'ì„ìƒì‹œí—˜': 'clinical trial',
            'ì•½ë¬¼': 'drug medication',
            'ìˆ˜ìˆ ': 'surgery surgical',
            'ë°©ì‚¬ì„ ': 'radiation radiotherapy',
            'í™”í•™ìš”ë²•': 'chemotherapy',
        }

        translated = text
        for ko, en in medical_terms.items():
            if ko in translated:
                translated = translated.replace(ko, en)

        # ë‚¨ì€ í•œê¸€ ì œê±°
        translated = re.sub(r'[ê°€-í£]+', '', translated).strip()
        return translated if translated else text

    try:
        from openai import OpenAI
        client = OpenAI(api_key=openai_api_key)

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a medical/scientific translator. Translate the following Korean medical query to English. Only output the English translation, nothing else."},
                {"role": "user", "content": text}
            ],
            max_tokens=100,
            temperature=0.1
        )

        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"   âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©: {str(e)[:30]}")
        return translate_to_english(text, None)  # API ì—†ì´ ì¬ì‹œë„


# ==================== ì„¤ì • í´ë˜ìŠ¤ ====================
class Config:
    """RAG ì‹œìŠ¤í…œ ì„¤ì •"""
    def __init__(self):
        self.search_source = 'pubmed'
        self.search_mode = 3  # 1: ê²€ìƒ‰ë¶„ì•¼ë§Œ, 2: ê²€ìƒ‰ë‚´ìš©ë§Œ(vectorDB), 3: ê²€ìƒ‰ë¶„ì•¼+ê²€ìƒ‰ë‚´ìš©
        self.search_field = ''  # ê²€ìƒ‰ë¶„ì•¼ (ì˜ˆ: cardiology, oncology)
        self.search_content = ''  # ê²€ìƒ‰ë‚´ìš© (êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´)
        self.search_query = ''  # ì›ë³¸ ê²€ìƒ‰ì–´ (ê²€ìƒ‰ë¶„ì•¼ + ê²€ìƒ‰ë‚´ìš©)
        self.search_query_en = ''  # ì˜ì–´ë¡œ ë²ˆì—­ëœ ê²€ìƒ‰ì–´ (ì‹¤ì œ ê²€ìƒ‰ìš©)
        self.max_results = 5
        self.embedding_model = 'pubmedbert'  # ê¸°ë³¸ê°’ì„ PubMedBERTë¡œ ë³€ê²½
        self.sparse_method = 'bm25'  # 'bm25' ë˜ëŠ” 'splade'
        self.vector_db = 'faiss'  # 'faiss' ë˜ëŠ” 'qdrant'
        self.chunk_size = 1000
        self.chunk_overlap = 200
        # Reranking ì„¤ì •
        self.use_reranking = False
        self.reranker_model = 'ms-marco'  # 'ms-marco', 'ms-marco-large', 'bge-reranker', 'bge-reranker-large'
        # LangSmith ì„¤ì •
        self.use_langsmith = False
        self.langsmith_project = 'medical-paper-rag'
        self.langchain_api_key = os.getenv('LANGCHAIN_API_KEY') or None
        # .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
        self.pubmed_api_key = os.getenv('PUBMED_API_KEY') or None
        self.pubmed_email = os.getenv('PUBMED_EMAIL') or None
        self.openai_api_key = os.getenv('OPENAI_API_KEY') or None
        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY') or None
        self.language = 'en'  # ê°ì§€ëœ ì–¸ì–´
        self.summary_language = 'ko'  # ìš”ì•½ ì¶œë ¥ ì–¸ì–´ (ko: í•œêµ­ì–´, en: English)

    def interactive_setup(self):
        """ëŒ€í™”í˜• ì„¤ì •"""
        print("\n" + "=" * 60)
        print("âš™ï¸  RAG ì‹œìŠ¤í…œ ì„¤ì •")
        print("=" * 60)

        # ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)
        print("\nğŸ“– ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ (ë³µìˆ˜ ì„ íƒ: ì½¤ë§ˆë¡œ êµ¬ë¶„, ì˜ˆ: 1,2,3):")
        print("   1. PubMed (ì˜í•™/ìƒë¬¼í•™, NCBI)")
        print("   2. arXiv (CS/ë¬¼ë¦¬/ìˆ˜í•™, í”„ë¦¬í”„ë¦°íŠ¸)")
        print("   3. Europe PMC (ìœ ëŸ½ ì˜í•™/ìƒëª…ê³¼í•™, Open Access)")
        print("   4. ì „ì²´ (PubMed + arXiv + Europe PMC)")
        choice = input("ì„ íƒ [1]: ").strip() or "1"

        # ë³µìˆ˜ ì„ íƒ ì²˜ë¦¬
        source_map = {'1': 'pubmed', '2': 'arxiv', '3': 'europepmc', '4': 'all'}
        if ',' in choice:
            # ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë³µìˆ˜ ì„ íƒ
            selected = []
            for c in choice.split(','):
                c = c.strip()
                if c in source_map:
                    selected.append(source_map[c])
            self.search_source = ','.join(selected) if selected else 'pubmed'
        else:
            self.search_source = source_map.get(choice, 'pubmed')

        # ê²€ìƒ‰ ë°©ì‹ ì„ íƒ
        print("\nğŸ” ê²€ìƒ‰ ë°©ì‹ ì„ íƒ:")
        print("   1. ê²€ìƒ‰ë¶„ì•¼ë§Œ (ìƒˆ ë…¼ë¬¸ ê²€ìƒ‰)")
        print("   2. ê²€ìƒ‰ë‚´ìš©ë§Œ (ê¸°ì¡´ VectorDBì—ì„œ ê²€ìƒ‰)")
        print("   3. ê²€ìƒ‰ë¶„ì•¼ + ê²€ìƒ‰ë‚´ìš© (ìƒˆ ë…¼ë¬¸ ê²€ìƒ‰) [ê¸°ë³¸ê°’]")
        mode_choice = input("ì„ íƒ [3]: ").strip() or "3"
        self.search_mode = int(mode_choice) if mode_choice in ['1', '2', '3'] else 3

        mode_names = {1: "ê²€ìƒ‰ë¶„ì•¼ë§Œ", 2: "ê²€ìƒ‰ë‚´ìš©ë§Œ (VectorDB)", 3: "ê²€ìƒ‰ë¶„ì•¼ + ê²€ìƒ‰ë‚´ìš©"}
        print(f"   âœ“ ì„ íƒëœ ë°©ì‹: {mode_names[self.search_mode]}")

        # ê²€ìƒ‰ ë°©ì‹ì— ë”°ë¥¸ ì…ë ¥ ì²˜ë¦¬
        if self.search_mode == 1:
            # ê²€ìƒ‰ë¶„ì•¼ë§Œ ì…ë ¥
            self.search_field = input("\nğŸ“‚ ê²€ìƒ‰ë¶„ì•¼ ì…ë ¥ (ì˜ˆ: cardiology, oncology, ì‹¬ì¥í•™): ").strip()
            if not self.search_field:
                self.search_field = "COVID-19"
                print(f"   ê¸°ë³¸ê°’ ì‚¬ìš©: {self.search_field}")
            self.search_content = ''
            self.search_query = self.search_field

        elif self.search_mode == 2:
            # VectorDB ê²€ìƒ‰ ëª¨ë“œ - ê²€ìƒ‰ë‚´ìš©ì€ Q&A ëª¨ë“œì—ì„œ ì…ë ¥
            self.search_field = ''
            self.search_content = ''
            self.search_query = ''
            print("   â„¹ï¸ VectorDB ëª¨ë“œ: Q&Aì—ì„œ ê²€ìƒ‰ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”")

        else:  # self.search_mode == 3
            # ê²€ìƒ‰ë¶„ì•¼ + ê²€ìƒ‰ë‚´ìš© ëª¨ë‘ ì…ë ¥
            self.search_field = input("\nğŸ“‚ ê²€ìƒ‰ë¶„ì•¼ ì…ë ¥ (ì˜ˆ: cardiology, oncology, ì‹¬ì¥í•™): ").strip()
            if not self.search_field:
                self.search_field = "COVID-19"
                print(f"   ê¸°ë³¸ê°’ ì‚¬ìš©: {self.search_field}")

            self.search_content = input("ğŸ” ê²€ìƒ‰ë‚´ìš© ì…ë ¥ (ì˜ˆ: treatment, diagnosis, ì¹˜ë£Œ): ").strip()
            if not self.search_content:
                self.search_content = "vaccine efficacy"
                print(f"   ê¸°ë³¸ê°’ ì‚¬ìš©: {self.search_content}")

            self.search_query = f"{self.search_field} {self.search_content}"

        # ëª¨ë“œ 2ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê²€ìƒ‰ì–´ ì¶œë ¥ ë° ì–¸ì–´ ê°ì§€
        if self.search_mode != 2:
            print(f"   ğŸ“‹ ê²€ìƒ‰ì–´: {self.search_query}")

            # ì–¸ì–´ ê°ì§€
            self.language = detect_language(self.search_query)
            lang_name = "í•œêµ­ì–´" if self.language == 'ko' else "English"
            print(f"   ğŸŒ ê°ì§€ëœ ì–¸ì–´: {lang_name}")

            # ìš”ì•½ ì¶œë ¥ ì–¸ì–´ ì„ íƒ
            print("\nğŸŒ ìš”ì•½ ì¶œë ¥ ì–¸ì–´ ì„ íƒ:")
            print("   1. í•œêµ­ì–´ (Korean) [ê¸°ë³¸ê°’]")
            print("   2. English")
            summary_lang_choice = input("ì„ íƒ [1]: ").strip() or "1"
            self.summary_language = 'en' if summary_lang_choice == '2' else 'ko'
            summary_lang_name = "í•œêµ­ì–´" if self.summary_language == 'ko' else "English"
            print(f"   âœ“ ìš”ì•½ ì–¸ì–´: {summary_lang_name}")

            # ìµœëŒ€ ê²°ê³¼ ìˆ˜
            max_res = input(f"\nğŸ“„ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜ [{self.max_results}]: ").strip()
            if max_res.isdigit():
                self.max_results = int(max_res)
        else:
            # ëª¨ë“œ 2: ê¸°ë³¸ ì–¸ì–´ ì„¤ì •
            self.language = 'en'
            lang_name = "English"

        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
        print("\nğŸ§  Dense ì„ë² ë”© ëª¨ë¸ ì„ íƒ:")
        print("   [PubMed/ì˜í•™ íŠ¹í™” - ë¡œì»¬ ì‹¤í–‰]")
        print("   1. PubMedBERT Full (PubMed ì „ë¬¸ í•™ìŠµ) [ê¶Œì¥]")
        print("   2. PubMedBERT Abstract (PubMed ì´ˆë¡ íŠ¹í™”)")
        print("   3. BioBERT (ì˜í•™/ìƒë¬¼í•™)")
        print("   4. BioLinkBERT (ì˜í•™ ë¬¸í—Œ ë§í¬ í•™ìŠµ)")
        print("   5. SciBERT (ê³¼í•™ ë…¼ë¬¸ ì „ë°˜)")
        print("   [ì¼ë°˜ ëª¨ë¸]")
        print("   6. BERT-base (ì¼ë°˜)")
        print("   [OpenAI API - ë¹ ë¥´ê³  ì •í™•]")
        print("   7. OpenAI Small (ë¹ ë¦„, ì €ë ´)")
        print("   8. OpenAI Large (ê³ ì„±ëŠ¥)")
        model_choice = input("ì„ íƒ [1 - PubMedBERT]: ").strip() or "1"
        self.embedding_model = {
            '1': 'pubmedbert',
            '2': 'pubmedbert-abs',
            '3': 'biobert',
            '4': 'biolinkbert',
            '5': 'scibert',
            '6': 'bert-base',
            '7': 'openai-small',
            '8': 'openai-large'
        }.get(model_choice, 'pubmedbert')

        # Sparse ê²€ìƒ‰ ë°©ì‹ ì„ íƒ
        print("\nğŸ” Sparse ê²€ìƒ‰ ë°©ì‹ ì„ íƒ:")
        print("   1. BM25 (ì „í†µì  í‚¤ì›Œë“œ ë§¤ì¹­) [ê¸°ë³¸ê°’]")
        print("   2. SPLADE (ì‹ ê²½ë§ ê¸°ë°˜ í™•ì¥ ê²€ìƒ‰)")
        sparse_choice = input("ì„ íƒ [1 - BM25]: ").strip() or "1"
        self.sparse_method = {
            '1': 'bm25',
            '2': 'splade'
        }.get(sparse_choice, 'bm25')

        # Vector DB ì„ íƒ
        print("\nğŸ—„ï¸ Vector DB ì„ íƒ:")
        print("   1. FAISS (Facebook AI, ë¡œì»¬ íŒŒì¼ ì €ì¥) [ê¸°ë³¸ê°’]")
        print("   2. Qdrant (Named Vectors, HNSW ì¸ë±ìŠ¤, Payload í•„í„°ë§)")
        db_choice = input("ì„ íƒ [1 - FAISS]: ").strip() or "1"
        self.vector_db = {
            '1': 'faiss',
            '2': 'qdrant'
        }.get(db_choice, 'faiss')

        # Reranking ì„¤ì •
        print("\nğŸ¯ Reranking ì„¤ì • (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ):")
        print("   1. ì‚¬ìš© ì•ˆí•¨ [ê¸°ë³¸ê°’]")
        print("   2. MS-MARCO MiniLM (ë¹ ë¦„)")
        print("   3. MS-MARCO MiniLM Large (ì •í™•)")
        print("   4. BGE Reranker (ë‹¤êµ­ì–´)")
        print("   5. BGE Reranker Large (ê³ ì •í™•ë„)")
        rerank_choice = input("ì„ íƒ [1]: ").strip() or "1"
        if rerank_choice == '1':
            self.use_reranking = False
        else:
            self.use_reranking = True
            self.reranker_model = {
                '2': 'ms-marco',
                '3': 'ms-marco-large',
                '4': 'bge-reranker',
                '5': 'bge-reranker-large'
            }.get(rerank_choice, 'ms-marco')

        # PubMed API ì„¤ì •
        if self.search_source in ['pubmed', 'both']:
            print("\nğŸ”‘ PubMed API ì„¤ì • (ì„ íƒì‚¬í•­ - ì†ë„ í–¥ìƒ):")
            if self.pubmed_api_key:
                print(f"   âœ… .envì—ì„œ ë¡œë“œë¨ (Key: {self.pubmed_api_key[:8]}...)")
            else:
                print("   API í‚¤ê°€ ì—†ìœ¼ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
                print("   ë°œê¸‰: https://www.ncbi.nlm.nih.gov/account/settings/")
                self.pubmed_api_key = input("   API Key: ").strip() or None
                if self.pubmed_api_key:
                    self.pubmed_email = input("   Email: ").strip() or None

        # OpenAI API ì„¤ì • (ë…¼ë¬¸ ìš”ì•½ ë° ë²ˆì—­ìš©)
        print("\nğŸ¤– OpenAI API ì„¤ì • (ë…¼ë¬¸ ìš”ì•½ ë° í•œâ†’ì˜ ë²ˆì—­ìš©):")
        if self.openai_api_key:
            print(f"   âœ… .envì—ì„œ ë¡œë“œë¨ (Key: {self.openai_api_key[:12]}...)")
        else:
            print("   API í‚¤ê°€ ì—†ìœ¼ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            print("   ë°œê¸‰: https://platform.openai.com/api-keys")
            self.openai_api_key = input("   OpenAI API Key: ").strip() or None

        # LangSmith ì„¤ì • (ì›¹ ëª¨ë‹ˆí„°ë§)
        print("\nğŸ“Š LangSmith ì„¤ì • (ì›¹ ê¸°ë°˜ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§):")
        print("   LangSmithë¡œ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤ì‹œê°„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   ë°œê¸‰: https://smith.langchain.com")
        if self.langchain_api_key:
            print(f"   âœ… .envì—ì„œ ë¡œë“œë¨ (Key: {self.langchain_api_key[:12]}...)")
            langsmith_choice = input("   LangSmith í™œì„±í™”? [y/N]: ").strip().lower()
            self.use_langsmith = langsmith_choice in ['y', 'yes', 'ì˜ˆ']
        else:
            print("   API í‚¤ê°€ ì—†ìœ¼ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            self.langchain_api_key = input("   LangChain API Key: ").strip() or None
            if self.langchain_api_key:
                self.use_langsmith = True
                project_name = input(f"   í”„ë¡œì íŠ¸ ì´ë¦„ [{self.langsmith_project}]: ").strip()
                if project_name:
                    self.langsmith_project = project_name

        # í•œêµ­ì–´ì¸ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­ (ëª¨ë“œ 2ëŠ” ë²ˆì—­ ë¶ˆí•„ìš”)
        if self.language == 'ko' and self.search_mode != 2:
            print("\nğŸ”„ í•œêµ­ì–´ ê²€ìƒ‰ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘...")
            self.search_query_en = translate_to_english(self.search_query, self.openai_api_key)
            print(f"   ğŸ‡°ğŸ‡· ì›ë³¸: {self.search_query}")
            print(f"   ğŸ‡ºğŸ‡¸ ë²ˆì—­: {self.search_query_en}")
        else:
            self.search_query_en = self.search_query

        mode_names = {1: "ê²€ìƒ‰ë¶„ì•¼ë§Œ (ìƒˆ ë…¼ë¬¸ ê²€ìƒ‰)", 2: "ê²€ìƒ‰ë‚´ìš©ë§Œ (VectorDB ê²€ìƒ‰)", 3: "ê²€ìƒ‰ë¶„ì•¼ + ê²€ìƒ‰ë‚´ìš© (ìƒˆ ë…¼ë¬¸ ê²€ìƒ‰)"}
        print("\n" + "-" * 60)
        print("âœ… ì„¤ì • ì™„ë£Œ!")
        print(f"   ğŸ” ê²€ìƒ‰ë°©ì‹: {mode_names[self.search_mode]}")
        if self.search_mode == 2:
            print(f"   ğŸ’¡ Q&A ëª¨ë“œì—ì„œ ê²€ìƒ‰ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”")
        else:
            print(f"   ğŸ“– ì†ŒìŠ¤: {self.search_source}")
            if self.search_field:
                print(f"   ğŸ“‚ ê²€ìƒ‰ë¶„ì•¼: {self.search_field}")
            if self.search_content:
                print(f"   ğŸ” ê²€ìƒ‰ë‚´ìš©: {self.search_content}")
            if self.language == 'ko':
                print(f"   ğŸ“‹ ê²€ìƒ‰ì–´(ì˜ë¬¸): {self.search_query_en}")
            print(f"   ğŸŒ ì–¸ì–´: {lang_name} (ì‘ë‹µë„ {lang_name}ë¡œ)")
            print(f"   ğŸ“„ ìµœëŒ€ ë…¼ë¬¸: {self.max_results}")
        print(f"   ğŸ§  Dense ëª¨ë¸: {self.embedding_model}")
        print(f"   ğŸ”¤ Sparse ë°©ì‹: {self.sparse_method.upper()}")
        print(f"   ğŸ—„ï¸ Vector DB: {self.vector_db.upper()}")
        if self.use_reranking:
            print(f"   ğŸ¯ Reranking: {self.reranker_model.upper()}")
        else:
            print(f"   ğŸ¯ Reranking: ì‚¬ìš© ì•ˆí•¨")
        if self.pubmed_api_key:
            print(f"   ğŸ”‘ PubMed API: ì„¤ì •ë¨")
        if self.openai_api_key:
            print(f"   ğŸ¤– OpenAI API: ì„¤ì •ë¨ (ìš”ì•½/ë²ˆì—­ í™œì„±í™”)")
        if self.use_langsmith:
            print(f"   ğŸ“Š LangSmith: í™œì„±í™” ({self.langsmith_project})")
        print("-" * 60)

        return self


# ==================== LangSmith ì„¤ì • ====================
_LANGSMITH_ENABLED = False

def setup_langsmith(config: Config) -> bool:
    """LangSmith í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”"""
    global _LANGSMITH_ENABLED

    if not config.use_langsmith:
        return False

    if not config.langchain_api_key:
        print("\nâš ï¸ LangSmith API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— LANGCHAIN_API_KEYë¥¼ ì¶”ê°€í•˜ê±°ë‚˜")
        print("   https://smith.langchain.com ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.")
        config.use_langsmith = False
        return False

    try:
        # LangSmith í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_PROJECT"] = config.langsmith_project
        os.environ["LANGCHAIN_API_KEY"] = config.langchain_api_key
        os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"

        _LANGSMITH_ENABLED = True

        print(f"\nâœ… LangSmith í™œì„±í™”ë¨")
        print(f"   ğŸ“Š í”„ë¡œì íŠ¸: {config.langsmith_project}")
        print(f"   ğŸŒ ëŒ€ì‹œë³´ë“œ: https://smith.langchain.com/o/default/projects")

        return True
    except Exception as e:
        print(f"\nâš ï¸ LangSmith ì„¤ì • ì‹¤íŒ¨: {str(e)[:50]}")
        config.use_langsmith = False
        return False


def get_langsmith_callback():
    """LangSmith ì½œë°± í•¸ë“¤ëŸ¬ ë°˜í™˜"""
    try:
        from langchain_core.tracers import LangChainTracer
        return LangChainTracer()
    except ImportError:
        return None


def traceable_wrapper(run_type: str = "chain", name: str = None):
    """LangSmith traceable ë°ì½”ë ˆì´í„° ë˜í¼ - ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ë¬´ì‹œ"""
    def decorator(func):
        # LangSmithê°€ í™œì„±í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì›ë³¸ í•¨ìˆ˜ ë°˜í™˜
        if not os.environ.get("LANGCHAIN_TRACING_V2") == "true":
            return func

        try:
            from langsmith import traceable
            trace_name = name or func.__name__
            return traceable(run_type=run_type, name=trace_name)(func)
        except ImportError:
            return func
    return decorator


def get_traceable():
    """langsmith traceable ë°ì½”ë ˆì´í„° ë°˜í™˜ (ì—†ìœ¼ë©´ ë”ë¯¸ ë°˜í™˜)"""
    if os.environ.get("LANGCHAIN_TRACING_V2") == "true":
        try:
            from langsmith import traceable
            return traceable
        except ImportError:
            pass

    # ë”ë¯¸ ë°ì½”ë ˆì´í„°
    def dummy_traceable(*args, **kwargs):
        def decorator(func):
            return func
        return decorator
    return dummy_traceable


# ì „ì—­ traceable ë°ì½”ë ˆì´í„° (import ì‹œì ì— ì„¤ì •)
try:
    if os.environ.get("LANGCHAIN_TRACING_V2") == "true":
        from langsmith import traceable as ls_traceable
    else:
        def ls_traceable(*args, **kwargs):
            def decorator(func):
                return func
            return decorator
except ImportError:
    def ls_traceable(*args, **kwargs):
        def decorator(func):
            return func
        return decorator


class LangSmithTracer:
    """LangSmith ì¶”ì ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (langsmith 0.5+ í˜¸í™˜)"""

    def __init__(self, name: str, run_type: str = "chain", metadata: dict = None):
        self.name = name
        self.run_type = run_type
        self.metadata = metadata or {}
        self.run_id = None
        self.client = None
        self.inputs = {}
        self.outputs = {}
        self.start_time = None

    def __enter__(self):
        if not os.environ.get("LANGCHAIN_TRACING_V2") == "true":
            return self

        try:
            from langsmith import Client
            import uuid
            from datetime import datetime

            self.client = Client()
            self.run_id = str(uuid.uuid4())
            self.start_time = datetime.utcnow()

        except ImportError:
            pass
        except Exception:
            pass
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.client and self.run_id:
            try:
                from datetime import datetime

                self.client.create_run(
                    name=self.name,
                    run_type=self.run_type,
                    id=self.run_id,
                    inputs=self.inputs,
                    outputs=self.outputs,
                    start_time=self.start_time,
                    end_time=datetime.utcnow(),
                    extra={"metadata": self.metadata},
                    error=str(exc_val) if exc_val else None
                )
            except Exception:
                pass
        return False

    def log_input(self, inputs: dict):
        """ì…ë ¥ ë¡œê¹…"""
        self.inputs.update(inputs)

    def log_output(self, outputs: dict):
        """ì¶œë ¥ ë¡œê¹…"""
        self.outputs.update(outputs)


# ==================== LangChain Agent ====================
class RAGAgent:
    """LangChain Agentë¥¼ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ"""

    def __init__(self, openai_api_key: str = None, rag_system=None):
        self.openai_api_key = openai_api_key
        self.rag_system = rag_system
        self.agent = None
        self.tools = []

    def _create_tools(self):
        """ì—ì´ì „íŠ¸ ë„êµ¬ ìƒì„±"""
        from langchain.tools import Tool

        tools = []

        # ë‚ ì”¨ ë„êµ¬ (ì˜ˆì‹œ)
        def get_weather(city: str) -> str:
            """Get weather for a given city."""
            return f"It's always sunny in {city}!"

        tools.append(Tool(
            name="get_weather",
            func=get_weather,
            description="Get the current weather for a city"
        ))

        # RAG ê²€ìƒ‰ ë„êµ¬
        if self.rag_system:
            def search_papers(query: str) -> str:
                """Search medical/scientific papers."""
                results = self.rag_system.search(query, k=3)
                if not results:
                    return "No relevant papers found."
                response = "Found papers:\n"
                for i, r in enumerate(results, 1):
                    response += f"{i}. {r['source'][:50]}...\n   {r['content'][:200]}...\n\n"
                return response

            tools.append(Tool(
                name="search_papers",
                func=search_papers,
                description="Search medical and scientific papers for information"
            ))

        self.tools = tools
        return tools

    def create_agent(self):
        """LangChain Agent ìƒì„±"""
        if not self.openai_api_key:
            print("âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return None

        try:
            from langchain_openai import ChatOpenAI
            from langchain.agents import AgentExecutor, create_react_agent
            from langchain import hub

            # LLM ì„¤ì •
            llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0,
                api_key=self.openai_api_key
            )

            # ë„êµ¬ ìƒì„±
            tools = self._create_tools()

            # ReAct í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            prompt = hub.pull("hwchase17/react")

            # Agent ìƒì„±
            agent = create_react_agent(llm, tools, prompt)

            # Agent Executor ìƒì„±
            self.agent = AgentExecutor(
                agent=agent,
                tools=tools,
                verbose=True,
                handle_parsing_errors=True
            )

            print("âœ… LangChain Agent ìƒì„± ì™„ë£Œ!")
            return self.agent

        except ImportError as e:
            print(f"âš ï¸ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install langchain langchain-openai langchainhub")
            print(f"   ì˜¤ë¥˜: {str(e)[:50]}")
            return None
        except Exception as e:
            print(f"âš ï¸ Agent ìƒì„± ì‹¤íŒ¨: {str(e)[:50]}")
            return None

    def invoke(self, query: str) -> str:
        """Agent ì‹¤í–‰"""
        if not self.agent:
            self.create_agent()

        if not self.agent:
            return "Agentë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        try:
            # LangSmith íŠ¸ë ˆì´ì‹±
            with LangSmithTracer("RAG_Agent", run_type="chain",
                                metadata={"query": query}) as tracer:
                tracer.log_input({"query": query})

                result = self.agent.invoke({"input": query})
                output = result.get("output", "No response")

                tracer.log_output({"output": output})

            return output
        except Exception as e:
            return f"Agent ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"

    def chat(self):
        """ëŒ€í™”í˜• Agent ì„¸ì…˜"""
        print("\n" + "=" * 60)
        print("ğŸ¤– LangChain Agent ëŒ€í™” ëª¨ë“œ")
        print("=" * 60)
        print("   'quit' ë˜ëŠ” 'exit'ë¡œ ì¢…ë£Œ")
        print("-" * 60)

        while True:
            query = input("\nğŸ§‘ You: ").strip()
            if query.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ Agent ì„¸ì…˜ ì¢…ë£Œ")
                break
            if not query:
                continue

            print("\nğŸ¤– Agent:")
            response = self.invoke(query)
            print(response)


# ==================== LangGraph RAG Workflow ====================
class LangGraphRAG:
    """LangGraph ê¸°ë°˜ RAG ì›Œí¬í”Œë¡œìš° - ì§„í–‰ ìƒíƒœ ì¶”ì  ê°€ëŠ¥"""

    def __init__(self, openai_api_key: str = None, rag_system=None, language: str = 'en'):
        self.openai_api_key = openai_api_key
        self.rag_system = rag_system
        self.language = language
        self.graph = None
        self.workflow_history = []

    def _create_state_schema(self):
        """ìƒíƒœ ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        from typing import TypedDict, Annotated, Sequence
        from langchain_core.messages import BaseMessage
        import operator

        class RAGState(TypedDict):
            """RAG ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
            query: str                          # ì‚¬ìš©ì ì§ˆë¬¸
            query_processed: str                # ì²˜ë¦¬ëœ ì§ˆë¬¸
            documents: list                     # ê²€ìƒ‰ëœ ë¬¸ì„œ
            context: str                        # ì»¨í…ìŠ¤íŠ¸
            answer: str                         # ìƒì„±ëœ ë‹µë³€
            sources: list                       # ì°¸ê³  ë¬¸ì„œ
            current_step: str                   # í˜„ì¬ ë‹¨ê³„
            steps_completed: list               # ì™„ë£Œëœ ë‹¨ê³„
            error: str                          # ì—ëŸ¬ ë©”ì‹œì§€

        return RAGState

    def build_graph(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""
        try:
            from langgraph.graph import StateGraph, END
            from langchain_openai import ChatOpenAI

            RAGState = self._create_state_schema()

            # LLM ì„¤ì •
            llm = None
            if self.openai_api_key:
                llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    temperature=0.3,
                    api_key=self.openai_api_key
                )

            # ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
            def process_query(state: RAGState) -> RAGState:
                """Step 1: ì¿¼ë¦¬ ì²˜ë¦¬"""
                print("   ğŸ“ [1/4] ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘...")
                query = state["query"]

                # ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
                lang = detect_language(query)
                if lang == 'ko' and self.openai_api_key:
                    query_processed = translate_to_english(query, self.openai_api_key)
                else:
                    query_processed = query

                self.workflow_history.append({
                    "step": "process_query",
                    "input": query,
                    "output": query_processed,
                    "status": "completed"
                })

                return {
                    **state,
                    "query_processed": query_processed,
                    "current_step": "process_query",
                    "steps_completed": state.get("steps_completed", []) + ["process_query"]
                }

            def retrieve_documents(state: RAGState) -> RAGState:
                """Step 2: ë¬¸ì„œ ê²€ìƒ‰"""
                print("   ğŸ” [2/4] ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
                query = state["query_processed"]
                documents = []

                if self.rag_system:
                    results = self.rag_system.search(query, k=5)
                    documents = results

                self.workflow_history.append({
                    "step": "retrieve_documents",
                    "input": query,
                    "output": f"{len(documents)} documents found",
                    "status": "completed"
                })

                return {
                    **state,
                    "documents": documents,
                    "current_step": "retrieve_documents",
                    "steps_completed": state.get("steps_completed", []) + ["retrieve_documents"]
                }

            def create_context(state: RAGState) -> RAGState:
                """Step 3: ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
                print("   ğŸ“š [3/4] ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
                documents = state.get("documents", [])

                context_parts = []
                sources = []
                for i, doc in enumerate(documents[:5]):
                    content = doc.get('content', '')[:500]
                    source = doc.get('source', f'Document {i+1}')
                    context_parts.append(f"[{i+1}] {content}")
                    sources.append(source)

                context = "\n\n".join(context_parts)

                self.workflow_history.append({
                    "step": "create_context",
                    "input": f"{len(documents)} documents",
                    "output": f"Context: {len(context)} chars",
                    "status": "completed"
                })

                return {
                    **state,
                    "context": context,
                    "sources": sources,
                    "current_step": "create_context",
                    "steps_completed": state.get("steps_completed", []) + ["create_context"]
                }

            def generate_answer(state: RAGState) -> RAGState:
                """Step 4: ë‹µë³€ ìƒì„±"""
                print("   ğŸ¤– [4/4] ë‹µë³€ ìƒì„± ì¤‘...")
                query = state["query"]
                context = state.get("context", "")
                answer = ""

                if llm and context:
                    try:
                        if self.language == 'ko':
                            system_msg = "ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                        else:
                            system_msg = "You are an expert answering questions based on medical/scientific papers. Answer based on the provided context."

                        from langchain_core.messages import SystemMessage, HumanMessage
                        messages = [
                            SystemMessage(content=system_msg),
                            HumanMessage(content=f"Context:\n{context}\n\nQuestion: {query}")
                        ]
                        response = llm.invoke(messages)
                        answer = response.content
                    except Exception as e:
                        answer = f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)[:50]}"
                else:
                    answer = "ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

                self.workflow_history.append({
                    "step": "generate_answer",
                    "input": query,
                    "output": answer[:100] + "...",
                    "status": "completed"
                })

                return {
                    **state,
                    "answer": answer,
                    "current_step": "generate_answer",
                    "steps_completed": state.get("steps_completed", []) + ["generate_answer"]
                }

            # ê·¸ë˜í”„ ìƒì„±
            workflow = StateGraph(RAGState)

            # ë…¸ë“œ ì¶”ê°€
            workflow.add_node("process_query", process_query)
            workflow.add_node("retrieve_documents", retrieve_documents)
            workflow.add_node("create_context", create_context)
            workflow.add_node("generate_answer", generate_answer)

            # ì—£ì§€ ì¶”ê°€ (ìˆœì°¨ ì‹¤í–‰)
            workflow.set_entry_point("process_query")
            workflow.add_edge("process_query", "retrieve_documents")
            workflow.add_edge("retrieve_documents", "create_context")
            workflow.add_edge("create_context", "generate_answer")
            workflow.add_edge("generate_answer", END)

            # ê·¸ë˜í”„ ì»´íŒŒì¼
            self.graph = workflow.compile()
            print("âœ… LangGraph RAG ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ!")

            return self.graph

        except ImportError as e:
            print(f"âš ï¸ LangGraph íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”: pip install langgraph langchain-openai")
            print(f"   ì˜¤ë¥˜: {str(e)[:50]}")
            return None
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None

    def invoke(self, query: str) -> Dict:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        if not self.graph:
            self.build_graph()

        if not self.graph:
            return {"error": "ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        self.workflow_history = []

        print("\n" + "=" * 50)
        print("ğŸ”„ LangGraph RAG ì›Œí¬í”Œë¡œìš° ì‹¤í–‰")
        print("=" * 50)

        # LangSmith íŠ¸ë ˆì´ì‹±
        with LangSmithTracer("LangGraph_RAG_Workflow", run_type="chain",
                            metadata={"query": query}) as tracer:
            tracer.log_input({"query": query})

            initial_state = {
                "query": query,
                "query_processed": "",
                "documents": [],
                "context": "",
                "answer": "",
                "sources": [],
                "current_step": "start",
                "steps_completed": [],
                "error": ""
            }

            try:
                result = self.graph.invoke(initial_state)
                tracer.log_output({
                    "answer": result.get("answer", "")[:200],
                    "sources_count": len(result.get("sources", []))
                })
                return result
            except Exception as e:
                error_msg = str(e)
                tracer.log_output({"error": error_msg})
                return {"error": error_msg}

    def get_workflow_status(self) -> List[Dict]:
        """ì›Œí¬í”Œë¡œìš° ì§„í–‰ ìƒíƒœ ë°˜í™˜"""
        return self.workflow_history

    def print_workflow_status(self):
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¶œë ¥"""
        print("\n" + "-" * 50)
        print("ğŸ“Š ì›Œí¬í”Œë¡œìš° ì§„í–‰ ìƒíƒœ")
        print("-" * 50)

        steps = ["process_query", "retrieve_documents", "create_context", "generate_answer"]
        step_names = {
            "process_query": "ì¿¼ë¦¬ ì²˜ë¦¬",
            "retrieve_documents": "ë¬¸ì„œ ê²€ìƒ‰",
            "create_context": "ì»¨í…ìŠ¤íŠ¸ ìƒì„±",
            "generate_answer": "ë‹µë³€ ìƒì„±"
        }

        for step in steps:
            history_item = next((h for h in self.workflow_history if h["step"] == step), None)
            if history_item:
                status = "âœ…" if history_item["status"] == "completed" else "âŒ"
                print(f"   {status} {step_names[step]}: {history_item['output'][:50]}")
            else:
                print(f"   â³ {step_names[step]}: ëŒ€ê¸° ì¤‘")

        print("-" * 50)

    def visualize_graph(self):
        """ê·¸ë˜í”„ ì‹œê°í™” (Mermaid í˜•ì‹)"""
        if not self.graph:
            print("âš ï¸ ê·¸ë˜í”„ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        print("\n" + "=" * 50)
        print("ğŸ“Š LangGraph RAG ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨")
        print("=" * 50)
        print("""
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   START         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ process_query   â”‚  â† ì¿¼ë¦¬ ì²˜ë¦¬ & ë²ˆì—­
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚retrieve_documentsâ”‚ â† ë²¡í„° DB ê²€ìƒ‰
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ create_context  â”‚  â† ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ generate_answer â”‚  â† LLM ë‹µë³€ ìƒì„±
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      END        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)

    def save_graph(self, filename: str = "langgraph_workflow"):
        """ê·¸ë˜í”„ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (Mermaid, PNG, HTML)"""
        if not self.graph:
            print("âš ï¸ ê·¸ë˜í”„ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        print("\nğŸ“Š ê·¸ë˜í”„ íŒŒì¼ ìƒì„± ì¤‘...")

        try:
            graph = self.graph.get_graph()

            # 1. Mermaid íŒŒì¼ ì €ì¥
            mermaid = graph.draw_mermaid()
            with open(f"{filename}.mmd", 'w') as f:
                f.write(mermaid)
            print(f"   âœ… {filename}.mmd ì €ì¥ ì™„ë£Œ")

            # 2. PNG ì´ë¯¸ì§€ ì €ì¥ ì‹œë„
            try:
                png_data = graph.draw_mermaid_png()
                with open(f"{filename}.png", 'wb') as f:
                    f.write(png_data)
                print(f"   âœ… {filename}.png ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸ PNG ì €ì¥ ì‹¤íŒ¨: {str(e)[:30]}")

            # 3. HTML íŒŒì¼ ì €ì¥
            html_content = f'''<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LangGraph RAG Workflow</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            margin: 0; padding: 20px; min-height: 100vh;
        }}
        .container {{
            max-width: 900px; margin: 0 auto; background: white;
            border-radius: 16px; box-shadow: 0 20px 60px rgba(0,0,0,0.3); padding: 40px;
        }}
        h1 {{ text-align: center; color: #1e293b; margin-bottom: 10px; }}
        .subtitle {{ text-align: center; color: #64748b; margin-bottom: 30px; }}
        .mermaid {{
            display: flex; justify-content: center; background: #f8fafc;
            border-radius: 12px; padding: 30px; margin: 20px 0;
        }}
        .steps {{
            margin-top: 30px; padding: 20px; background: #f1f5f9; border-radius: 12px;
        }}
        .step {{
            display: flex; align-items: center; padding: 12px 0;
            border-bottom: 1px solid #e2e8f0;
        }}
        .step:last-child {{ border-bottom: none; }}
        .step-num {{
            width: 30px; height: 30px; background: #0ea5e9; color: white;
            border-radius: 50%; display: flex; align-items: center;
            justify-content: center; font-weight: bold; margin-right: 15px;
        }}
        .step-content h3 {{ margin: 0; color: #1e293b; }}
        .step-content p {{ margin: 5px 0 0; color: #64748b; font-size: 14px; }}
        .footer {{ text-align: center; margin-top: 30px; color: #94a3b8; font-size: 14px; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ”„ LangGraph RAG Workflow</h1>
        <p class="subtitle">Medical/Scientific Paper RAG System</p>
        <div class="mermaid">
{mermaid}
        </div>
        <div class="steps">
            <div class="step">
                <div class="step-num">1</div>
                <div class="step-content">
                    <h3>process_query</h3>
                    <p>ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ë° ì–¸ì–´ ê°ì§€, í•„ìš”ì‹œ ì˜ì–´ë¡œ ë²ˆì—­</p>
                </div>
            </div>
            <div class="step">
                <div class="step-num">2</div>
                <div class="step-content">
                    <h3>retrieve_documents</h3>
                    <p>ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (FAISS/Qdrant)</p>
                </div>
            </div>
            <div class="step">
                <div class="step-num">3</div>
                <div class="step-content">
                    <h3>create_context</h3>
                    <p>ê²€ìƒ‰ëœ ë¬¸ì„œë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±</p>
                </div>
            </div>
            <div class="step">
                <div class="step-num">4</div>
                <div class="step-content">
                    <h3>generate_answer</h3>
                    <p>LLM (GPT-3.5)ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±</p>
                </div>
            </div>
        </div>
        <div class="footer">
            <p>ğŸ“Š LangSmith: <a href="https://smith.langchain.com">smith.langchain.com</a></p>
        </div>
    </div>
    <script>mermaid.initialize({{ startOnLoad: true, theme: 'base' }});</script>
</body>
</html>'''
            with open(f"{filename}.html", 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"   âœ… {filename}.html ì €ì¥ ì™„ë£Œ")

            print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
            print(f"   - {filename}.mmd (Mermaid)")
            print(f"   - {filename}.png (ì´ë¯¸ì§€)")
            print(f"   - {filename}.html (ì›¹ ë·°ì–´)")

        except Exception as e:
            print(f"âŒ ê·¸ë˜í”„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def chat(self):
        """ëŒ€í™”í˜• LangGraph RAG ì„¸ì…˜"""
        print("\n" + "=" * 60)
        print("ğŸ”„ LangGraph RAG ëŒ€í™” ëª¨ë“œ")
        print("=" * 60)
        print("   ëª…ë ¹ì–´:")
        print("   - 'status': ì›Œí¬í”Œë¡œìš° ìƒíƒœ ë³´ê¸°")
        print("   - 'graph': ê·¸ë˜í”„ ë‹¤ì´ì–´ê·¸ë¨ ë³´ê¸°")
        print("   - 'save': ê·¸ë˜í”„ íŒŒì¼ë¡œ ì €ì¥")
        print("   - 'quit': ì¢…ë£Œ")
        print("-" * 60)

        while True:
            query = input("\nğŸ§‘ ì§ˆë¬¸: ").strip()

            if query.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ ì„¸ì…˜ ì¢…ë£Œ")
                break
            if query.lower() == 'status':
                self.print_workflow_status()
                continue
            if query.lower() == 'graph':
                self.visualize_graph()
                continue
            if query.lower() == 'save':
                self.save_graph()
                continue
            if not query:
                continue

            result = self.invoke(query)

            if result.get("error"):
                print(f"\nâŒ ì˜¤ë¥˜: {result['error']}")
            else:
                print(f"\nğŸ¤– ë‹µë³€:\n{result.get('answer', 'No answer')}")
                if result.get('sources'):
                    print(f"\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")
                    for i, src in enumerate(result['sources'][:3], 1):
                        print(f"   [{i}] {src[:50]}...")

            self.print_workflow_status()


# ==================== ë…¼ë¬¸ ìš”ì•½ í´ë˜ìŠ¤ ====================
class PaperSummarizer:
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ìš”ì•½ (Full PDF vs Abstract êµ¬ë¶„)"""

    def __init__(self, api_key: str = None, language: str = 'en', summary_language: str = 'ko'):
        self.api_key = api_key
        self.language = language
        self.summary_language = summary_language  # ìš”ì•½ ì¶œë ¥ ì–¸ì–´

    def _is_full_paper(self, paper: Dict, documents: List[Dict]) -> tuple:
        """
        ë…¼ë¬¸ì´ Full PDFì¸ì§€ Abstractë§Œì¸ì§€ í™•ì¸

        Returns:
            (is_full: bool, content: str, source_file: str)
        """
        paper_content = ""
        source_file = ""

        for doc in documents:
            if paper['id'] in doc['source']:
                paper_content = doc['text']
                source_file = doc['source']
                break

        # Full PDF íŒë‹¨ ê¸°ì¤€:
        # 1. ë‚´ìš© ê¸¸ì´ê°€ 2000ì ì´ìƒ
        # 2. Abstract ì™¸ì˜ ë‚´ìš©ì´ í¬í•¨ë¨ (Introduction, Methods, Results ë“±)
        is_full = len(paper_content) > 2000

        # ì¶”ê°€ í™•ì¸: ë…¼ë¬¸ êµ¬ì¡° í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€
        structure_keywords = ['introduction', 'methods', 'results', 'discussion',
                            'conclusion', 'references', 'materials', 'background']
        content_lower = paper_content.lower()
        has_structure = sum(1 for kw in structure_keywords if kw in content_lower) >= 2

        is_full = is_full and has_structure

        return is_full, paper_content, source_file

    def _get_paper_urls(self, paper: Dict) -> Dict[str, str]:
        """ë…¼ë¬¸ ê´€ë ¨ URL ìˆ˜ì§‘"""
        urls = {}

        if paper.get('pubmed_url'):
            urls['PubMed'] = paper['pubmed_url']
        if paper.get('pmc_url'):
            urls['PMC (Full Text)'] = paper['pmc_url']
        if paper.get('europepmc_url'):
            urls['Europe PMC'] = paper['europepmc_url']
        if paper.get('pdf_url'):
            urls['PDF'] = paper['pdf_url']
        if paper.get('doi'):
            urls['DOI'] = f"https://doi.org/{paper['doi']}"

        # PubMed IDë¡œ URL ìƒì„±
        if paper.get('id') and paper['id'].startswith('PMID_'):
            pmid = paper['id'].replace('PMID_', '')
            if 'PubMed' not in urls:
                urls['PubMed'] = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
            if 'Europe PMC' not in urls:
                urls['Europe PMC'] = f"https://europepmc.org/article/MED/{pmid}"

        # PMC IDë¡œ URL ìƒì„±
        if paper.get('id') and paper['id'].startswith('PMC_'):
            pmcid = paper['id'].replace('PMC_', '')
            if 'PMC (Full Text)' not in urls:
                urls['PMC (Full Text)'] = f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/"
            if 'Europe PMC' not in urls:
                urls['Europe PMC'] = f"https://europepmc.org/article/PMC/{pmcid}"

        # Google Scholar ê²€ìƒ‰ ë§í¬
        urls['Google Scholar'] = f"https://scholar.google.com/scholar?q={paper['title'][:50].replace(' ', '+')}"

        # arXiv
        if paper.get('source') == 'arXiv' and paper.get('id'):
            arxiv_id = paper['id']
            urls['arXiv'] = f"https://arxiv.org/abs/{arxiv_id}"
            urls['arXiv PDF'] = f"https://arxiv.org/pdf/{arxiv_id}.pdf"

        # Europe PMC
        if paper.get('source') == 'Europe PMC':
            if paper.get('is_open_access'):
                urls['ğŸ”“ Open Access'] = paper.get('europepmc_url', '')

        return urls

    def summarize(self, papers: List[Dict], documents: List[Dict]) -> List[Dict]:
        """ë…¼ë¬¸ë“¤ì„ ìš”ì•½ (Full PDFëŠ” AI ìš”ì•½, Abstractë§Œ ìˆìœ¼ë©´ ë§í¬ ì œê³µ)"""
        if not self.api_key:
            print("\nâš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return papers

        print("\n" + "=" * 60)
        print("ğŸ“ ë…¼ë¬¸ ìš”ì•½ (OpenAI API)")
        print("=" * 60)

        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.api_key)
        except ImportError:
            print("âš ï¸ OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai")
            return papers
        except Exception as e:
            print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:50]}")
            return papers

        # ë…¼ë¬¸ ë¶„ë¥˜
        full_papers = []
        abstract_only = []

        for paper in papers:
            is_full, content, source = self._is_full_paper(paper, documents)
            paper['_is_full_paper'] = is_full
            paper['_content'] = content
            paper['_urls'] = self._get_paper_urls(paper)

            if is_full:
                full_papers.append(paper)
            else:
                abstract_only.append(paper)

        print(f"   ğŸ“„ Full Paper (AI ìš”ì•½): {len(full_papers)}ê°œ")
        print(f"   ğŸ“‹ Abstract Only (ë§í¬ ì œê³µ): {len(abstract_only)}ê°œ\n")

        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ (Full Paperìš©)
        if self.language == 'ko':
            system_prompt = """ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì„ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë…¼ë¬¸ì˜ ì „ì²´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìƒì„¸í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:
- ì—°êµ¬ ëª©ì : ì´ ì—°êµ¬ê°€ í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œ
- ì£¼ìš” ë°©ë²•: ì‚¬ìš©ëœ ì‹¤í—˜/ë¶„ì„ ë°©ë²•
- í•µì‹¬ ê²°ê³¼: ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ (ìˆ˜ì¹˜ í¬í•¨)
- ê²°ë¡  ë° ì˜ì˜: ì—°êµ¬ì˜ ì˜ë¯¸ì™€ í–¥í›„ ì „ë§"""
        else:
            system_prompt = """You are an expert at summarizing medical/scientific papers.
Based on the full paper content, provide a detailed summary.
Follow this format:
- Research Objective: The problem this research addresses
- Key Methods: Experimental/analytical methods used
- Main Results: Most important findings (include numbers)
- Conclusion & Significance: Implications and future directions"""

        summarized_papers = []
        full_success = 0
        full_fail = 0

        # Full Paper AI ìš”ì•½
        if full_papers:
            print("-" * 60)
            print("ğŸ¤– Full Paper AI ìš”ì•½ ì§„í–‰")
            print("-" * 60)

            with tqdm(
                total=len(full_papers),
                desc="   ğŸ¤– AI ìš”ì•½ ìƒì„±",
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
                colour='cyan'
            ) as pbar:
                for paper in full_papers:
                    title_short = paper['title'][:35] + "..." if len(paper['title']) > 35 else paper['title']
                    pbar.set_postfix_str(f"ğŸ“„ {title_short}")

                    content_to_summarize = f"""
Title: {paper['title']}
Authors: {', '.join(paper['authors'][:5])}
Published: {paper['published']}
Source: {paper['source']}

Full Paper Content:
{paper['_content'][:6000]}
"""

                    try:
                        with LangSmithTracer("Paper_Summary_LLM", run_type="llm",
                                            metadata={"paper_title": paper['title'][:50], "type": "full_paper"}) as tracer:
                            tracer.log_input({"system": system_prompt, "user": content_to_summarize[:500]})

                            response = client.chat.completions.create(
                                model="gpt-3.5-turbo",
                                messages=[
                                    {"role": "system", "content": system_prompt},
                                    {"role": "user", "content": content_to_summarize}
                                ],
                                max_tokens=800,  # Full paperëŠ” ë” ê¸´ ìš”ì•½
                                temperature=0.3
                            )

                            summary = response.choices[0].message.content
                            paper['summary'] = summary
                            paper['summary_type'] = 'full_paper'
                            full_success += 1

                            tracer.log_output({"summary": summary[:200]})

                    except Exception as e:
                        paper['summary'] = f"[ìš”ì•½ ì‹¤íŒ¨ - ì´ˆë¡ ì›ë¬¸]\n{paper['abstract']}"
                        paper['summary_type'] = 'fallback'
                        full_fail += 1

                    summarized_papers.append(paper)
                    pbar.update(1)
                    time.sleep(0.5)

        # Abstract Only ì²˜ë¦¬ (AI ìš”ì•½ìœ¼ë¡œ ì„ íƒëœ ì–¸ì–´ë¡œ ë³€í™˜)
        abstract_success = 0
        abstract_fail = 0

        if abstract_only:
            # ì„ íƒëœ ì–¸ì–´ì— ë”°ë¥¸ ì¶œë ¥ ë©”ì‹œì§€
            lang_label = "í•œêµ­ì–´" if self.summary_language == 'ko' else "English"
            print("\n" + "-" * 60)
            print(f"ğŸ“‹ Abstract Only ë…¼ë¬¸ â†’ AI {lang_label} ìš”ì•½")
            print("-" * 60)

            # Abstractìš© í”„ë¡¬í”„íŠ¸ (ì„ íƒëœ ì–¸ì–´ë¡œ ìš”ì•½)
            if self.summary_language == 'ko':
                abstract_prompt = """ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ ì´ˆë¡ì„ ìš”ì•½í•˜ê³  ë²ˆì—­í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë…¼ë¬¸ì˜ ì´ˆë¡(Abstract)ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

ìš”ì•½ í˜•ì‹:
- ì—°êµ¬ ë°°ê²½: ì´ ì—°êµ¬ì˜ ë°°ê²½ê³¼ í•„ìš”ì„±
- ì—°êµ¬ ë°©ë²•: ì‚¬ìš©ëœ ì£¼ìš” ë°©ë²•ë¡ 
- ì£¼ìš” ê²°ê³¼: í•µì‹¬ ë°œê²¬ ì‚¬í•­
- ê²°ë¡ : ì—°êµ¬ì˜ ì˜ì˜ì™€ ì‹œì‚¬ì """
            else:
                abstract_prompt = """You are an expert at summarizing medical/scientific paper abstracts.
Summarize the key points of the following abstract.

Format:
- Background: Research context and motivation
- Methods: Key methodology used
- Results: Main findings
- Conclusion: Implications and significance"""

            # ì§„í–‰ ë°” ì„¤ëª… (ì„ íƒëœ ì–¸ì–´ì— ë”°ë¼)
            pbar_desc = "   ğŸŒ í•œêµ­ì–´ ë³€í™˜" if self.summary_language == 'ko' else "   ğŸŒ English Summary"
            with tqdm(
                total=len(abstract_only),
                desc=pbar_desc,
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
                colour='green'
            ) as pbar:
                for paper in abstract_only:
                    title_short = paper['title'][:35] + "..." if len(paper['title']) > 35 else paper['title']
                    pbar.set_postfix_str(f"ğŸ“‹ {title_short}")

                    abstract_content = f"""
Title: {paper['title']}
Authors: {', '.join(paper['authors'][:5])}
Published: {paper['published']}
Source: {paper['source']}

Abstract:
{paper['abstract']}
"""

                    try:
                        with LangSmithTracer("Abstract_Summary_LLM", run_type="llm",
                                            metadata={"paper_title": paper['title'][:50], "type": "abstract"}) as tracer:
                            tracer.log_input({"system": abstract_prompt, "user": abstract_content[:500]})

                            response = client.chat.completions.create(
                                model="gpt-3.5-turbo",
                                messages=[
                                    {"role": "system", "content": abstract_prompt},
                                    {"role": "user", "content": abstract_content}
                                ],
                                max_tokens=500,
                                temperature=0.3
                            )

                            summary = response.choices[0].message.content
                            paper['summary'] = summary
                            paper['summary_type'] = 'abstract_summarized'
                            abstract_success += 1

                            tracer.log_output({"summary": summary[:200]})

                    except Exception as e:
                        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì´ˆë¡ ì‚¬ìš©
                        paper['summary'] = f"[ë²ˆì—­ ì‹¤íŒ¨ - ì›ë¬¸]\n{paper['abstract']}"
                        paper['summary_type'] = 'abstract_only'
                        abstract_fail += 1

                    summarized_papers.append(paper)
                    pbar.update(1)
                    time.sleep(0.3)

        # ê²°ê³¼ í†µê³„
        summary_lang_label = "í•œêµ­ì–´" if self.summary_language == 'ko' else "English"
        print("\n" + "-" * 60)
        print("ğŸ“Š ìš”ì•½ ê²°ê³¼ í†µê³„")
        print("-" * 60)
        if full_papers:
            print(f"   ğŸ“„ Full Paper AI ìš”ì•½: {full_success}ê°œ ì„±ê³µ" + (f", {full_fail}ê°œ ì‹¤íŒ¨" if full_fail > 0 else ""))
        if abstract_only:
            print(f"   ğŸ“‹ Abstract {summary_lang_label} ìš”ì•½: {abstract_success}ê°œ ì„±ê³µ" + (f", {abstract_fail}ê°œ ì‹¤íŒ¨" if abstract_fail > 0 else ""))
        print("-" * 60)

        # ìš”ì•½ ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 70)
        print("ğŸ“‹ ë…¼ë¬¸ ìš”ì•½ ê²°ê³¼")
        print("=" * 70)

        for i, paper in enumerate(summarized_papers):
            print(f"\n{'â”€' * 70}")
            summary_type = paper.get('summary_type', 'unknown')

            # ìœ í˜•ë³„ ì•„ì´ì½˜ ë° ë¼ë²¨
            if summary_type == 'full_paper':
                type_icon = "ğŸ“„"
                type_label = "[Full Paper]"
            elif summary_type == 'abstract_summarized':
                type_icon = "ğŸ“‹"
                type_label = f"[Abstract â†’ {summary_lang_label}]"
            else:
                type_icon = "ğŸ“‹"
                type_label = "[Abstract Only]"

            print(f"{type_icon} [{i+1}/{len(summarized_papers)}] {type_label} {paper['title']}")
            print(f"{'â”€' * 70}")
            print(f"ğŸ‘¤ ì €ì: {', '.join(paper['authors'][:3])}" + (" ì™¸" if len(paper['authors']) > 3 else ""))
            print(f"ğŸ“… ë°œí–‰: {paper['published'][:10] if paper.get('published') else 'N/A'}")
            print(f"ğŸ“– ì¶œì²˜: {paper['source']}")

            # ë§í¬ ì¶œë ¥
            urls = paper.get('_urls', {})
            if urls:
                print(f"{'â”€' * 70}")
                print("ğŸ”— ë…¼ë¬¸ ë§í¬:")
                for name, url in urls.items():
                    print(f"   â€¢ {name}: {url}")

            print(f"{'â”€' * 70}")

            if summary_type == 'full_paper':
                print("ğŸ“ AI ìš”ì•½ (ì „ì²´ ë…¼ë¬¸ ê¸°ë°˜):")
            elif summary_type == 'abstract_summarized':
                print(f"ğŸ“ AI {summary_lang_label} ìš”ì•½ (ì´ˆë¡ ê¸°ë°˜):")
            else:
                print("ğŸ“ ì´ˆë¡ ì›ë¬¸ (ìš”ì•½ ì‹¤íŒ¨):")
                print("   âš ï¸ ìš”ì•½ì— ì‹¤íŒ¨í•˜ì—¬ ì›ë¬¸ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
                print()

            summary = paper.get('summary', 'No summary available')
            for line in summary.split('\n'):
                print(f"   {line}")
            print()

        print("=" * 70)

        # ë¹„ëŒ€í™”í˜• ëª¨ë“œì—ì„œëŠ” input ê±´ë„ˆë›°ê¸°
        try:
            import sys
            if sys.stdin.isatty():
                input("\nâ Enterë¥¼ ëˆŒëŸ¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰...")
        except:
            pass

        return summarized_papers


# ==================== PaperSearcher í´ë˜ìŠ¤ ====================
class PaperSearcher:
    def __init__(self, api_key: str = None, email: str = None):
        self.papers = []
        self.api_key = api_key
        self.email = email

    def search_arxiv(self, query: str, max_results: int = 5) -> List[Dict]:
        print(f"\nğŸ” arXivì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")

        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance
        )

        papers = []
        for result in search.results():
            paper = {
                'source': 'arXiv',
                'title': result.title,
                'authors': [author.name for author in result.authors],
                'abstract': result.summary,
                'pdf_url': result.pdf_url,
                'published': result.published.strftime('%Y-%m-%d'),
                'id': result.entry_id.split('/')[-1]
            }
            papers.append(paper)
            print(f"   ğŸ“„ {paper['title'][:60]}...")

        print(f"   âœ… arXiv: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
        return papers

    def search_pubmed(self, query: str, max_results: int = 5) -> List[Dict]:
        print(f"\nğŸ” PubMedì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")

        search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        search_params = {
            'db': 'pubmed',
            'term': query,
            'retmax': max_results,
            'retmode': 'json',
            'sort': 'relevance'
        }

        if self.api_key:
            search_params['api_key'] = self.api_key
            print("   ğŸ”‘ API í‚¤ ì‚¬ìš© ì¤‘...")
        if self.email:
            search_params['email'] = self.email

        try:
            response = requests.get(search_url, params=search_params, timeout=30)
            response.raise_for_status()
            search_data = response.json()
            pmids = search_data.get('esearchresult', {}).get('idlist', [])

            if not pmids:
                print("   âš ï¸ PubMed: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return []

            fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
            fetch_params = {
                'db': 'pubmed',
                'id': ','.join(pmids),
                'retmode': 'xml'
            }
            if self.api_key:
                fetch_params['api_key'] = self.api_key

            if not self.api_key:
                time.sleep(0.35)

            response = requests.get(fetch_url, params=fetch_params, timeout=30)
            response.raise_for_status()

            data = xmltodict.parse(response.content)
            articles = data.get('PubmedArticleSet', {}).get('PubmedArticle', [])

            if isinstance(articles, dict):
                articles = [articles]

            papers = []
            for article in articles:
                try:
                    medline = article.get('MedlineCitation', {})
                    article_data = medline.get('Article', {})

                    title = article_data.get('ArticleTitle', 'No Title')
                    if isinstance(title, dict):
                        title = title.get('#text', 'No Title')

                    abstract_data = article_data.get('Abstract', {}).get('AbstractText', '')
                    if isinstance(abstract_data, list):
                        abstract = ' '.join([a.get('#text', str(a)) if isinstance(a, dict) else str(a) for a in abstract_data])
                    elif isinstance(abstract_data, dict):
                        abstract = abstract_data.get('#text', str(abstract_data))
                    else:
                        abstract = str(abstract_data) if abstract_data else 'No abstract available'

                    author_list = article_data.get('AuthorList', {}).get('Author', [])
                    if isinstance(author_list, dict):
                        author_list = [author_list]
                    authors = []
                    for author in author_list[:5]:
                        if isinstance(author, dict):
                            last = author.get('LastName', '')
                            first = author.get('ForeName', '')
                            if last:
                                authors.append(f"{last} {first}".strip())

                    pmid = medline.get('PMID', {}).get('#text', 'Unknown')
                    pub_date = article_data.get('Journal', {}).get('JournalIssue', {}).get('PubDate', {})
                    year = pub_date.get('Year', 'Unknown')

                    paper = {
                        'source': 'PubMed',
                        'title': title,
                        'authors': authors if authors else ['Unknown'],
                        'abstract': abstract,
                        'pdf_url': None,
                        'pubmed_url': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                        'pmc_url': None,
                        'published': str(year),
                        'id': f"PMID_{pmid}"
                    }
                    papers.append(paper)
                    print(f"   ğŸ“„ {paper['title'][:60]}...")

                except Exception as e:
                    continue

            print(f"   âœ… PubMed: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
            return papers

        except Exception as e:
            print(f"   âŒ PubMed ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []

    def search_europepmc(self, query: str, max_results: int = 5) -> List[Dict]:
        """Europe PMCì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰"""
        print(f"\nğŸ” Europe PMCì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")

        search_url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
        params = {
            'query': query,
            'format': 'json',
            'pageSize': max_results,
            'resultType': 'core'  # ìƒì„¸ ì •ë³´ í¬í•¨ (ê¸°ë³¸ ì •ë ¬: relevance)
        }

        try:
            response = requests.get(search_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            results = data.get('resultList', {}).get('result', [])

            if not results:
                print("   âš ï¸ Europe PMC: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return []

            papers = []
            for article in results:
                try:
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    title = article.get('title', 'No Title')
                    if title:
                        title = title.replace('<i>', '').replace('</i>', '')
                        title = title.replace('<b>', '').replace('</b>', '')

                    # ì´ˆë¡ ì¶”ì¶œ
                    abstract = article.get('abstractText', '')
                    if not abstract:
                        abstract = 'No abstract available'

                    # ì €ì ì •ë³´
                    author_string = article.get('authorString', '')
                    if author_string:
                        authors = [a.strip() for a in author_string.split(',')[:5]]
                    else:
                        authors = ['Unknown']

                    # ì¶œíŒ ì •ë³´
                    pub_year = article.get('pubYear', 'Unknown')

                    # ID ë° URL
                    pmid = article.get('pmid', '')
                    pmcid = article.get('pmcid', '')
                    doi = article.get('doi', '')
                    source_db = article.get('source', 'MED')

                    # ID ê²°ì • (ìš°ì„ ìˆœìœ„: PMCID > PMID > DOI)
                    if pmcid:
                        paper_id = f"PMC_{pmcid}"
                    elif pmid:
                        paper_id = f"PMID_{pmid}"
                    elif doi:
                        paper_id = f"DOI_{doi.replace('/', '_')}"
                    else:
                        paper_id = f"EPMC_{article.get('id', 'unknown')}"

                    # URL êµ¬ì„±
                    europepmc_url = None
                    pdf_url = None
                    pubmed_url = None
                    pmc_url = None

                    if pmcid:
                        europepmc_url = f"https://europepmc.org/article/PMC/{pmcid}"
                        pmc_url = f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/"
                        # PMC ë…¼ë¬¸ì€ ë³´í†µ ë¬´ë£Œ PDF ì œê³µ
                        pdf_url = f"https://europepmc.org/backend/ptpmcrender.fcgi?accid={pmcid}&blobtype=pdf"
                    elif pmid:
                        europepmc_url = f"https://europepmc.org/article/MED/{pmid}"
                        pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"

                    # ì „ë¬¸ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€
                    is_open_access = article.get('isOpenAccess', 'N') == 'Y'
                    has_full_text = article.get('hasFullText', 'N') == 'Y'

                    paper = {
                        'source': 'Europe PMC',
                        'title': title,
                        'authors': authors,
                        'abstract': abstract,
                        'pdf_url': pdf_url,
                        'europepmc_url': europepmc_url,
                        'pubmed_url': pubmed_url,
                        'pmc_url': pmc_url,
                        'doi': doi,
                        'published': str(pub_year),
                        'id': paper_id,
                        'is_open_access': is_open_access,
                        'has_full_text': has_full_text
                    }
                    papers.append(paper)

                    # ìƒíƒœ í‘œì‹œ
                    oa_icon = "ğŸ”“" if is_open_access else "ğŸ”’"
                    print(f"   ğŸ“„ {oa_icon} {paper['title'][:55]}...")

                except Exception as e:
                    continue

            oa_count = sum(1 for p in papers if p.get('is_open_access'))
            print(f"   âœ… Europe PMC: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬ (Open Access: {oa_count}ê°œ)")
            return papers

        except Exception as e:
            print(f"   âŒ Europe PMC ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []

    def search(self, query: str, source: str = 'both', max_results: int = 5) -> List[Dict]:
        """
        ë…¼ë¬¸ ê²€ìƒ‰ (ë³µìˆ˜ ì†ŒìŠ¤ ì§€ì›)

        Args:
            query: ê²€ìƒ‰ì–´
            source: ê²€ìƒ‰ ì†ŒìŠ¤ (ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)
                    ì˜ˆ: 'pubmed', 'arxiv', 'europepmc', 'pubmed,arxiv', 'all'
            max_results: ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜
        """
        papers = []

        # ì†ŒìŠ¤ ëª©ë¡ íŒŒì‹±
        if source == 'all' or source == 'both':
            sources = ['pubmed', 'arxiv', 'europepmc']
        else:
            sources = [s.strip().lower() for s in source.split(',')]

        # ê° ì†ŒìŠ¤ì—ì„œ ê²€ìƒ‰
        if 'arxiv' in sources:
            papers.extend(self.search_arxiv(query, max_results))

        if 'pubmed' in sources:
            papers.extend(self.search_pubmed(query, max_results))

        if 'europepmc' in sources:
            papers.extend(self.search_europepmc(query, max_results))

        # ì¤‘ë³µ ì œê±° (PMID ê¸°ì¤€)
        seen_ids = set()
        unique_papers = []
        for paper in papers:
            paper_id = paper.get('id', '')
            # PMIDê°€ ê°™ì€ ê²½ìš° ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬
            pmid = None
            if 'PMID_' in paper_id:
                pmid = paper_id.replace('PMID_', '')
            elif paper.get('pubmed_url'):
                pmid = paper.get('pubmed_url', '').split('/')[-2]

            check_id = pmid if pmid else paper_id

            if check_id not in seen_ids:
                seen_ids.add(check_id)
                unique_papers.append(paper)

        if len(papers) != len(unique_papers):
            print(f"\n   ğŸ”„ ì¤‘ë³µ ì œê±°: {len(papers)}ê°œ â†’ {len(unique_papers)}ê°œ")

        self.papers = unique_papers
        return unique_papers


# ==================== íŠ¸ë Œë“œ ë¶„ì„ í´ë˜ìŠ¤ ====================
class TrendAnalyzer:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„"""

    def __init__(self, api_key: str = None, email: str = None, openai_api_key: str = None):
        self.searcher = PaperSearcher(api_key=api_key, email=email)
        self.papers = []
        self.trend_data = {}
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')

    def search_papers_for_trend(self, keyword: str, max_results: int = 50, source: str = 'pubmed') -> List[Dict]:
        """íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ë…¼ë¬¸ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼)"""
        print(f"\nğŸ” '{keyword}' íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...")
        self.papers = self.searcher.search(keyword, source=source, max_results=max_results)
        print(f"   ğŸ“Š ì´ {len(self.papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")
        return self.papers

    def analyze_publication_trend(self) -> Dict:
        """ì—°ë„ë³„ ì¶œíŒ íŠ¸ë Œë“œ ë¶„ì„"""
        from collections import Counter

        if not self.papers:
            return {}

        # ì—°ë„ ì¶”ì¶œ ë° ì§‘ê³„
        years = []
        for paper in self.papers:
            pub_date = paper.get('published', '')
            if pub_date:
                try:
                    year = int(pub_date[:4])
                    if 2000 <= year <= 2030:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„
                        years.append(year)
                except:
                    pass

        year_counts = Counter(years)
        sorted_years = sorted(year_counts.items())

        self.trend_data['year_trend'] = {
            'years': [y[0] for y in sorted_years],
            'counts': [y[1] for y in sorted_years],
            'total': len(years)
        }

        return self.trend_data['year_trend']

    def extract_key_terms(self, top_n: int = 20) -> Dict:
        """ë…¼ë¬¸ ì´ˆë¡ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        from collections import Counter
        import re

        if not self.papers:
            return {}

        # ë¶ˆìš©ì–´ (ì˜ì–´)
        stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
            'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these',
            'those', 'it', 'its', 'they', 'them', 'their', 'we', 'our', 'you',
            'your', 'i', 'me', 'my', 'he', 'she', 'his', 'her', 'which', 'who',
            'whom', 'what', 'when', 'where', 'why', 'how', 'all', 'each', 'every',
            'both', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',
            'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'just',
            'also', 'now', 'here', 'there', 'then', 'once', 'if', 'because',
            'while', 'although', 'though', 'after', 'before', 'since', 'until',
            'about', 'into', 'through', 'during', 'above', 'below', 'between',
            'under', 'again', 'further', 'then', 'once', 'study', 'studies',
            'results', 'conclusion', 'methods', 'background', 'objective',
            'aim', 'purpose', 'introduction', 'discussion', 'however', 'thus',
            'therefore', 'moreover', 'furthermore', 'additionally', 'including',
            'included', 'using', 'used', 'based', 'associated', 'related',
            'compared', 'among', 'within', 'without', 'between', 'across'
        }

        # ëª¨ë“  ì´ˆë¡ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
        all_words = []
        for paper in self.papers:
            abstract = paper.get('abstract', '')
            # ë‹¨ì–´ ì¶”ì¶œ (ì˜ë¬¸ìë§Œ)
            words = re.findall(r'\b[a-zA-Z]{3,}\b', abstract.lower())
            # ë¶ˆìš©ì–´ ì œê±°
            words = [w for w in words if w not in stopwords]
            all_words.extend(words)

        # ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(all_words)
        top_terms = word_counts.most_common(top_n)

        self.trend_data['key_terms'] = {
            'terms': [t[0] for t in top_terms],
            'counts': [t[1] for t in top_terms]
        }

        return self.trend_data['key_terms']

    def identify_emerging_topics(self) -> List[Dict]:
        """ìµœê·¼ ê¸‰ë¶€ìƒí•˜ëŠ” ì£¼ì œ ì‹ë³„"""
        from collections import Counter, defaultdict
        import re

        if not self.papers or len(self.papers) < 5:
            return []

        # ìµœê·¼ 2ë…„ vs ì´ì „ ë…¼ë¬¸ ë¹„êµ
        recent_papers = []
        older_papers = []

        current_year = 2024  # í˜„ì¬ ì—°ë„
        for paper in self.papers:
            pub_date = paper.get('published', '')
            try:
                year = int(pub_date[:4])
                if year >= current_year - 1:
                    recent_papers.append(paper)
                else:
                    older_papers.append(paper)
            except:
                pass

        if not recent_papers or not older_papers:
            return []

        # ë¶ˆìš©ì–´
        stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
            'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'study', 'studies', 'results', 'patients', 'treatment', 'using'
        }

        def extract_terms(papers):
            words = []
            for paper in papers:
                abstract = paper.get('abstract', '')
                w = re.findall(r'\b[a-zA-Z]{4,}\b', abstract.lower())
                words.extend([x for x in w if x not in stopwords])
            return Counter(words)

        recent_terms = extract_terms(recent_papers)
        older_terms = extract_terms(older_papers)

        # ìµœê·¼ì— ê¸‰ì¦í•œ ìš©ì–´ ì°¾ê¸°
        emerging = []
        for term, recent_count in recent_terms.most_common(50):
            older_count = older_terms.get(term, 0)
            if older_count == 0:
                growth = float('inf')
            else:
                growth = (recent_count / len(recent_papers)) / (older_count / len(older_papers))

            if growth > 1.5 and recent_count >= 3:
                emerging.append({
                    'term': term,
                    'recent_count': recent_count,
                    'older_count': older_count,
                    'growth_rate': growth if growth != float('inf') else 999
                })

        # ì„±ì¥ë¥  ìˆœ ì •ë ¬
        emerging.sort(key=lambda x: x['growth_rate'], reverse=True)
        self.trend_data['emerging_topics'] = emerging[:10]

        return self.trend_data['emerging_topics']

    def summarize_research_content(self, keyword: str) -> Dict:
        """ì—°êµ¬ ë‚´ìš© ìš”ì•½ ìƒì„±"""
        if not self.papers:
            return {}

        # 1. ì£¼ìš” ì—°êµ¬ ì£¼ì œ ì¶”ì¶œ (ì œëª© ê¸°ë°˜)
        titles = [p.get('title', '') for p in self.papers[:20]]

        # 2. ì£¼ìš” ì—°êµ¬ ë°©í–¥ íŒŒì•… (ì´ˆë¡ì—ì„œ íŒ¨í„´ ì¶”ì¶œ)
        research_themes = {
            'diagnosis': 0,      # ì§„ë‹¨
            'treatment': 0,      # ì¹˜ë£Œ
            'mechanism': 0,      # ë©”ì»¤ë‹ˆì¦˜
            'prevention': 0,     # ì˜ˆë°©
            'clinical': 0,       # ì„ìƒ
            'review': 0,         # ë¦¬ë·°/ê°œê´€
            'epidemiology': 0,   # ì—­í•™
            'genetics': 0,       # ìœ ì „í•™
        }

        theme_keywords = {
            'diagnosis': ['diagnosis', 'diagnostic', 'detection', 'screening', 'biomarker'],
            'treatment': ['treatment', 'therapy', 'therapeutic', 'drug', 'intervention', 'medication'],
            'mechanism': ['mechanism', 'pathway', 'molecular', 'cellular', 'signaling'],
            'prevention': ['prevention', 'preventive', 'risk factor', 'protective'],
            'clinical': ['clinical', 'trial', 'patient', 'outcome', 'efficacy'],
            'review': ['review', 'overview', 'comprehensive', 'systematic', 'meta-analysis'],
            'epidemiology': ['epidemiology', 'prevalence', 'incidence', 'population'],
            'genetics': ['genetic', 'gene', 'mutation', 'hereditary', 'genomic'],
        }

        for paper in self.papers:
            text = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()
            for theme, keywords in theme_keywords.items():
                for kw in keywords:
                    if kw in text:
                        research_themes[theme] += 1
                        break

        # ìƒìœ„ ì—°êµ¬ ì£¼ì œ
        sorted_themes = sorted(research_themes.items(), key=lambda x: x[1], reverse=True)
        top_themes = [(t, c) for t, c in sorted_themes if c > 0][:5]

        # 3. ëŒ€í‘œ ë…¼ë¬¸ ì„ ì • (ìµœì‹  + ê´€ë ¨ì„±) - 10ê°œ
        representative_papers = self.papers[:10]

        # 4. ì—°êµ¬ ë‚´ìš© ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
        summary_text = self._generate_content_summary(keyword, top_themes, representative_papers)

        self.trend_data['content_summary'] = {
            'themes': top_themes,
            'representative_papers': representative_papers,
            'summary_text': summary_text
        }

        return self.trend_data['content_summary']

    def generate_search_summary(self, keyword: str) -> Dict:
        """ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±"""
        if not self.papers:
            return {}

        # ê¸°ë³¸ í†µê³„
        total_papers = len(self.papers)

        # ì—°ë„ ë²”ìœ„ ê³„ì‚°
        years = []
        for paper in self.papers:
            pub_date = paper.get('published', '')
            if pub_date:
                try:
                    year = int(pub_date[:4])
                    years.append(year)
                except:
                    pass

        year_range = f"{min(years)}-{max(years)}" if years else "N/A"

        # ì†ŒìŠ¤ë³„ ë¶„ë¥˜
        sources = {}
        for paper in self.papers:
            src = paper.get('source', 'Unknown')
            sources[src] = sources.get(src, 0) + 1

        # AIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
        search_summary_text = self._generate_search_overview(keyword)

        self.trend_data['search_summary'] = {
            'total_papers': total_papers,
            'year_range': year_range,
            'sources': sources,
            'summary_text': search_summary_text
        }

        return self.trend_data['search_summary']

    def _generate_search_overview(self, keyword: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ê°œìš” ìš”ì•½ ìƒì„±"""
        if not self.openai_api_key or len(self.papers) < 2:
            return self._generate_basic_search_overview(keyword)

        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.openai_api_key)

            print("   ğŸ” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„± ì¤‘...")

            # ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ ìˆ˜ì§‘
            paper_info = []
            for paper in self.papers[:15]:  # ìµœëŒ€ 15ê°œ ë…¼ë¬¸ ë¶„ì„
                title = paper.get('title', '')
                abstract = paper.get('abstract', '')[:300]
                paper_info.append(f"ì œëª©: {title}\nì´ˆë¡: {abstract}")

            combined_info = "\n\n".join(paper_info)

            prompt = f"""ë‹¤ìŒì€ '{keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ëœ {len(self.papers)}ê°œì˜ í•™ìˆ  ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤.

{combined_info}

ìœ„ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:

ğŸ” **ê²€ìƒ‰ ê²°ê³¼ ê°œìš”**:
('{keyword}'ì— ëŒ€í•´ ì–´ë–¤ ë…¼ë¬¸ë“¤ì´ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…)

ğŸ“‘ **ì£¼ìš” ì—°êµ¬ ë‚´ìš©**:
- (ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì´ ë‹¤ë£¨ëŠ” í•µì‹¬ ì£¼ì œ 3-4ê°€ì§€ë¥¼ bullet pointë¡œ)

ğŸ¯ **ì—°êµ¬ ì´ˆì **:
(ì´ ë¶„ì•¼ ì—°êµ¬ìë“¤ì´ ì£¼ë¡œ ê´€ì‹¬ì„ ê°€ì§€ëŠ” ë¬¸ì œë‚˜ ì§ˆë¬¸ 1-2ë¬¸ì¥)

í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )

            summary = response.choices[0].message.content.strip()
            print("   âœ… ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì™„ë£Œ!")
            return summary

        except Exception as e:
            print(f"   âš ï¸ AI ìš”ì•½ ì‹¤íŒ¨: {str(e)[:50]}")
            return self._generate_basic_search_overview(keyword)

    def _generate_basic_search_overview(self, keyword: str) -> str:
        """ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ê°œìš” ìƒì„±"""
        if not self.papers:
            return ""

        # ì œëª©ì—ì„œ ê³µí†µ ì£¼ì œ ì¶”ì¶œ
        title_words = []
        for paper in self.papers[:10]:
            title = paper.get('title', '').lower()
            words = [w for w in title.split() if len(w) > 4 and w != keyword.lower()]
            title_words.extend(words)

        from collections import Counter
        common_words = Counter(title_words).most_common(5)
        common_topics = [w[0] for w in common_words]

        summary = f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼, ì´ {len(self.papers)}ê°œì˜ ë…¼ë¬¸ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        if common_topics:
            summary += f"ì£¼ìš” ê´€ë ¨ ì£¼ì œ: {', '.join(common_topics[:3])}"

        return summary

    def _generate_content_summary(self, keyword: str, themes: List, papers: List) -> str:
        """ì—°êµ¬ ë‚´ìš© ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±"""
        # OpenAI APIê°€ ìˆìœ¼ë©´ AI ìš”ì•½ ì‚¬ìš©
        if self.openai_api_key and len(self.papers) >= 3:
            ai_summary = self._generate_ai_summary(keyword)
            if ai_summary:
                return ai_summary

        # ê¸°ë³¸ ìš”ì•½ ìƒì„±
        summary_parts = []

        # ì£¼ìš” ì—°êµ¬ ë¶„ì•¼
        if themes:
            theme_names = {
                'diagnosis': 'ì§„ë‹¨/ê²€ì¶œ',
                'treatment': 'ì¹˜ë£Œ/ì•½ë¬¼',
                'mechanism': 'ë©”ì»¤ë‹ˆì¦˜/ê²½ë¡œ',
                'prevention': 'ì˜ˆë°©/ìœ„í—˜ìš”ì¸',
                'clinical': 'ì„ìƒì—°êµ¬',
                'review': 'ì¢…í•©ë¦¬ë·°',
                'epidemiology': 'ì—­í•™ì—°êµ¬',
                'genetics': 'ìœ ì „í•™ì—°êµ¬'
            }
            main_themes = [theme_names.get(t[0], t[0]) for t in themes[:3]]
            summary_parts.append(f"'{keyword}' ì—°êµ¬ëŠ” ì£¼ë¡œ {', '.join(main_themes)} ë¶„ì•¼ì— ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ìµœì‹  ì—°êµ¬ ë™í–¥
        if papers:
            recent_titles = [p.get('title', '')[:80] for p in papers[:3]]
            summary_parts.append(f"\nìµœê·¼ ì£¼ìš” ì—°êµ¬: ")
            for i, title in enumerate(recent_titles, 1):
                summary_parts.append(f"  {i}. {title}...")

        return "\n".join(summary_parts)

    def _generate_ai_summary(self, keyword: str) -> Optional[str]:
        """OpenAIë¥¼ ì‚¬ìš©í•œ AI ìš”ì•½ ìƒì„±"""
        if not self.openai_api_key:
            return None

        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.openai_api_key)

            print("   ğŸ¤– OpenAI GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ ë‚´ìš© ë¶„ì„ ì¤‘...")

            # ë…¼ë¬¸ ì´ˆë¡ ìˆ˜ì§‘ (ìµœëŒ€ 10ê°œë¡œ í™•ëŒ€)
            abstracts = []
            for paper in self.papers[:10]:
                title = paper.get('title', '')
                abstract = paper.get('abstract', '')[:600]
                year = paper.get('published', '')[:4]
                authors = paper.get('authors', '')[:50] if paper.get('authors') else ''
                abstracts.append(f"[{year}] {title}\nì €ì: {authors}\nì´ˆë¡: {abstract}")

            combined_text = "\n\n---\n\n".join(abstracts)

            # ì—°ë„ë³„ í†µê³„ ì •ë³´ ì¶”ê°€
            year_stats = ""
            if 'year_trend' in self.trend_data:
                years = self.trend_data['year_trend']['years'][-5:]
                counts = self.trend_data['year_trend']['counts'][-5:]
                year_stats = f"\n\nì—°ë„ë³„ ë…¼ë¬¸ ìˆ˜: " + ", ".join([f"{y}ë…„: {c}í¸" for y, c in zip(years, counts)])

            prompt = f"""ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ì—°êµ¬ ë™í–¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ '{keyword}'ì— ê´€í•œ ìµœê·¼ ì—°êµ¬ ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤.

ì´ ë¶„ì„ ë…¼ë¬¸ ìˆ˜: {len(self.papers)}í¸{year_stats}

=== ë…¼ë¬¸ ëª©ë¡ ===
{combined_text}

ìœ„ ì—°êµ¬ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ '{keyword}' ë¶„ì•¼ì˜ ì—°êµ¬ ë™í–¥ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ğŸ“Œ **ì—°êµ¬ ê°œìš”**: (ì´ ë¶„ì•¼ê°€ ë¬´ì—‡ì„ ë‹¤ë£¨ëŠ”ì§€ 1-2ë¬¸ì¥)

ğŸ”¬ **ì£¼ìš” ì—°êµ¬ ë°©í–¥**:
- (ìµœê·¼ ì—°êµ¬ë“¤ì´ ì§‘ì¤‘í•˜ëŠ” í•µì‹¬ ì£¼ì œ 2-3ê°€ì§€)

ğŸ’¡ **í•µì‹¬ ë°œê²¬/ê²°ê³¼**:
- (ì£¼ìš” ì—°êµ¬ ê²°ê³¼ë‚˜ ë°œê²¬ 2-3ê°€ì§€)

ğŸ”® **í–¥í›„ ì „ë§**:
- (ì—°êµ¬ íŠ¸ë Œë“œ ë° í–¥í›„ ë°©í–¥ 1-2ë¬¸ì¥)

í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì „ë¬¸ ìš©ì–´ëŠ” í•„ìš”ì‹œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."""

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ì—°êµ¬ ë™í–¥ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³µì¡í•œ ì—°êµ¬ ë‚´ìš©ì„ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.3
            )

            summary = response.choices[0].message.content.strip()
            print("   âœ… AI ìš”ì•½ ìƒì„± ì™„ë£Œ!")
            return summary

        except Exception as e:
            print(f"   âš ï¸ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)[:80]}")
            print("   ğŸ“ ê¸°ë³¸ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            return None

    def visualize_trends(self, keyword: str, save_path: str = None) -> str:
        """íŠ¸ë Œë“œ ì‹œê°í™”"""
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt

        try:
            plt.rcParams['font.family'] = 'AppleGothic'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'ğŸ“ˆ Research Trend Analysis: "{keyword}"',
                    fontsize=14, fontweight='bold')

        # 1. ì—°ë„ë³„ ì¶œíŒ íŠ¸ë Œë“œ
        ax1 = axes[0, 0]
        if 'year_trend' in self.trend_data:
            years = self.trend_data['year_trend']['years']
            counts = self.trend_data['year_trend']['counts']
            colors = plt.cm.Blues([0.3 + 0.7 * i / len(years) for i in range(len(years))])
            bars = ax1.bar(years, counts, color=colors, edgecolor='navy', alpha=0.8)
            ax1.set_xlabel('Year', fontweight='bold')
            ax1.set_ylabel('Number of Publications', fontweight='bold')
            ax1.set_title('ğŸ“… Publication Trend by Year', fontweight='bold')
            ax1.set_xticks(years)
            ax1.tick_params(axis='x', rotation=45)
            for bar, count in zip(bars, counts):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        str(count), ha='center', va='bottom', fontsize=9)

        # 2. í•µì‹¬ í‚¤ì›Œë“œ
        ax2 = axes[0, 1]
        if 'key_terms' in self.trend_data:
            terms = self.trend_data['key_terms']['terms'][:10]
            term_counts = self.trend_data['key_terms']['counts'][:10]
            colors = plt.cm.Greens([0.3 + 0.7 * i / len(terms) for i in range(len(terms))])
            bars = ax2.barh(terms[::-1], term_counts[::-1], color=colors[::-1], edgecolor='darkgreen', alpha=0.8)
            ax2.set_xlabel('Frequency', fontweight='bold')
            ax2.set_title('ğŸ”‘ Top Keywords in Abstracts', fontweight='bold')
            for bar, count in zip(bars, term_counts[::-1]):
                ax2.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
                        str(count), va='center', fontsize=9)

        # 3. ê¸‰ë¶€ìƒ ì£¼ì œ
        ax3 = axes[1, 0]
        if 'emerging_topics' in self.trend_data and self.trend_data['emerging_topics']:
            emerging = self.trend_data['emerging_topics'][:8]
            e_terms = [e['term'] for e in emerging]
            e_growth = [min(e['growth_rate'], 10) for e in emerging]  # ìµœëŒ€ 10ìœ¼ë¡œ ì œí•œ
            colors = plt.cm.Reds([0.3 + 0.7 * i / len(e_terms) for i in range(len(e_terms))])
            bars = ax3.barh(e_terms[::-1], e_growth[::-1], color=colors[::-1], edgecolor='darkred', alpha=0.8)
            ax3.set_xlabel('Growth Rate', fontweight='bold')
            ax3.set_title('ğŸš€ Emerging Topics (Recent vs Older)', fontweight='bold')
            for bar, growth in zip(bars, e_growth[::-1]):
                ax3.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                        f'{growth:.1f}x', va='center', fontsize=9)
        else:
            ax3.text(0.5, 0.5, 'Not enough data\nfor trend analysis',
                    ha='center', va='center', fontsize=12, transform=ax3.transAxes)
            ax3.set_title('ğŸš€ Emerging Topics', fontweight='bold')

        # 4. ìš”ì•½ ì •ë³´
        ax4 = axes[1, 1]
        ax4.axis('off')

        summary = f"ğŸ“Š Trend Analysis Summary\n{'='*40}\n\n"
        summary += f"ğŸ” Keyword: {keyword}\n"
        summary += f"ğŸ“„ Total Papers: {len(self.papers)}\n\n"

        if 'year_trend' in self.trend_data:
            years = self.trend_data['year_trend']['years']
            counts = self.trend_data['year_trend']['counts']
            if years:
                summary += f"ğŸ“… Year Range: {min(years)} - {max(years)}\n"
                max_idx = counts.index(max(counts))
                summary += f"ğŸ“ˆ Peak Year: {years[max_idx]} ({counts[max_idx]} papers)\n\n"

        if 'key_terms' in self.trend_data:
            top_5 = self.trend_data['key_terms']['terms'][:5]
            summary += f"ğŸ”‘ Top Keywords:\n"
            for i, term in enumerate(top_5, 1):
                summary += f"   {i}. {term}\n"

        if 'emerging_topics' in self.trend_data and self.trend_data['emerging_topics']:
            summary += f"\nğŸš€ Emerging Topics:\n"
            for i, e in enumerate(self.trend_data['emerging_topics'][:3], 1):
                summary += f"   {i}. {e['term']} ({e['growth_rate']:.1f}x growth)\n"

        ax4.text(0.05, 0.95, summary, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow',
                         alpha=0.9, edgecolor='orange'))

        plt.tight_layout()

        if save_path is None:
            save_path = f"trend_analysis_{keyword.replace(' ', '_')[:20]}.png"

        plt.savefig(save_path, dpi=100, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close(fig)

        print(f"   ğŸ“Š íŠ¸ë Œë“œ ì‹œê°í™” ì €ì¥: {save_path}")
        return save_path

    def generate_trend_report(self, keyword: str) -> str:
        """íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("\n" + "=" * 60)
        report.append(f"ğŸ“ˆ '{keyword}' ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 60)

        report.append(f"\nğŸ“Š ë¶„ì„ ëŒ€ìƒ: {len(self.papers)}ê°œ ë…¼ë¬¸\n")

        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        if 'search_summary' in self.trend_data and self.trend_data['search_summary']:
            search_data = self.trend_data['search_summary']
            report.append("-" * 60)
            report.append("ğŸ” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
            report.append("-" * 60)

            # ê¸°ë³¸ í†µê³„
            report.append(f"\n   ğŸ“š ê²€ìƒ‰ëœ ë…¼ë¬¸: {search_data.get('total_papers', 0)}ê°œ")
            report.append(f"   ğŸ“… ì—°ë„ ë²”ìœ„: {search_data.get('year_range', 'N/A')}")

            # ì†ŒìŠ¤ë³„ ë¶„ë¥˜
            sources = search_data.get('sources', {})
            if sources:
                source_str = ", ".join([f"{k}: {v}ê°œ" for k, v in sources.items()])
                report.append(f"   ğŸ“– ì¶œì²˜: {source_str}")

            # ìš”ì•½ í…ìŠ¤íŠ¸
            summary_text = search_data.get('summary_text', '')
            if summary_text:
                report.append("\n" + "-" * 40)
                summary_lines = summary_text.split('\n')
                for line in summary_lines:
                    if line.strip():
                        report.append(f"   {line}")
            report.append("")

        # ì—°ë„ë³„ íŠ¸ë Œë“œ
        if 'year_trend' in self.trend_data:
            years = self.trend_data['year_trend']['years']
            counts = self.trend_data['year_trend']['counts']
            if years:
                report.append("ğŸ“… ì—°ë„ë³„ ì¶œíŒ ë™í–¥:")
                for year, count in zip(years[-5:], counts[-5:]):  # ìµœê·¼ 5ë…„
                    bar = "â–ˆ" * (count * 2)
                    report.append(f"   {year}: {bar} ({count})")

                # íŠ¸ë Œë“œ ë¶„ì„
                if len(counts) >= 2:
                    recent_avg = sum(counts[-2:]) / 2
                    older_avg = sum(counts[:-2]) / max(len(counts) - 2, 1) if len(counts) > 2 else counts[0]
                    if recent_avg > older_avg * 1.2:
                        report.append(f"\n   ğŸ“ˆ íŠ¸ë Œë“œ: ìƒìŠ¹ì„¸ (ìµœê·¼ ì—°êµ¬ ì¦ê°€)")
                    elif recent_avg < older_avg * 0.8:
                        report.append(f"\n   ğŸ“‰ íŠ¸ë Œë“œ: í•˜ë½ì„¸ (ì—°êµ¬ ê´€ì‹¬ ê°ì†Œ)")
                    else:
                        report.append(f"\n   â¡ï¸ íŠ¸ë Œë“œ: ì•ˆì •ì  (ê¾¸ì¤€í•œ ì—°êµ¬)")

        # í•µì‹¬ í‚¤ì›Œë“œ
        if 'key_terms' in self.trend_data:
            report.append("\nğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ (Top 10):")
            terms = self.trend_data['key_terms']['terms'][:10]
            counts = self.trend_data['key_terms']['counts'][:10]
            for i, (term, count) in enumerate(zip(terms, counts), 1):
                report.append(f"   {i:2d}. {term} ({count}íšŒ)")

        # ê¸‰ë¶€ìƒ ì£¼ì œ
        if 'emerging_topics' in self.trend_data and self.trend_data['emerging_topics']:
            report.append("\nğŸš€ ê¸‰ë¶€ìƒ ì£¼ì œ (ìµœê·¼ vs ê³¼ê±°):")
            for i, e in enumerate(self.trend_data['emerging_topics'][:5], 1):
                growth = e['growth_rate']
                if growth == 999:
                    growth_str = "NEW!"
                else:
                    growth_str = f"{growth:.1f}x"
                report.append(f"   {i}. {e['term']} - {growth_str} ì„±ì¥")

        # ì—°êµ¬ ë‚´ìš© ìš”ì•½
        if 'content_summary' in self.trend_data and self.trend_data['content_summary']:
            summary_data = self.trend_data['content_summary']
            report.append("\n" + "-" * 60)
            report.append("ğŸ“‹ ì—°êµ¬ ë‚´ìš© ìš”ì•½")
            report.append("-" * 60)

            # ì£¼ìš” ì—°êµ¬ ì£¼ì œ
            if 'themes' in summary_data and summary_data['themes']:
                theme_names = {
                    'diagnosis': 'ì§„ë‹¨/ê²€ì¶œ',
                    'treatment': 'ì¹˜ë£Œ/ì•½ë¬¼',
                    'mechanism': 'ë©”ì»¤ë‹ˆì¦˜/ê²½ë¡œ',
                    'prevention': 'ì˜ˆë°©/ìœ„í—˜ìš”ì¸',
                    'clinical': 'ì„ìƒì—°êµ¬',
                    'review': 'ì¢…í•©ë¦¬ë·°',
                    'epidemiology': 'ì—­í•™ì—°êµ¬',
                    'genetics': 'ìœ ì „í•™ì—°êµ¬'
                }
                report.append("\nğŸ”¬ ì£¼ìš” ì—°êµ¬ ë¶„ì•¼:")
                for theme, count in summary_data['themes'][:5]:
                    theme_name = theme_names.get(theme, theme)
                    bar = "â–“" * min(count, 20)
                    report.append(f"   â€¢ {theme_name}: {bar} ({count}í¸)")

            # ìš”ì•½ í…ìŠ¤íŠ¸
            if 'summary_text' in summary_data and summary_data['summary_text']:
                report.append("\nğŸ“ ì—°êµ¬ ë™í–¥ ìš”ì•½:")
                summary_lines = summary_data['summary_text'].split('\n')
                for line in summary_lines:
                    if line.strip():
                        report.append(f"   {line}")

            # ëŒ€í‘œ ë…¼ë¬¸ (ìµœê·¼ 10ê°œ)
            if 'representative_papers' in summary_data and summary_data['representative_papers']:
                report.append("\nğŸ“š ëŒ€í‘œ ë…¼ë¬¸ (ìµœê·¼ 10ê°œ):")
                report.append("-" * 50)
                for i, paper in enumerate(summary_data['representative_papers'][:10], 1):
                    title = paper.get('title', '')[:65]
                    year = paper.get('published', paper.get('year', 'N/A'))[:4] if paper.get('published') or paper.get('year') else 'N/A'
                    authors = paper.get('authors', [])
                    if isinstance(authors, list):
                        author_str = ', '.join(authors[:2])
                        if len(authors) > 2:
                            author_str += ' et al.'
                    else:
                        author_str = str(authors)[:40]
                    source = paper.get('source', '')

                    report.append(f"\n   [{i:2d}] {title}...")
                    report.append(f"       ğŸ“… {year} | ğŸ‘¤ {author_str}")
                    if source:
                        report.append(f"       ğŸ“– {source}")

        report.append("\n" + "=" * 60)

        return "\n".join(report)

    def analyze(self, keyword: str, max_results: int = 50, source: str = 'pubmed') -> str:
        """ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰"""
        # 1. ë…¼ë¬¸ ê²€ìƒ‰
        self.search_papers_for_trend(keyword, max_results, source)

        if not self.papers:
            return f"âŒ '{keyword}'ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 2. ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
        print("\nğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        self.generate_search_summary(keyword)

        # 3. ë¶„ì„ ìˆ˜í–‰
        print("ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        self.analyze_publication_trend()
        self.extract_key_terms()
        self.identify_emerging_topics()

        # 4. ì—°êµ¬ ë‚´ìš© ìš”ì•½ ìƒì„±
        print("ğŸ“ ì—°êµ¬ ë‚´ìš© ìš”ì•½ ìƒì„± ì¤‘...")
        self.summarize_research_content(keyword)

        # 5. ì‹œê°í™”
        print("ğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
        self.visualize_trends(keyword)

        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_trend_report(keyword)
        print(report)

        return report


# ==================== PDFDownloader í´ë˜ìŠ¤ ====================
class PDFDownloader:
    def __init__(self, save_dir: str = PAPERS_DIR):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def download(self, paper: Dict, show_progress: bool = True) -> Optional[str]:
        safe_title = re.sub(r'[^\w\s-]', '', paper['title'])[:50]
        filename = f"{paper['id']}_{safe_title}.pdf"
        filepath = os.path.join(self.save_dir, filename)
        txt_filepath = filepath.replace('.pdf', '.txt')

        if os.path.exists(txt_filepath):
            return txt_filepath
        if os.path.exists(filepath):
            return filepath

        pdf_url = paper.get('pdf_url') or paper.get('pmc_url')

        if not pdf_url:
            if paper['source'] == 'PubMed':
                return self._save_abstract_as_text(paper, filename)
            return None

        try:
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; ResearchBot/1.0)'}
            response = requests.get(pdf_url, headers=headers, timeout=60, stream=True)
            response.raise_for_status()

            # Content-Type í™•ì¸
            content_type = response.headers.get('content-type', '')
            if 'html' in content_type or 'text' in content_type:
                return self._save_abstract_as_text(paper, filename)

            # íŒŒì¼ í¬ê¸° í™•ì¸
            total_size = int(response.headers.get('content-length', 0))

            # ì²˜ìŒ ëª‡ ë°”ì´íŠ¸ë¡œ HTML ì—¬ë¶€ í™•ì¸
            first_chunk = next(response.iter_content(chunk_size=5), b'')
            if first_chunk.startswith(b'<html') or first_chunk.startswith(b'<!DOC'):
                return self._save_abstract_as_text(paper, filename)

            # ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ with ì§„í–‰ ë°”
            with open(filepath, 'wb') as f:
                f.write(first_chunk)  # ì²« ì²­í¬ ê¸°ë¡

                if show_progress and total_size > 0:
                    # ì§„í–‰ ë°” í‘œì‹œ
                    with tqdm(
                        total=total_size,
                        initial=len(first_chunk),
                        unit='B',
                        unit_scale=True,
                        unit_divisor=1024,
                        desc=f"   ğŸ“¥ {filename[:30]}",
                        bar_format='{desc}: {percentage:3.0f}%|{bar:20}| {n_fmt}/{total_fmt} [{rate_fmt}]',
                        leave=False
                    ) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
                else:
                    # ì§„í–‰ ë°” ì—†ì´ ë‹¤ìš´ë¡œë“œ
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)

            return filepath

        except Exception as e:
            return self._save_abstract_as_text(paper, filename)

    def _save_abstract_as_text(self, paper: Dict, filename: str) -> Optional[str]:
        txt_filename = filename.replace('.pdf', '.txt')
        filepath = os.path.join(self.save_dir, txt_filename)

        content = f"""Title: {paper['title']}

Authors: {', '.join(paper['authors'])}

Source: {paper['source']}

Published: {paper['published']}

Abstract:
{paper['abstract']}

URL: {paper.get('pubmed_url', paper.get('pdf_url', 'N/A'))}
"""

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"   ğŸ“ ì´ˆë¡ ì €ì¥: {txt_filename[:40]}...")
        return filepath

    def download_all(self, papers: List[Dict]) -> List[str]:
        total = len(papers)
        if total == 0:
            print("   âš ï¸ ë‹¤ìš´ë¡œë“œí•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print("\n" + "=" * 60)
        print("ğŸ“¥ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ")
        print("=" * 60)
        print(f"   ì´ {total}ê°œ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì˜ˆì •\n")

        downloaded = []
        skipped = 0
        pdf_count = 0
        abstract_count = 0
        failed = 0

        # ì „ì²´ ì§„í–‰ ìƒí™© ë°”
        with tqdm(
            total=total,
            desc="   ğŸ“š ì „ì²´ ì§„í–‰",
            bar_format='{desc}: {percentage:3.0f}%|{bar:30}| {n}/{total} [{elapsed}<{remaining}]',
            colour='green'
        ) as pbar:
            for i, paper in enumerate(papers):
                title = paper.get('title', 'Unknown')[:35]

                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                safe_title = re.sub(r'[^\w\s-]', '', paper['title'])[:50]
                filename = f"{paper['id']}_{safe_title}.pdf"
                filepath = os.path.join(self.save_dir, filename)
                txt_filepath = filepath.replace('.pdf', '.txt')

                if os.path.exists(txt_filepath) or os.path.exists(filepath):
                    skipped += 1
                    pbar.set_postfix_str(f"â­ï¸ ìŠ¤í‚µ: {title}...")
                    downloaded.append(txt_filepath if os.path.exists(txt_filepath) else filepath)
                else:
                    pbar.set_postfix_str(f"ğŸ“¥ {title}...")
                    result_path = self.download(paper, show_progress=False)

                    if result_path:
                        downloaded.append(result_path)
                        if result_path.endswith('.pdf'):
                            pdf_count += 1
                        else:
                            abstract_count += 1
                    else:
                        failed += 1

                pbar.update(1)
                time.sleep(0.2)

        # ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½
        print("\n" + "-" * 60)
        print("ğŸ“Š ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½")
        print("-" * 60)
        print(f"   âœ… ì´ ë‹¤ìš´ë¡œë“œ: {len(downloaded)}ê°œ")
        print(f"      ğŸ“„ PDF íŒŒì¼: {pdf_count}ê°œ")
        print(f"      ğŸ“ ì´ˆë¡ ì €ì¥: {abstract_count}ê°œ")
        print(f"      â­ï¸ ê¸°ì¡´ íŒŒì¼: {skipped}ê°œ")
        if failed > 0:
            print(f"      âŒ ì‹¤íŒ¨: {failed}ê°œ")
        print("-" * 60)

        return downloaded


# ==================== TextExtractor í´ë˜ìŠ¤ ====================
class TextExtractor:
    @staticmethod
    def extract(filepath: str) -> str:
        if filepath.endswith('.txt'):
            return TextExtractor._extract_from_txt(filepath)
        elif filepath.endswith('.pdf'):
            return TextExtractor._extract_from_pdf(filepath)
        return ""

    @staticmethod
    def _extract_from_txt(filepath: str) -> str:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            return ""

    @staticmethod
    def _extract_from_pdf(filepath: str) -> str:
        text = ""
        try:
            reader = PdfReader(filepath)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"

            if len(text) < 100:
                text = ""
                with pdfplumber.open(filepath) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
        except Exception as e:
            print(f"      âš ï¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)[:50]}")

        return text.strip()

    @staticmethod
    def extract_all(filepaths: List[str]) -> List[Dict]:
        print("\nğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\n")

        documents = []
        for filepath in filepaths:
            filename = os.path.basename(filepath)
            print(f"   ğŸ“– {filename[:50]}...")

            text = TextExtractor.extract(filepath)
            if text:
                documents.append({
                    'text': text,
                    'source': filename,
                    'filepath': filepath
                })
                print(f"      âœ… {len(text):,} ê¸€ì ì¶”ì¶œ")
            else:
                print(f"      âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ")

        print(f"\nğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
        return documents


# ==================== ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤ (sentence-transformers ì—†ì´) ====================
from langchain_core.embeddings import Embeddings

os.environ["SAFETENSORS_FAST_GPU"] = "1"


class HuggingFaceEmbeddings(Embeddings):
    """HuggingFace Transformersë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© í´ë˜ìŠ¤ (sentence-transformers ë¶ˆí•„ìš”)"""

    def __init__(self, model_name: str, device: str = 'cpu'):
        import torch
        from transformers import AutoTokenizer, AutoModel

        self.device = device
        self.model_name = model_name

        print(f"   ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()

    def _mean_pooling(self, model_output, attention_mask):
        """Mean pooling - attention maskë¥¼ ê³ ë ¤í•œ í‰ê· """
        import torch
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def _encode(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        import torch
        import torch.nn.functional as F

        # í† í°í™”
        encoded_input = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            model_output = self.model(**encoded_input)

        # Mean pooling
        embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])

        # ì •ê·œí™”
        embeddings = F.normalize(embeddings, p=2, dim=1)

        return embeddings.cpu().numpy().tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”©"""
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = 8
        all_embeddings = []
        total_batches = (len(texts) + batch_size - 1) // batch_size

        if len(texts) > batch_size:
            # tqdm ì§„í–‰ ë°” ì‚¬ìš©
            with tqdm(
                total=len(texts),
                desc="   ğŸ§  ì„ë² ë”© ìƒì„±",
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}, {rate_fmt}]',
                unit="ë¬¸ì„œ",
                colour='cyan'
            ) as pbar:
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i+batch_size]
                    embeddings = self._encode(batch)
                    all_embeddings.extend(embeddings)
                    pbar.update(len(batch))
        else:
            # ì ì€ ìˆ˜ì˜ ë¬¸ì„œëŠ” ì§„í–‰ ë°” ì—†ì´ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                embeddings = self._encode(batch)
                all_embeddings.extend(embeddings)

        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        return self._encode([text])[0]


class OpenAIEmbeddings(Embeddings):
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© í´ë˜ìŠ¤"""

    def __init__(self, api_key: str, model: str = "text-embedding-3-small"):
        from openai import OpenAI
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.dimension = 1536 if "small" in model else 3072

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”©"""
        all_embeddings = []
        batch_size = 100  # OpenAI ë°°ì¹˜ ì œí•œ

        if len(texts) > batch_size:
            # tqdm ì§„í–‰ ë°” ì‚¬ìš©
            with tqdm(
                total=len(texts),
                desc="   ğŸ§  OpenAI ì„ë² ë”©",
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}, {rate_fmt}]',
                unit="ë¬¸ì„œ",
                colour='green'
            ) as pbar:
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i+batch_size]
                    response = self.client.embeddings.create(
                        model=self.model,
                        input=batch
                    )
                    batch_embeddings = [item.embedding for item in response.data]
                    all_embeddings.extend(batch_embeddings)
                    pbar.update(len(batch))
        else:
            # ì ì€ ìˆ˜ì˜ ë¬¸ì„œëŠ” ì§„í–‰ ë°” ì—†ì´ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                response = self.client.embeddings.create(
                    model=self.model,
                    input=batch
                )
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)

        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        response = self.client.embeddings.create(
            model=self.model,
            input=[text]
        )
        return response.data[0].embedding


# ==================== Reranker í´ë˜ìŠ¤ ====================
class Reranker:
    """Cross-Encoder ê¸°ë°˜ Reranking ëª¨ë¸"""

    MODELS = {
        'ms-marco': {
            'name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
            'description': 'MS MARCO í•™ìŠµ (ë¹ ë¦„, ì¼ë°˜ìš©)',
        },
        'ms-marco-large': {
            'name': 'cross-encoder/ms-marco-MiniLM-L-12-v2',
            'description': 'MS MARCO í•™ìŠµ (ì •í™•, ì¼ë°˜ìš©)',
        },
        'bge-reranker': {
            'name': 'BAAI/bge-reranker-base',
            'description': 'BGE Reranker (ë‹¤êµ­ì–´ ì§€ì›)',
        },
        'bge-reranker-large': {
            'name': 'BAAI/bge-reranker-large',
            'description': 'BGE Reranker Large (ê³ ì •í™•ë„)',
        },
    }

    def __init__(self, model_type: str = 'ms-marco', device: str = None):
        """
        Reranker ì´ˆê¸°í™”

        Args:
            model_type: ëª¨ë¸ íƒ€ì… ('ms-marco', 'ms-marco-large', 'bge-reranker', 'bge-reranker-large')
            device: 'cuda', 'mps', 'cpu' ë˜ëŠ” None (ìë™ ê°ì§€)
        """
        import torch
        from transformers import AutoModelForSequenceClassification, AutoTokenizer

        if device is None:
            if torch.cuda.is_available():
                device = 'cuda'
            elif torch.backends.mps.is_available():
                device = 'mps'
            else:
                device = 'cpu'

        self.device = device
        self.model_type = model_type

        model_info = self.MODELS.get(model_type, self.MODELS['ms-marco'])
        model_name = model_info['name']

        print(f"   ğŸ”„ Reranker ëª¨ë¸ ë¡œë”©: {model_name}")
        print(f"   ğŸ“ Device: {device}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()

        print(f"   âœ… Reranker ë¡œë“œ ì™„ë£Œ!")

    def compute_score(self, query: str, document: str) -> float:
        """ë‹¨ì¼ ì¿¼ë¦¬-ë¬¸ì„œ ìŒì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        import torch

        inputs = self.tokenizer(
            query, document,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model(**inputs)
            # Sigmoidë¥¼ ì ìš©í•˜ì—¬ 0-1 ë²”ìœ„ë¡œ ë³€í™˜
            score = torch.sigmoid(outputs.logits).squeeze().item()

        return score

    def rerank(
        self,
        query: str,
        documents: List[Dict],
        top_k: int = None,
        content_key: str = 'content',
        show_progress: bool = True
    ) -> List[Dict]:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ Reranking

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ê° ë¬¸ì„œëŠ” dict)
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ë°˜í™˜)
            content_key: ë¬¸ì„œ dictì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ í‚¤
            show_progress: ì§„í–‰ ë°” í‘œì‹œ ì—¬ë¶€

        Returns:
            rerank_scoreê°€ ì¶”ê°€ëœ ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        import torch

        if not documents:
            return []

        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
        pairs = [(query, doc.get(content_key, '')) for doc in documents]
        scores = []

        batch_size = 8

        if show_progress and len(documents) > batch_size:
            iterator = tqdm(
                range(0, len(pairs), batch_size),
                desc="   ğŸ¯ Reranking",
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
                colour='yellow',
                total=(len(pairs) + batch_size - 1) // batch_size
            )
        else:
            iterator = range(0, len(pairs), batch_size)

        for i in iterator:
            batch_pairs = pairs[i:i+batch_size]

            inputs = self.tokenizer(
                [p[0] for p in batch_pairs],
                [p[1] for p in batch_pairs],
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                batch_scores = torch.sigmoid(outputs.logits).squeeze(-1)

                if batch_scores.dim() == 0:
                    scores.append(batch_scores.item())
                else:
                    scores.extend(batch_scores.cpu().tolist())

        # ê²°ê³¼ì— rerank_score ì¶”ê°€
        reranked_docs = []
        for doc, score in zip(documents, scores):
            doc_copy = doc.copy()
            doc_copy['rerank_score'] = score
            doc_copy['original_score'] = doc.get('hybrid_score', doc.get('score', 0))
            reranked_docs.append(doc_copy)

        # rerank_scoreë¡œ ì •ë ¬
        reranked_docs.sort(key=lambda x: x['rerank_score'], reverse=True)

        if top_k:
            return reranked_docs[:top_k]
        return reranked_docs


# ==================== EmbeddingModelFactory í´ë˜ìŠ¤ ====================
class EmbeddingModelFactory:
    """ì„ë² ë”© ëª¨ë¸ íŒ©í† ë¦¬ - HuggingFace ë˜ëŠ” OpenAI ì„ íƒ ê°€ëŠ¥"""

    MODELS = {
        # PubMed/ì˜í•™ íŠ¹í™” ëª¨ë¸
        'pubmedbert': {
            'name': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',
            'description': 'PubMed ë…¼ë¬¸ íŠ¹í™” (PubMedBERT Full)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'pubmedbert-abs': {
            'name': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',
            'description': 'PubMed ì´ˆë¡ íŠ¹í™” (PubMedBERT Abstract)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'biobert': {
            'name': 'dmis-lab/biobert-base-cased-v1.2',
            'description': 'ì˜í•™/ìƒë¬¼í•™ íŠ¹í™” (BioBERT v1.2)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'scibert': {
            'name': 'allenai/scibert_scivocab_uncased',
            'description': 'ê³¼í•™ ë…¼ë¬¸ íŠ¹í™” (SciBERT)',
            'dimension': 768,
            'type': 'huggingface'
        },
        'biolinkbert': {
            'name': 'michiyasunaga/BioLinkBERT-base',
            'description': 'ì˜í•™ ë¬¸í—Œ ë§í¬ í•™ìŠµ (BioLinkBERT)',
            'dimension': 768,
            'type': 'huggingface'
        },
        # ì¼ë°˜ ëª¨ë¸
        'bert-base': {
            'name': 'bert-base-uncased',
            'description': 'BERT ê¸°ë³¸ ëª¨ë¸',
            'dimension': 768,
            'type': 'huggingface'
        },
        'minilm': {
            'name': 'sentence-transformers/all-MiniLM-L6-v2',
            'description': 'ì¼ë°˜ ëª©ì  (ê°€ë³ê³  ë¹ ë¦„)',
            'dimension': 384,
            'type': 'huggingface'
        },
        # OpenAI API ëª¨ë¸
        'openai-small': {
            'name': 'text-embedding-3-small',
            'description': 'OpenAI ì„ë² ë”© (ë¹ ë¦„, ì €ë ´)',
            'dimension': 1536,
            'type': 'openai'
        },
        'openai-large': {
            'name': 'text-embedding-3-large',
            'description': 'OpenAI ì„ë² ë”© (ê³ ì„±ëŠ¥)',
            'dimension': 3072,
            'type': 'openai'
        }
    }

    @classmethod
    def create(cls, model_type: str = 'biobert', device: str = 'cpu', openai_api_key: str = None):
        if model_type not in cls.MODELS:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_type}. 'biobert' ì‚¬ìš©")
            model_type = 'biobert'

        model_info = cls.MODELS[model_type]

        print(f"\nğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"   ëª¨ë¸: {model_type}")
        print(f"   ì„¤ëª…: {model_info['description']}")
        print(f"   ì°¨ì›: {model_info['dimension']}")

        # OpenAI ëª¨ë¸ì¸ ê²½ìš°
        if model_info['type'] == 'openai':
            if not openai_api_key:
                print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. biobertë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return cls.create('biobert', device, None)

            try:
                embeddings = OpenAIEmbeddings(api_key=openai_api_key, model=model_info['name'])
                print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                return embeddings
            except Exception as e:
                print(f"âš ï¸ OpenAI ì„ë² ë”© ì‹¤íŒ¨: {str(e)[:50]}")
                print("   biobertë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
                return cls.create('biobert', device, None)

        # HuggingFace ëª¨ë¸ì¸ ê²½ìš°
        try:
            embeddings = HuggingFaceEmbeddings(model_name=model_info['name'], device=device)
            print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            return embeddings

        except Exception as e:
            print(f"âš ï¸ {model_type} ë¡œë“œ ì‹¤íŒ¨: {str(e)[:100]}")
            print("   bert-base ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")

            try:
                return HuggingFaceEmbeddings(
                    model_name='bert-base-uncased',
                    device=device
                )
            except Exception as e2:
                print(f"âš ï¸ bert-baseë„ ì‹¤íŒ¨: {str(e2)[:50]}")
                print("   OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
                if openai_api_key:
                    return OpenAIEmbeddings(api_key=openai_api_key)
                raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")


# ==================== RAGSystem í´ë˜ìŠ¤ ====================
class RAGSystem:
    def __init__(self, embeddings, chunk_size: int = 1000, chunk_overlap: int = 200, language: str = 'en'):
        self.embeddings = embeddings
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.vectorstore = None
        self.language = language
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def build_vectorstore(self, documents: List[Dict]) -> FAISS:
        print("\n" + "=" * 60)
        print("ğŸ”§ ë²¡í„° DB ìƒì„±")
        print("=" * 60)

        # Step 1: í…ìŠ¤íŠ¸ ì²­í‚¹
        print("\nâœ‚ï¸ Step 1: í…ìŠ¤íŠ¸ ì²­í‚¹")
        all_chunks = []
        all_metadata = []

        with tqdm(
            total=len(documents),
            desc="   ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='yellow'
        ) as pbar:
            for doc in documents:
                chunks = self.text_splitter.split_text(doc['text'])
                for i, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'source': doc['source'],
                        'chunk_id': i
                    })
                pbar.set_postfix_str(f"{len(chunks)} ì²­í¬")
                pbar.update(1)

        print(f"   ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}ê°œ")

        if not all_chunks:
            raise ValueError("ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")

        # Step 2: ì„ë² ë”© ìƒì„± ë° ë²¡í„° DB ìƒì„±
        print("\nğŸ§  Step 2: ì„ë² ë”© ìƒì„± ë° ë²¡í„° DB êµ¬ì¶•")

        self.vectorstore = FAISS.from_texts(
            texts=all_chunks,
            embedding=self.embeddings,
            metadatas=all_metadata
        )

        print("\nâœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ!")
        print("-" * 60)
        return self.vectorstore

    def build_vectorstore_from_abstracts(self, papers: List[Dict], append: bool = False) -> FAISS:
        """
        ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ì˜ Abstractë§Œìœ¼ë¡œ VectorDB ìƒì„± (PDF ë‹¤ìš´ë¡œë“œ ì—†ì´)

        Args:
            papers: PaperSearcher.search()ì—ì„œ ë°˜í™˜ëœ ë…¼ë¬¸ ëª©ë¡
                   ê° ë…¼ë¬¸: {'title', 'abstract', 'source', 'authors', 'published', ...}
            append: Trueë©´ ê¸°ì¡´ VectorDBì— ì¶”ê°€, Falseë©´ ìƒˆë¡œ ìƒì„±

        Returns:
            FAISS vectorstore
        """
        if append and self.vectorstore:
            print("\n" + "=" * 60)
            print("â• ê¸°ì¡´ VectorDBì— ìƒˆ ë…¼ë¬¸ ì¶”ê°€")
            print("=" * 60)
        else:
            print("\n" + "=" * 60)
            print("ğŸš€ Abstract ê¸°ë°˜ ë²¡í„° DB ìƒì„± (PDF ë‹¤ìš´ë¡œë“œ ìƒëµ)")
            print("=" * 60)

        if not papers:
            raise ValueError("ë…¼ë¬¸ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")

        # Step 1: Abstractë¥¼ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print(f"\nğŸ“„ Step 1: {len(papers)}ê°œ ë…¼ë¬¸ Abstract ì¶”ì¶œ")
        documents = []

        for paper in papers:
            abstract = paper.get('abstract', '')
            title = paper.get('title', 'Unknown Title')
            source = paper.get('source', 'Unknown')
            authors = paper.get('authors', [])
            published = paper.get('published', 'Unknown')
            paper_id = paper.get('id', 'Unknown')

            if not abstract or abstract == 'No abstract available':
                print(f"   âš ï¸ Abstract ì—†ìŒ: {title[:40]}...")
                continue

            # ë…¼ë¬¸ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ì— í¬í•¨
            full_text = f"Title: {title}\n\n"
            if authors:
                full_text += f"Authors: {', '.join(authors[:5])}\n"
            full_text += f"Published: {published}\n"
            full_text += f"Source: {source}\n\n"
            full_text += f"Abstract:\n{abstract}"

            documents.append({
                'text': full_text,
                'source': f"{source}_{paper_id}"
            })
            print(f"   âœ… {title[:50]}...")

        print(f"   ğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œ ìƒì„±")

        if not documents:
            raise ValueError("ì¶”ì¶œëœ Abstractê°€ ì—†ìŠµë‹ˆë‹¤!")

        # Step 2: VectorDB ìƒì„± ë˜ëŠ” ì¶”ê°€
        if append and self.vectorstore:
            # ê¸°ì¡´ VectorDBì— ìƒˆ ë¬¸ì„œ ì¶”ê°€
            return self._append_to_vectorstore(documents)
        else:
            # ìƒˆë¡œ ìƒì„±
            return self.build_vectorstore(documents)

    def _append_to_vectorstore(self, documents: List[Dict]) -> FAISS:
        """ê¸°ì¡´ VectorDBì— ìƒˆ ë¬¸ì„œ ì¶”ê°€"""
        print("\nğŸ”§ ìƒˆ ë¬¸ì„œë¥¼ ê¸°ì¡´ VectorDBì— ì¶”ê°€")

        # ìƒˆ ë¬¸ì„œ ì²­í‚¹
        all_chunks = []
        all_metadata = []

        for doc in documents:
            chunks = self.text_splitter.split_text(doc['text'])
            for i, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                all_metadata.append({
                    'source': doc['source'],
                    'chunk_id': i
                })

        print(f"   ğŸ“Š ìƒˆ ì²­í¬ ìˆ˜: {len(all_chunks)}ê°œ")

        if not all_chunks:
            print("   âš ï¸ ì¶”ê°€í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self.vectorstore

        # ìƒˆ VectorDB ìƒì„±
        new_vectorstore = FAISS.from_texts(
            texts=all_chunks,
            embedding=self.embeddings,
            metadatas=all_metadata
        )

        # ê¸°ì¡´ VectorDBì™€ ë³‘í•©
        self.vectorstore.merge_from(new_vectorstore)

        print("   âœ… VectorDB ë³‘í•© ì™„ë£Œ!")
        return self.vectorstore

    def save_vectorstore(self, path: str = VECTORSTORE_DIR):
        if self.vectorstore:
            self.vectorstore.save_local(path)
            print(f"ğŸ’¾ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {path}")

    def load_vectorstore(self, path: str = VECTORSTORE_DIR) -> bool:
        """ì €ì¥ëœ VectorDB ë¡œë“œ"""
        import os
        if os.path.exists(path):
            try:
                self.vectorstore = FAISS.load_local(
                    path,
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print(f"ğŸ“‚ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ: {path}")
                return True
            except Exception as e:
                print(f"âŒ ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
        else:
            print(f"âŒ ë²¡í„° DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
            return False

    def search(self, query: str, k: int = 3) -> List[Dict]:
        if not self.vectorstore:
            print("âŒ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # LangSmith íŠ¸ë ˆì´ì‹±
        with LangSmithTracer("RAG_Search", run_type="retriever", metadata={"query": query, "k": k}) as tracer:
            tracer.log_input({"query": query, "k": k})

            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)

            results = []
            for doc, score in docs_with_scores:
                results.append({
                    'content': doc.page_content,
                    'source': doc.metadata.get('source', 'Unknown'),
                    'score': float(score)
                })

            tracer.log_output({"results_count": len(results), "results": results})

        return results

    def answer(self, question: str, k: int = 3) -> Dict:
        # LangSmith íŠ¸ë ˆì´ì‹±
        with LangSmithTracer("RAG_Answer", run_type="chain", metadata={"question": question}) as tracer:
            tracer.log_input({"question": question, "k": k})

            # ì§ˆë¬¸ ì–¸ì–´ ê°ì§€
            q_language = detect_language(question)

            results = self.search(question, k=k)

            response = {
                'question': question,
                'contexts': results,
                'sources': list(set([r['source'] for r in results])),
                'language': q_language
            }

            tracer.log_output(response)

        return response

    def get_all_chunks(self) -> List[Dict]:
        """ì €ì¥ëœ ëª¨ë“  ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜"""
        if not self.vectorstore:
            return []

        # FAISSì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        docstore = self.vectorstore.docstore
        index_to_id = self.vectorstore.index_to_docstore_id

        chunks = []
        for idx, doc_id in index_to_id.items():
            doc = docstore.search(doc_id)
            if doc:
                chunks.append({
                    'content': doc.page_content,
                    'source': doc.metadata.get('source', 'Unknown'),
                    'chunk_id': doc.metadata.get('chunk_id', idx)
                })
        return chunks


# ==================== Qdrant Hybrid Search ì‹œìŠ¤í…œ ====================
class QdrantHybridSearch:
    """
    Qdrant ê¸°ë°˜ Hybrid Search ì‹œìŠ¤í…œ
    - Named Vectors: dense + sparse ë™ì‹œ ì €ì¥
    - Payload Filtering: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§
    - HNSW Index: ë¹ ë¥¸ ANN ê²€ìƒ‰
    """

    def __init__(
        self,
        embeddings,
        sparse_encoder=None,
        collection_name: str = "papers",
        use_memory: bool = True,
        qdrant_url: str = None,
        sparse_method: str = 'bm25',
        reranker: 'Reranker' = None,
        use_reranking: bool = False,
        reranker_model: str = 'ms-marco'
    ):
        from qdrant_client import QdrantClient
        from qdrant_client.models import (
            VectorParams, SparseVectorParams, Distance,
            HnswConfigDiff, OptimizersConfigDiff
        )

        self.embeddings = embeddings
        self.collection_name = collection_name
        self.sparse_method = sparse_method.lower()
        self.chunks = []
        self.splade_encoder = None
        self.reranker = reranker
        self.use_reranking = use_reranking

        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if use_memory:
            print("\nğŸ—„ï¸ Qdrant ì¸ë©”ëª¨ë¦¬ ëª¨ë“œë¡œ ì´ˆê¸°í™”...")
            self.client = QdrantClient(":memory:")
        else:
            print(f"\nğŸ—„ï¸ Qdrant ì„œë²„ ì—°ê²°: {qdrant_url}")
            self.client = QdrantClient(url=qdrant_url)

        # Reranker ì´ˆê¸°í™” (ìš”ì²­ ì‹œ)
        if use_reranking and self.reranker is None:
            print("\nğŸ¯ Reranker ì´ˆê¸°í™”...")
            self.reranker = Reranker(model_type=reranker_model)

        # Dense ë²¡í„° ì°¨ì› í™•ì¸
        test_embedding = self.embeddings.embed_query("test")
        self.dense_dim = len(test_embedding)
        print(f"   ğŸ“ Dense ë²¡í„° ì°¨ì›: {self.dense_dim}")

        # Sparse ì¸ì½”ë” ì´ˆê¸°í™”
        if self.sparse_method == 'splade':
            self._init_splade_encoder(sparse_encoder)
        else:
            # BM25ìš© ìŠ¤í…Œë¨¸ ì´ˆê¸°í™”
            self._init_stemmer()
            print(f"   ğŸ”¤ Sparse ë°©ì‹: BM25 (ìŠ¤í…Œë°)")

    def _init_splade_encoder(self, sparse_encoder=None):
        """SPLADE ì¸ì½”ë” ì´ˆê¸°í™”"""
        if sparse_encoder is not None:
            self.splade_encoder = sparse_encoder
            print(f"   ğŸ”¤ Sparse ë°©ì‹: SPLADE (ì™¸ë¶€ ì¸ì½”ë”)")
        else:
            try:
                print(f"   ğŸ”¤ SPLADE ì¸ì½”ë” ë¡œë”© ì¤‘...")
                self.splade_encoder = SPLADEEncoder(device='cpu')
                print(f"   âœ… SPLADE ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                print(f"   âš ï¸ SPLADE ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"   âš ï¸ BM25ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                self.sparse_method = 'bm25'
                self._init_stemmer()

    def _init_stemmer(self):
        """ìŠ¤í…Œë¨¸ ì´ˆê¸°í™”"""
        try:
            from nltk.stem import PorterStemmer
            self.stemmer = PorterStemmer()
            self.use_stemming = True
        except ImportError:
            self.stemmer = None
            self.use_stemming = False

    def _tokenize(self, text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì € + ìŠ¤í…Œë°"""
        import re
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)
        if self.use_stemming and self.stemmer:
            tokens = [self.stemmer.stem(token) for token in tokens]
        return tokens

    def _text_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """í…ìŠ¤íŠ¸ë¥¼ ìŠ¤íŒŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜ (BM25 ë˜ëŠ” SPLADE)"""
        if self.sparse_method == 'splade' and self.splade_encoder is not None:
            return self._splade_to_sparse_vector(text)
        else:
            return self._bm25_to_sparse_vector(text)

    def _bm25_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """BM25 ìŠ¤íƒ€ì¼ ìŠ¤íŒŒìŠ¤ ë²¡í„° ìƒì„±"""
        from collections import Counter
        import math

        tokens = self._tokenize(text)
        token_counts = Counter(tokens)

        sparse_vector = {}
        for token, count in token_counts.items():
            # í•´ì‹œ í•¨ìˆ˜ë¡œ ì¸ë±ìŠ¤ ìƒì„± (vocabulary ëŒ€ì‹ )
            idx = hash(token) % 30000  # 30000 ì°¨ì›ìœ¼ë¡œ ì œí•œ
            if idx < 0:
                idx = -idx
            tf = 1 + math.log(count) if count > 0 else 0
            sparse_vector[idx] = tf

        return sparse_vector

    def _splade_to_sparse_vector(self, text: str) -> Dict[int, float]:
        """SPLADE ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŒŒìŠ¤ ë²¡í„° ìƒì„±"""
        # SPLADE ì¸ì½”ë”© (token -> weight ë”•ì…”ë„ˆë¦¬ ë°˜í™˜)
        splade_result = self.splade_encoder.encode([text])[0]

        # token ë¬¸ìì—´ì„ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        sparse_vector = {}
        for token, weight in splade_result.items():
            # í•´ì‹œ í•¨ìˆ˜ë¡œ ì¸ë±ìŠ¤ ìƒì„±
            idx = hash(token) % 30000
            if idx < 0:
                idx = -idx
            # ê°™ì€ ì¸ë±ìŠ¤ì— ì—¬ëŸ¬ í† í°ì´ ë§¤í•‘ë˜ë©´ ìµœëŒ€ê°’ ì‚¬ìš©
            if idx in sparse_vector:
                sparse_vector[idx] = max(sparse_vector[idx], weight)
            else:
                sparse_vector[idx] = weight

        return sparse_vector

    def create_collection(self):
        """ì»¬ë ‰ì…˜ ìƒì„± (Named Vectors + HNSW ì¸ë±ìŠ¤)"""
        from qdrant_client.models import (
            VectorParams, SparseVectorParams, Distance,
            HnswConfigDiff, OptimizersConfigDiff
        )

        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        try:
            self.client.delete_collection(self.collection_name)
        except:
            pass

        print(f"\nğŸ“¦ Qdrant ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
        print("   ğŸ”§ HNSW ì¸ë±ìŠ¤ ì„¤ì • ì¤‘...")

        # Named Vectorsë¡œ ì»¬ë ‰ì…˜ ìƒì„±
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config={
                # Dense ë²¡í„° (HNSW ì¸ë±ìŠ¤)
                "dense": VectorParams(
                    size=self.dense_dim,
                    distance=Distance.COSINE,
                    hnsw_config=HnswConfigDiff(
                        m=16,              # ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•, ëŠë¦¼)
                        ef_construct=100,  # êµ¬ì¶• ì‹œ íƒìƒ‰ ë²”ìœ„
                        full_scan_threshold=10000
                    )
                )
            },
            sparse_vectors_config={
                # Sparse ë²¡í„°
                "sparse": SparseVectorParams()
            },
            optimizers_config=OptimizersConfigDiff(
                indexing_threshold=20000  # ì¸ë±ì‹± ì„ê³„ê°’
            )
        )

        print("   âœ… Dense ë²¡í„°: HNSW ì¸ë±ìŠ¤ (COSINE)")
        print(f"   âœ… Sparse ë²¡í„°: {self.sparse_method.upper()} + Inverted Index")

    def add_documents(self, documents: List[Dict], text_splitter):
        """ë¬¸ì„œ ì¶”ê°€ (ì²­í‚¹ + ì„ë² ë”© + ì €ì¥)"""
        from qdrant_client.models import PointStruct, SparseVector
        import uuid

        print("\n" + "=" * 60)
        print("ğŸ”§ Qdrant ë²¡í„° DB êµ¬ì¶•")
        print("=" * 60)

        # Step 1: ì²­í‚¹
        print("\nâœ‚ï¸ Step 1: í…ìŠ¤íŠ¸ ì²­í‚¹")
        all_chunks = []
        all_metadata = []

        with tqdm(
            total=len(documents),
            desc="   ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='yellow'
        ) as pbar:
            for doc in documents:
                chunks = text_splitter.split_text(doc['text'])
                for i, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'source': doc['source'],
                        'filepath': doc.get('filepath', ''),
                        'chunk_id': i,
                        'chunk_total': len(chunks),
                        'text_length': len(chunk)
                    })
                pbar.set_postfix_str(f"{len(chunks)} ì²­í¬")
                pbar.update(1)

        print(f"   ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}ê°œ")

        if not all_chunks:
            raise ValueError("ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")

        self.chunks = [{'content': c, **m} for c, m in zip(all_chunks, all_metadata)]

        # Step 2: Dense ì„ë² ë”© ìƒì„±
        print("\nğŸ§  Step 2: Dense ì„ë² ë”© ìƒì„±")
        dense_vectors = self.embeddings.embed_documents(all_chunks)

        # Step 3: Sparse ë²¡í„° ìƒì„±
        sparse_method_name = self.sparse_method.upper()
        print(f"\nğŸ”¤ Step 3: Sparse ë²¡í„° ìƒì„± ({sparse_method_name})")
        sparse_vectors = []

        with tqdm(
            total=len(all_chunks),
            desc=f"   ğŸ“Š {sparse_method_name}",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='magenta'
        ) as pbar:
            for chunk in all_chunks:
                sv = self._text_to_sparse_vector(chunk)
                sparse_vectors.append(sv)
                pbar.update(1)

        # Step 4: Qdrantì— ì €ì¥
        print("\nğŸ’¾ Step 4: Qdrant ì €ì¥")
        points = []

        with tqdm(
            total=len(all_chunks),
            desc="   ğŸ”§ í¬ì¸íŠ¸ ìƒì„±",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='blue'
        ) as pbar:
            for i, (chunk, dense_vec, sparse_vec, metadata) in enumerate(
                zip(all_chunks, dense_vectors, sparse_vectors, all_metadata)
            ):
                sparse_indices = list(sparse_vec.keys())
                sparse_values = list(sparse_vec.values())

                point = PointStruct(
                    id=str(uuid.uuid4()),
                    vector={
                        "dense": dense_vec,
                        "sparse": SparseVector(
                            indices=sparse_indices,
                            values=sparse_values
                        )
                    },
                    payload={
                        "text": chunk,
                        **metadata
                    }
                )
                points.append(point)
                pbar.update(1)

        # ë°°ì¹˜ ì—…ë¡œë“œ
        batch_size = 100
        total_batches = (len(points) + batch_size - 1) // batch_size

        with tqdm(
            total=total_batches,
            desc="   ğŸ“¤ Qdrant ì—…ë¡œë“œ",
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total} [{elapsed}<{remaining}]',
            colour='green'
        ) as pbar:
            for i in range(0, len(points), batch_size):
                batch = points[i:i + batch_size]
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=batch
                )
                pbar.update(1)

        print(f"\nâœ… Qdrant ì €ì¥ ì™„ë£Œ! ({len(points)}ê°œ ë²¡í„°)")
        print("-" * 60)

        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        info = self.client.get_collection(self.collection_name)
        print(f"   ğŸ“Š ì»¬ë ‰ì…˜ ìƒíƒœ: {info.status}")
        print(f"   ğŸ“Š í¬ì¸íŠ¸ ìˆ˜: {info.points_count}")

    def dense_search(self, query: str, k: int = 5, filter_conditions: Dict = None) -> List[Dict]:
        """Dense ë²¡í„° ê²€ìƒ‰ (HNSW ANN)"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, SearchParams

        query_vector = self.embeddings.embed_query(query)

        # í•„í„° ì¡°ê±´ êµ¬ì„±
        query_filter = None
        if filter_conditions:
            conditions = []
            for field, value in filter_conditions.items():
                conditions.append(
                    FieldCondition(key=field, match=MatchValue(value=value))
                )
            query_filter = Filter(must=conditions)

        # ìƒˆë¡œìš´ Qdrant API ì‚¬ìš©
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=query_vector,
            using="dense",
            query_filter=query_filter,
            limit=k,
            with_payload=True,
            search_params=SearchParams(hnsw_ef=128)
        )

        return [
            {
                'content': r.payload.get('text', ''),
                'source': r.payload.get('source', 'Unknown'),
                'score': r.score,
                'chunk_id': r.payload.get('chunk_id', 0),
                'method': 'dense'
            }
            for r in results.points
        ]

    def sparse_search(self, query: str, k: int = 5, filter_conditions: Dict = None) -> List[Dict]:
        """Sparse ë²¡í„° ê²€ìƒ‰"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, SparseVector

        sparse_vec = self._text_to_sparse_vector(query)
        sparse_indices = list(sparse_vec.keys())
        sparse_values = list(sparse_vec.values())

        # í•„í„° ì¡°ê±´ êµ¬ì„±
        query_filter = None
        if filter_conditions:
            conditions = []
            for field, value in filter_conditions.items():
                conditions.append(
                    FieldCondition(key=field, match=MatchValue(value=value))
                )
            query_filter = Filter(must=conditions)

        # ìƒˆë¡œìš´ Qdrant API ì‚¬ìš©
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=SparseVector(
                indices=sparse_indices,
                values=sparse_values
            ),
            using="sparse",
            query_filter=query_filter,
            limit=k,
            with_payload=True
        )

        return [
            {
                'content': r.payload.get('text', ''),
                'source': r.payload.get('source', 'Unknown'),
                'score': r.score,
                'chunk_id': r.payload.get('chunk_id', 0),
                'method': 'sparse'
            }
            for r in results.points
        ]

    def hybrid_search(
        self,
        query: str,
        k: int = 5,
        alpha: float = 0.5,
        filter_conditions: Dict = None,
        use_reranking: bool = None,
        rerank_top_k: int = None
    ) -> List[Dict]:
        """
        Hybrid ê²€ìƒ‰ (Dense + Sparse) + ì„ íƒì  Reranking

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: 0.0 = ìˆœìˆ˜ Sparse, 1.0 = ìˆœìˆ˜ Dense
            filter_conditions: í•„í„° ì¡°ê±´
            use_reranking: Reranking ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ self.use_reranking ì‚¬ìš©)
            rerank_top_k: Rerankingí•  í›„ë³´ ìˆ˜ (Noneì´ë©´ k*3)
        """
        import numpy as np

        # LangSmith íŠ¸ë ˆì´ì‹± ì‹œì‘
        _tracer = LangSmithTracer("Qdrant_Hybrid_Search", run_type="retriever",
                                 metadata={"query": query, "k": k, "alpha": alpha})
        _tracer.__enter__()
        _tracer.log_input({"query": query, "k": k, "alpha": alpha, "use_reranking": use_reranking})

        # Reranking ì„¤ì •
        apply_reranking = use_reranking if use_reranking is not None else self.use_reranking
        rerank_candidates = rerank_top_k if rerank_top_k else max(k * 3, 20)

        # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰ (reranking ì‹œ ë” ë§ì´)
        num_candidates = rerank_candidates if apply_reranking else max(k * 3, 20)

        dense_results = self.dense_search(query, k=num_candidates, filter_conditions=filter_conditions)
        sparse_results = self.sparse_search(query, k=num_candidates, filter_conditions=filter_conditions)

        # ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’
        dense_scores = [r['score'] for r in dense_results] if dense_results else [0]
        sparse_scores = [r['score'] for r in sparse_results] if sparse_results else [0]

        dense_max, dense_min = max(dense_scores), min(dense_scores)
        sparse_max, sparse_min = max(sparse_scores), min(sparse_scores)

        dense_range = dense_max - dense_min if dense_max > dense_min else 1.0
        sparse_range = sparse_max - sparse_min if sparse_max > sparse_min else 1.0

        # ê²°ê³¼ í†µí•© (source + chunk_idë¥¼ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš©)
        doc_scores = {}

        for r in dense_results:
            # source + chunk_id ì¡°í•©ìœ¼ë¡œ ê³ ìœ  í‚¤ ìƒì„±
            key = f"{r['source']}_{r['chunk_id']}"
            dense_norm = (r['score'] - dense_min) / dense_range
            doc_scores[key] = {
                'content': r['content'],
                'source': r['source'],
                'chunk_id': r['chunk_id'],
                'dense_score': r['score'],
                'dense_norm': dense_norm,
                'sparse_score': 0,
                'sparse_norm': 0
            }

        for r in sparse_results:
            # source + chunk_id ì¡°í•©ìœ¼ë¡œ ê³ ìœ  í‚¤ ìƒì„±
            key = f"{r['source']}_{r['chunk_id']}"
            sparse_norm = (r['score'] - sparse_min) / sparse_range

            if key in doc_scores:
                doc_scores[key]['sparse_score'] = r['score']
                doc_scores[key]['sparse_norm'] = sparse_norm
            else:
                doc_scores[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'chunk_id': r['chunk_id'],
                    'dense_score': 0,
                    'dense_norm': 0,
                    'sparse_score': r['score'],
                    'sparse_norm': sparse_norm
                }

        # Hybrid ì ìˆ˜ ê³„ì‚°
        results = []
        for key, data in doc_scores.items():
            hybrid_score = alpha * data['dense_norm'] + (1 - alpha) * data['sparse_norm']
            results.append({
                'content': data['content'],
                'source': data['source'],
                'chunk_id': data['chunk_id'],
                'hybrid_score': hybrid_score,
                'dense_score': data['dense_score'],
                'dense_norm': data['dense_norm'],
                'sparse_score': data['sparse_score'],
                'sparse_norm': data['sparse_norm'],
                'method': 'hybrid'
            })

        # ì •ë ¬
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)

        # Reranking ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if apply_reranking and self.reranker is not None:
            # ìƒìœ„ í›„ë³´ë“¤ë§Œ reranking
            candidates = results[:rerank_candidates]
            reranked = self.reranker.rerank(
                query=query,
                documents=candidates,
                top_k=k,
                content_key='content',
                show_progress=len(candidates) > 10
            )
            # reranked ê²°ê³¼ì— method ì—…ë°ì´íŠ¸
            for r in reranked:
                r['method'] = 'hybrid+rerank'
            # LangSmith íŠ¸ë ˆì´ì‹± ì¢…ë£Œ
            _tracer.log_output({"results_count": len(reranked), "method": "hybrid+rerank"})
            _tracer.__exit__(None, None, None)
            return reranked

        # LangSmith íŠ¸ë ˆì´ì‹± ì¢…ë£Œ
        _tracer.log_output({"results_count": len(results[:k]), "method": "hybrid"})
        _tracer.__exit__(None, None, None)
        return results[:k]

    def search_with_filter(
        self,
        query: str,
        source_filter: str = None,
        min_chunk_length: int = None,
        k: int = 5,
        search_type: str = 'hybrid'
    ) -> List[Dict]:
        """Payload í•„í„°ë§ì„ ì ìš©í•œ ê²€ìƒ‰"""
        from qdrant_client.models import Filter, FieldCondition, MatchValue, Range

        conditions = []

        if source_filter:
            conditions.append(
                FieldCondition(
                    key="source",
                    match=MatchValue(value=source_filter)
                )
            )

        if min_chunk_length:
            conditions.append(
                FieldCondition(
                    key="text_length",
                    range=Range(gte=min_chunk_length)
                )
            )

        query_filter = Filter(must=conditions) if conditions else None

        filter_dict = {}
        if source_filter:
            filter_dict['source'] = source_filter

        if search_type == 'dense':
            return self.dense_search(query, k, filter_conditions=filter_dict if filter_dict else None)
        elif search_type == 'sparse':
            return self.sparse_search(query, k, filter_conditions=filter_dict if filter_dict else None)
        else:
            return self.hybrid_search(query, k, filter_conditions=filter_dict if filter_dict else None)

    def get_collection_info(self) -> Dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        info = self.client.get_collection(self.collection_name)
        return {
            'name': self.collection_name,
            'status': str(info.status),
            'points_count': info.points_count,
            'vectors_count': info.vectors_count,
            'indexed_vectors_count': info.indexed_vectors_count
        }

    def get_sparse_method_name(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ sparse ë°©ì‹ ì´ë¦„ ë°˜í™˜"""
        return self.sparse_method.upper()

    def visualize_comparison(
        self,
        query: str,
        sparse_results: List[Dict],
        dense_results: List[Dict],
        hybrid_results: List[Dict],
        alpha: float = 0.7,
        sparse_method: str = 'BM25',
        dense_model: str = 'Dense',
        save_path: str = None,
        show_plot: bool = False
    ) -> str:
        """Qdrant ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™” (íŒŒì¼ ì €ì¥)"""
        import matplotlib
        matplotlib.use('Agg')  # ë¹„ëŒ€í™”í˜• ë°±ì—”ë“œ (ë¹ ë¥´ê³  ì•ˆì •ì )
        import matplotlib.pyplot as plt

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            plt.rcParams['font.family'] = 'AppleGothic'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(15, 11))
        fig.suptitle(f'ğŸ” Qdrant Hybrid Search Analysis\nQuery: "{query[:60]}..."',
                    fontsize=14, fontweight='bold')

        # 1. Sparse ì ìˆ˜ (ì™¼ìª½ ìƒë‹¨)
        ax1 = axes[0, 0]
        if sparse_results:
            sparse_labels = [f"Doc {i+1}\n{r['source'][:20]}..." for i, r in enumerate(sparse_results[:5])]
            sparse_scores = [r['score'] for r in sparse_results[:5]]
            colors1 = plt.cm.Blues([(0.4 + 0.12*i) for i in range(len(sparse_scores))])
            bars1 = ax1.barh(sparse_labels, sparse_scores, color=colors1, edgecolor='navy', alpha=0.85)
            ax1.set_xlabel(f'{sparse_method} Score', fontweight='bold')
            ax1.set_title(f'ğŸ”µ Sparse Search ({sparse_method})', fontweight='bold', fontsize=11)
            ax1.invert_yaxis()
            for bar, score in zip(bars1, sparse_scores):
                ax1.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,
                        f'{score:.3f}', va='center', fontsize=9, fontweight='bold')
            ax1.set_xlim(0, max(sparse_scores) * 1.3 if sparse_scores else 1)

        # 2. Dense ì ìˆ˜ (ì˜¤ë¥¸ìª½ ìƒë‹¨)
        ax2 = axes[0, 1]
        if dense_results:
            dense_labels = [f"Doc {i+1}\n{r['source'][:20]}..." for i, r in enumerate(dense_results[:5])]
            dense_scores = [r['score'] for r in dense_results[:5]]
            colors2 = plt.cm.Reds([(0.4 + 0.12*i) for i in range(len(dense_scores))])
            bars2 = ax2.barh(dense_labels, dense_scores, color=colors2, edgecolor='darkred', alpha=0.85)
            ax2.set_xlabel('Cosine Similarity (higher = better)', fontweight='bold')
            ax2.set_title(f'ğŸ”´ Dense Search (HNSW, {dense_model})', fontweight='bold', fontsize=11)
            ax2.invert_yaxis()
            for bar, score in zip(bars2, dense_scores):
                ax2.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                        f'{score:.4f}', va='center', fontsize=9, fontweight='bold')
            ax2.set_xlim(0, max(dense_scores) * 1.2 if dense_scores else 1)

        # 3. Hybrid ì ìˆ˜ ë¹„êµ (ì™¼ìª½ í•˜ë‹¨)
        ax3 = axes[1, 0]
        if hybrid_results:
            hybrid_labels = [f"Doc {i+1}" for i in range(len(hybrid_results[:5]))]
            x = range(len(hybrid_labels))
            width = 0.25

            sparse_norm = [r.get('sparse_norm', 0) for r in hybrid_results[:5]]
            dense_norm = [r.get('dense_norm', 0) for r in hybrid_results[:5]]
            hybrid_scores = [r['hybrid_score'] for r in hybrid_results[:5]]

            bars_sparse = ax3.bar([i - width for i in x], sparse_norm, width,
                                 label=f'{sparse_method} (norm)', color='#3498db', alpha=0.85, edgecolor='navy')
            bars_dense = ax3.bar(x, dense_norm, width,
                                label='Dense (norm)', color='#e74c3c', alpha=0.85, edgecolor='darkred')
            bars_hybrid = ax3.bar([i + width for i in x], hybrid_scores, width,
                                 label='Hybrid', color='#2ecc71', alpha=0.85, edgecolor='darkgreen')

            ax3.set_xlabel('Document', fontweight='bold')
            ax3.set_ylabel('Normalized Score (0-1)', fontweight='bold')
            ax3.set_title(f'ğŸŸ¢ Hybrid Score Fusion (Î±={alpha})\n{sparse_method}Ã—{1-alpha:.1f} + DenseÃ—{alpha:.1f}',
                         fontweight='bold', fontsize=11)
            ax3.set_xticks(x)
            ax3.set_xticklabels(hybrid_labels)
            ax3.set_ylim(0, 1.15)
            ax3.legend(loc='upper right', fontsize=9)
            ax3.grid(axis='y', alpha=0.3)

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ í‘œì‹œ
            for bar, score in zip(bars_hybrid, hybrid_scores):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                        f'{score:.2f}', ha='center', va='bottom', fontsize=8, fontweight='bold')

        # 4. ìƒì„¸ ê²°ê³¼ ìš”ì•½ (ì˜¤ë¥¸ìª½ í•˜ë‹¨)
        ax4 = axes[1, 1]
        ax4.axis('off')

        info_text = "ğŸ“Š Qdrant Search Results Summary\n" + "="*45 + "\n\n"

        info_text += f"ğŸ”µ Sparse ({sparse_method}) - Top 3:\n"
        for i, r in enumerate(sparse_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     Score: {r['score']:.4f}\n"

        info_text += f"\nğŸ”´ Dense (HNSW) - Top 3:\n"
        for i, r in enumerate(dense_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     Cosine: {r['score']:.4f}\n"

        info_text += f"\nğŸŸ¢ Hybrid (Î±={alpha}) - Top 3:\n"
        for i, r in enumerate(hybrid_results[:3], 1):
            source = r['source'][:35] + "..." if len(r['source']) > 35 else r['source']
            sparse_s = r.get('sparse_score', 0)
            dense_s = r.get('dense_score', 0)
            hybrid_s = r.get('hybrid_score', 0)
            info_text += f"  {i}. {source}\n"
            info_text += f"     Hybrid: {hybrid_s:.3f} (S:{sparse_s:.3f} D:{dense_s:.3f})\n"

        info_text += f"\nğŸ“ˆ Statistics:\n"
        info_text += f"  â€¢ Collection: {self.collection_name}\n"
        info_text += f"  â€¢ Total chunks: {len(self.chunks)}\n"
        info_text += f"  â€¢ Dense dim: {self.dense_dim}\n"

        ax4.text(0.02, 0.98, info_text, transform=ax4.transAxes, fontsize=9,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow',
                         alpha=0.9, edgecolor='orange'))

        plt.tight_layout()

        # ì €ì¥
        if save_path is None:
            save_path = f"qdrant_hybrid_search_{query[:20].replace(' ', '_')}.png"

        plt.savefig(save_path, dpi=100, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close(fig)  # ëª…ì‹œì ìœ¼ë¡œ figure ë‹«ê¸°

        print(f"   ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
        return save_path


# ==================== SPLADE Encoder ====================
class SPLADEEncoder:
    """SPLADE (Sparse Lexical and Expansion) ì¸ì½”ë”"""

    def __init__(self, model_name: str = "naver/splade-cocondenser-ensembledistil", device: str = 'cpu'):
        import torch
        from transformers import AutoTokenizer, AutoModelForMaskedLM

        self.device = device
        self.model_name = model_name

        print(f"   ğŸ“¥ SPLADE ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()
        self.torch = torch

    def encode(self, texts: List[str], max_length: int = 256) -> List[Dict[str, float]]:
        """í…ìŠ¤íŠ¸ë¥¼ SPLADE ìŠ¤íŒŒìŠ¤ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
        sparse_vectors = []

        for text in texts:
            # í† í°í™”
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                max_length=max_length,
                truncation=True,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # SPLADE ì¸ì½”ë”©
            with self.torch.no_grad():
                outputs = self.model(**inputs)
                # SPLADE: log(1 + ReLU(logits)) * attention_mask
                logits = outputs.logits
                relu_log = self.torch.log1p(self.torch.relu(logits))
                # Max pooling over sequence
                weighted = relu_log * inputs['attention_mask'].unsqueeze(-1)
                sparse_vec = self.torch.max(weighted, dim=1).values.squeeze()

            # 0ì´ ì•„ë‹Œ ê°’ë§Œ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
            sparse_dict = {}
            indices = self.torch.nonzero(sparse_vec).squeeze(-1)
            for idx in indices:
                idx = idx.item()
                token = self.tokenizer.decode([idx])
                weight = sparse_vec[idx].item()
                if weight > 0.1:  # ì„ê³„ê°’ ì´ìƒë§Œ ì €ì¥
                    sparse_dict[token] = weight

            sparse_vectors.append(sparse_dict)

        return sparse_vectors

    def compute_similarity(self, query_vec: Dict[str, float], doc_vec: Dict[str, float]) -> float:
        """ì¿¼ë¦¬ì™€ ë¬¸ì„œ ìŠ¤íŒŒìŠ¤ ë²¡í„°ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ë‚´ì )"""
        score = 0.0
        for token, weight in query_vec.items():
            if token in doc_vec:
                score += weight * doc_vec[token]
        return score


# ==================== Hybrid Search ì‹œìŠ¤í…œ ====================
class HybridSearchSystem:
    """Sparse (BM25/SPLADE) + Dense (Semantic) + Hybrid ê²€ìƒ‰ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        rag_system: RAGSystem,
        sparse_method: str = 'bm25',
        use_reranking: bool = False,
        reranker_model: str = 'ms-marco'
    ):
        """
        Args:
            rag_system: RAG ì‹œìŠ¤í…œ
            sparse_method: 'bm25' ë˜ëŠ” 'splade'
            use_reranking: Reranking ì‚¬ìš© ì—¬ë¶€
            reranker_model: Reranker ëª¨ë¸ íƒ€ì…
        """
        from rank_bm25 import BM25Okapi
        import numpy as np

        self.rag = rag_system
        self.chunks = rag_system.get_all_chunks()
        self.np = np
        self.sparse_method = sparse_method.lower()
        self.use_reranking = use_reranking
        self.reranker = None

        # Reranker ì´ˆê¸°í™” (ìš”ì²­ ì‹œ)
        if use_reranking:
            print("\nğŸ¯ Reranker ì´ˆê¸°í™”...")
            self.reranker = Reranker(model_type=reranker_model)

        # ìŠ¤í…Œë¨¸ ì´ˆê¸°í™” (BM25ìš©)
        self._init_stemmer()

        # Sparse ì¸ë±ìŠ¤ êµ¬ì¶•
        if self.sparse_method == 'splade':
            self._init_splade()
        else:
            self._init_bm25()

    def _init_bm25(self):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        from rank_bm25 import BM25Okapi

        print("\nğŸ” BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (ìŠ¤í…Œë° ì ìš©)...")
        tokenized_corpus = [self._tokenize(chunk['content']) for chunk in self.chunks]
        self.bm25 = BM25Okapi(tokenized_corpus)
        self.splade = None
        self.splade_vectors = None
        print(f"âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ({len(self.chunks)} ì²­í¬)")

    def _init_splade(self):
        """SPLADE ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("\nğŸ” SPLADE ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

        try:
            self.splade = SPLADEEncoder()
            self.bm25 = None

            # ëª¨ë“  ë¬¸ì„œ ì¸ì½”ë”©
            print(f"   ğŸ“„ {len(self.chunks)}ê°œ ë¬¸ì„œ ì¸ì½”ë”© ì¤‘...")
            doc_texts = [chunk['content'] for chunk in self.chunks]

            # ë°°ì¹˜ ì²˜ë¦¬
            batch_size = 8
            self.splade_vectors = []
            for i in range(0, len(doc_texts), batch_size):
                batch = doc_texts[i:i+batch_size]
                vectors = self.splade.encode(batch)
                self.splade_vectors.extend(vectors)
                print(f"   ì§„í–‰: {min(i+batch_size, len(doc_texts))}/{len(doc_texts)}", end='\r')

            print(f"\nâœ… SPLADE ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ({len(self.chunks)} ì²­í¬)")

        except Exception as e:
            print(f"âš ï¸ SPLADE ë¡œë“œ ì‹¤íŒ¨: {str(e)[:50]}")
            print("   BM25ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            self.sparse_method = 'bm25'
            self._init_bm25()

    def _init_stemmer(self):
        """ìŠ¤í…Œë¨¸ ì´ˆê¸°í™” - Porter Stemmer ì‚¬ìš©"""
        try:
            from nltk.stem import PorterStemmer
            self.stemmer = PorterStemmer()
            self.use_stemming = True
        except ImportError:
            self.stemmer = None
            self.use_stemming = False

    def _tokenize(self, text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì € + ìŠ¤í…Œë°"""
        import re
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)

        # ìŠ¤í…Œë° ì ìš© (diabete -> diabet, diabetes -> diabet)
        if self.use_stemming and self.stemmer:
            tokens = [self.stemmer.stem(token) for token in tokens]

        return tokens

    def sparse_search(self, query: str, k: int = 5) -> List[Dict]:
        """Sparse ê²€ìƒ‰ (BM25 ë˜ëŠ” SPLADE)"""
        if self.sparse_method == 'splade' and self.splade is not None:
            return self._splade_search(query, k)
        else:
            return self._bm25_search(query, k)

    def _bm25_search(self, query: str, k: int = 5) -> List[Dict]:
        """BM25 ê¸°ë°˜ ê²€ìƒ‰"""
        tokenized_query = self._tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = self.np.argsort(scores)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                'content': self.chunks[idx]['content'],
                'source': self.chunks[idx]['source'],
                'chunk_idx': int(idx),  # ê³ ìœ  ì²­í¬ ì¸ë±ìŠ¤ ì¶”ê°€
                'score': float(scores[idx]),
                'method': 'sparse (BM25)'
            })
        return results

    def _splade_search(self, query: str, k: int = 5) -> List[Dict]:
        """SPLADE ê¸°ë°˜ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì¸ì½”ë”©
        query_vec = self.splade.encode([query])[0]

        # ëª¨ë“  ë¬¸ì„œì™€ ìœ ì‚¬ë„ ê³„ì‚°
        scores = []
        for doc_vec in self.splade_vectors:
            score = self.splade.compute_similarity(query_vec, doc_vec)
            scores.append(score)

        scores = self.np.array(scores)

        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = self.np.argsort(scores)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                'content': self.chunks[idx]['content'],
                'source': self.chunks[idx]['source'],
                'chunk_idx': int(idx),  # ê³ ìœ  ì²­í¬ ì¸ë±ìŠ¤ ì¶”ê°€
                'score': float(scores[idx]),
                'method': 'sparse (SPLADE)'
            })
        return results

    def dense_search(self, query: str, k: int = 5) -> List[Dict]:
        """FAISS ê¸°ë°˜ Dense ê²€ìƒ‰ (ì˜ë¯¸ì  ìœ ì‚¬ë„)"""
        results = self.rag.search(query, k=k)

        # ì½˜í…ì¸  â†’ ì²­í¬ ì¸ë±ìŠ¤ ë§µí•‘ ìƒì„± (ë” ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´)
        content_to_idx = {}
        for idx, chunk in enumerate(self.chunks):
            content_key = chunk['content'][:100]  # ì²˜ìŒ 100ìë¡œ í‚¤ ìƒì„±
            if content_key not in content_to_idx:
                content_to_idx[content_key] = idx

        for r in results:
            r['method'] = 'dense'
            # ì²­í¬ ì¸ë±ìŠ¤ ì°¾ê¸°
            content_key = r['content'][:100]
            r['chunk_idx'] = content_to_idx.get(content_key, -1)

        return results

    def hybrid_search(
        self,
        query: str,
        k: int = 5,
        alpha: float = 0.5,
        rrf_k: int = 10,
        use_reranking: bool = None,
        rerank_top_k: int = None
    ) -> List[Dict]:
        """
        Hybrid ê²€ìƒ‰ (Sparse + Dense ê²°í•©) - RRF (Reciprocal Rank Fusion) ì‚¬ìš© + Reranking

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: 0.0 = ìˆœìˆ˜ Sparse, 1.0 = ìˆœìˆ˜ Dense
            rrf_k: RRF ìƒìˆ˜ (ê¸°ë³¸ê°’ 10)
            use_reranking: Reranking ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ self.use_reranking)
            rerank_top_k: Rerankingí•  í›„ë³´ ìˆ˜
        """
        # Reranking ì„¤ì •
        apply_reranking = use_reranking if use_reranking is not None else self.use_reranking
        rerank_candidates = rerank_top_k if rerank_top_k else max(k * 3, 20)

        # ì¶©ë¶„í•œ í›„ë³´ ê²€ìƒ‰
        num_candidates = rerank_candidates if apply_reranking else max(k * 3, 20)
        sparse_results = self.sparse_search(query, k=num_candidates)
        dense_results = self.dense_search(query, k=num_candidates)

        # BM25 ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’ ê³„ì‚°
        bm25_scores = [r['score'] for r in sparse_results]
        bm25_max = max(bm25_scores) if bm25_scores else 1.0
        bm25_min = min(bm25_scores) if bm25_scores else 0.0
        bm25_range = bm25_max - bm25_min if bm25_max > bm25_min else 1.0

        # Dense ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œê°’ ê³„ì‚° (L2 ê±°ë¦¬ - ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        dense_scores = [r['score'] for r in dense_results]
        dense_max = max(dense_scores) if dense_scores else 1.0
        dense_min = min(dense_scores) if dense_scores else 0.0
        dense_range = dense_max - dense_min if dense_max > dense_min else 1.0

        # ë¬¸ì„œ í†µí•©ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ (chunk_idxë¥¼ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš©)
        doc_data = {}

        # Sparse ê²°ê³¼ ì²˜ë¦¬ - ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ + BM25 ì •ê·œí™”
        for rank, r in enumerate(sparse_results):
            # chunk_idxë¥¼ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš© (ì—†ìœ¼ë©´ content[:100] ì‚¬ìš©)
            chunk_idx = r.get('chunk_idx', -1)
            key = f"chunk_{chunk_idx}" if chunk_idx >= 0 else r['content'][:100]
            sparse_rrf = 1.0 / (rrf_k + rank + 1)  # RRF ì ìˆ˜
            # BM25 ì ìˆ˜ë¥¼ 0-1ë¡œ ì •ê·œí™”
            bm25_norm = (r['score'] - bm25_min) / bm25_range if bm25_range > 0 else 1.0

            if key not in doc_data:
                doc_data[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'chunk_idx': chunk_idx,
                    'sparse_rank': rank + 1,
                    'sparse_score': r['score'],  # ì›ë³¸ BM25 ì ìˆ˜ (0~30+ ë²”ìœ„)
                    'sparse_score_norm': bm25_norm,  # ì •ê·œí™”ëœ BM25 (0-1)
                    'sparse_rrf': sparse_rrf,
                    'dense_rank': 0,
                    'dense_score': 0,
                    'dense_score_norm': 0,
                    'dense_rrf': 0
                }
            else:
                doc_data[key]['sparse_rank'] = rank + 1
                doc_data[key]['sparse_score'] = r['score']
                doc_data[key]['sparse_score_norm'] = bm25_norm
                doc_data[key]['sparse_rrf'] = sparse_rrf

        # Dense ê²°ê³¼ ì²˜ë¦¬ - ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ + ê±°ë¦¬ ì •ê·œí™”
        for rank, r in enumerate(dense_results):
            # chunk_idxë¥¼ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš© (ì—†ìœ¼ë©´ content[:100] ì‚¬ìš©)
            chunk_idx = r.get('chunk_idx', -1)
            key = f"chunk_{chunk_idx}" if chunk_idx >= 0 else r['content'][:100]
            dense_rrf = 1.0 / (rrf_k + rank + 1)  # RRF ì ìˆ˜
            # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (1 - ì •ê·œí™”ëœ ê±°ë¦¬)
            dense_norm = 1.0 - ((r['score'] - dense_min) / dense_range) if dense_range > 0 else 1.0

            if key not in doc_data:
                doc_data[key] = {
                    'content': r['content'],
                    'source': r['source'],
                    'chunk_idx': chunk_idx,
                    'sparse_rank': 0,
                    'sparse_score': 0,
                    'sparse_score_norm': 0,
                    'sparse_rrf': 0,
                    'dense_rank': rank + 1,
                    'dense_score': r['score'],  # ì›ë³¸ L2 ê±°ë¦¬
                    'dense_score_norm': dense_norm,  # ì •ê·œí™”ëœ ìœ ì‚¬ë„ (0-1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                    'dense_rrf': dense_rrf
                }
            else:
                doc_data[key]['dense_rank'] = rank + 1
                doc_data[key]['dense_score'] = r['score']
                doc_data[key]['dense_score_norm'] = dense_norm
                doc_data[key]['dense_rrf'] = dense_rrf

        # Hybrid ìŠ¤ì½”ì–´ ê³„ì‚° (ì •ê·œí™”ëœ ì ìˆ˜ ê¸°ë°˜)
        results = []
        for key, data in doc_data.items():
            # ë°©ë²• 1: RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ
            hybrid_rrf = (1 - alpha) * data['sparse_rrf'] + alpha * data['dense_rrf']

            # ë°©ë²• 2: ì •ê·œí™”ëœ ì ìˆ˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ (ë” ì§ê´€ì )
            hybrid_norm = (1 - alpha) * data['sparse_score_norm'] + alpha * data['dense_score_norm']

            results.append({
                'content': data['content'],
                'source': data['source'],
                # ì›ë³¸ ì ìˆ˜ë“¤
                'sparse_score': data['sparse_score'],      # ì›ë³¸ BM25 (0~30+)
                'dense_score': data['dense_score'],        # ì›ë³¸ L2 ê±°ë¦¬
                # ì •ê·œí™”ëœ ì ìˆ˜ë“¤ (0-1)
                'sparse_score_norm': data['sparse_score_norm'],  # BM25 ì •ê·œí™”
                'dense_score_norm': data['dense_score_norm'],    # ìœ ì‚¬ë„ ì •ê·œí™”
                # ìˆœìœ„ ì •ë³´
                'sparse_rank': data['sparse_rank'],
                'dense_rank': data['dense_rank'],
                # RRF ì ìˆ˜
                'sparse_rrf': data['sparse_rrf'],
                'dense_rrf': data['dense_rrf'],
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
                'hybrid_score': hybrid_norm,        # ì •ê·œí™” ê¸°ë°˜ (0-1)
                'hybrid_score_rrf': hybrid_rrf,     # RRF ê¸°ë°˜
                'method': 'hybrid'
            })

        # Hybrid ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)

        # Reranking ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if apply_reranking and self.reranker is not None:
            candidates = results[:rerank_candidates]
            reranked = self.reranker.rerank(
                query=query,
                documents=candidates,
                top_k=k,
                content_key='content',
                show_progress=len(candidates) > 10
            )
            for r in reranked:
                r['method'] = 'hybrid+rerank'
            return reranked

        return results[:k]

    def compare_all(self, query: str, k: int = 5, alpha: float = 0.5) -> Dict:
        """ì„¸ ê°€ì§€ ê²€ìƒ‰ ë°©ë²• ë¹„êµ"""
        sparse = self.sparse_search(query, k)
        dense = self.dense_search(query, k)
        hybrid = self.hybrid_search(query, k, alpha)

        return {
            'query': query,
            'sparse': sparse,
            'dense': dense,
            'hybrid': hybrid
        }

    def visualize_comparison(self, query: str, k: int = 5, alpha: float = 0.5,
                            save_path: str = None) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™” (íŒŒì¼ ì €ì¥)"""
        import matplotlib
        matplotlib.use('Agg')  # ë¹„ëŒ€í™”í˜• ë°±ì—”ë“œ (ë¹ ë¥´ê³  ì•ˆì •ì )
        import matplotlib.pyplot as plt

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            plt.rcParams['font.family'] = 'AppleGothic'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False

        results = self.compare_all(query, k, alpha)

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'Hybrid Search Comparison\nQuery: "{query[:50]}..."', fontsize=14, fontweight='bold')

        # 1. Sparse (BM25) ì ìˆ˜
        ax1 = axes[0, 0]
        sparse_labels = [f"Doc {i+1}" for i in range(len(results['sparse']))]
        sparse_scores = [r['score'] for r in results['sparse']]
        bars1 = ax1.barh(sparse_labels, sparse_scores, color='#3498db', alpha=0.8)
        ax1.set_xlabel('BM25 Score')
        ax1.set_title('Sparse Search (BM25)', fontweight='bold')
        ax1.invert_yaxis()
        for bar, score in zip(bars1, sparse_scores):
            ax1.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                    f'{score:.2f}', va='center', fontsize=9)

        # 2. Dense (Semantic) ì ìˆ˜
        ax2 = axes[0, 1]
        dense_labels = [f"Doc {i+1}" for i in range(len(results['dense']))]
        dense_scores = [r['score'] for r in results['dense']]
        bars2 = ax2.barh(dense_labels, dense_scores, color='#e74c3c', alpha=0.8)
        ax2.set_xlabel('L2 Distance (lower = better)')
        ax2.set_title('Dense Search (Semantic)', fontweight='bold')
        ax2.invert_yaxis()
        for bar, score in zip(bars2, dense_scores):
            ax2.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                    f'{score:.3f}', va='center', fontsize=9)

        # 3. Hybrid ì •ê·œí™” ì ìˆ˜ ë¹„êµ
        ax3 = axes[1, 0]
        hybrid_labels = [f"Doc {i+1}" for i in range(len(results['hybrid']))]
        x = range(len(hybrid_labels))
        width = 0.25

        # ì •ê·œí™”ëœ ì ìˆ˜ ì‚¬ìš© (0-1 ë²”ìœ„)
        sparse_norm = [r.get('sparse_score_norm', 0) for r in results['hybrid']]
        dense_norm = [r.get('dense_score_norm', 0) for r in results['hybrid']]
        hybrid_scores = [r['hybrid_score'] for r in results['hybrid']]

        ax3.bar([i - width for i in x], sparse_norm, width, label='BM25 (norm)', color='#3498db', alpha=0.8)
        ax3.bar(x, dense_norm, width, label='Semantic (norm)', color='#e74c3c', alpha=0.8)
        ax3.bar([i + width for i in x], hybrid_scores, width, label='Hybrid', color='#2ecc71', alpha=0.8)
        ax3.set_xlabel('Document')
        ax3.set_ylabel('Normalized Score (0-1)')
        ax3.set_title(f'Hybrid Search - Score Fusion (Î±={alpha})', fontweight='bold')
        ax3.set_xticks(x)
        ax3.set_xticklabels(hybrid_labels)
        ax3.set_ylim(0, 1.1)  # 0-1 ë²”ìœ„ ëª…í™•íˆ í‘œì‹œ
        ax3.legend()

        # 4. ë¬¸ì„œ ì¶œì²˜ ì •ë³´ (ìˆœìœ„ í¬í•¨)
        ax4 = axes[1, 1]
        ax4.axis('off')

        info_text = "ğŸ“Š Search Results Summary\n" + "="*40 + "\n\n"

        info_text += "ğŸ”µ Sparse (BM25) - Top Results:\n"
        for i, r in enumerate(results['sparse'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     BM25: {r['score']:.2f}\n"

        info_text += "\nğŸ”´ Dense (Semantic) - Top Results:\n"
        for i, r in enumerate(results['dense'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            info_text += f"  {i}. {source}\n"
            info_text += f"     L2 Dist: {r['score']:.4f}\n"

        info_text += "\nğŸŸ¢ Hybrid - Top Results:\n"
        for i, r in enumerate(results['hybrid'][:3], 1):
            source = r['source'][:30] + "..." if len(r['source']) > 30 else r['source']
            s_rank = r.get('sparse_rank', 0)
            d_rank = r.get('dense_rank', 0)
            bm25 = r.get('sparse_score', 0)
            s_norm = r.get('sparse_score_norm', 0)
            d_norm = r.get('dense_score_norm', 0)
            info_text += f"  {i}. {source}\n"
            info_text += f"     Hybrid: {r['hybrid_score']:.2f}\n"
            info_text += f"     BM25={bm25:.1f}({s_norm:.2f}) Sem={d_norm:.2f}\n"

        ax4.text(0.05, 0.95, info_text, transform=ax4.transAxes, fontsize=9,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.tight_layout()

        # ì €ì¥
        if save_path is None:
            save_path = f"./hybrid_search_comparison.png"

        plt.savefig(save_path, dpi=100, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close(fig)  # ëª…ì‹œì ìœ¼ë¡œ figure ë‹«ê¸°

        print(f"   ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
        return results


# ==================== ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ====================
def interactive_qa(rag: RAGSystem, openai_api_key: str = None, config=None):
    """ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ ëª¨ë“œ (ê²€ìƒ‰ë¶„ì•¼/ê²€ìƒ‰ë‚´ìš© ì„ íƒ ê°€ëŠ¥)"""

    # ì–¸ì–´ë³„ ë©”ì‹œì§€
    if rag.language == 'ko':
        print("\n" + "=" * 60)
        print("ğŸ’¬ RAG ì§ˆì˜ì‘ë‹µ ëª¨ë“œ")
        print("=" * 60)
        print("ğŸ“‹ ëª…ë ¹ì–´:")
        print("   â€¢ ê²€ìƒ‰ë‚´ìš© ì…ë ¥: VectorDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰")
        print("   â€¢ '/ë¶„ì•¼' ë˜ëŠ” '/field': ìƒˆë¡œìš´ ê²€ìƒ‰ë¶„ì•¼ ì¶”ê°€")
        print("   â€¢ '/info': í˜„ì¬ VectorDB ì •ë³´ í™•ì¸")
        print("   â€¢ 'quit', 'exit', 'q': ì¢…ë£Œ")
        prompt_text = "ğŸ” ê²€ìƒ‰ë‚´ìš©: "
        exit_msg = "ğŸ‘‹ ì§ˆì˜ì‘ë‹µì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
    else:
        print("\n" + "=" * 60)
        print("ğŸ’¬ RAG Q&A Mode")
        print("=" * 60)
        print("ğŸ“‹ Commands:")
        print("   â€¢ Enter query: Search from VectorDB")
        print("   â€¢ '/field': Add new search field (papers)")
        print("   â€¢ '/info': Show VectorDB info")
        print("   â€¢ 'quit', 'exit', 'q': Exit")
        prompt_text = "ğŸ” Query: "
        exit_msg = "ğŸ‘‹ Exiting Q&A mode."

    print("-" * 60)

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìˆëŠ” ê²½ìš°)
    client = None
    if openai_api_key:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai_api_key)
        except:
            pass

    while True:
        try:
            question = input(f"\n{prompt_text}").strip()

            if not question:
                continue

            if question.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ', 'ë']:
                print(f"\n{exit_msg}")
                break

            # /info ëª…ë ¹ì–´: VectorDB ì •ë³´ í™•ì¸
            if question.lower() in ['/info', '/ì •ë³´']:
                chunks = rag.get_all_chunks()
                print("\n" + "=" * 50)
                print("ğŸ“Š í˜„ì¬ VectorDB ì •ë³´")
                print("=" * 50)
                print(f"   â€¢ ì €ì¥ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")
                if chunks:
                    sources = list(set([c.get('source', 'Unknown') for c in chunks]))
                    print(f"   â€¢ ë…¼ë¬¸ ìˆ˜: {len(sources)}ê°œ")
                    print(f"   â€¢ ì¶œì²˜ ëª©ë¡:")
                    for src in sources[:10]:
                        print(f"      - {src[:60]}...")
                    if len(sources) > 10:
                        print(f"      ... ì™¸ {len(sources) - 10}ê°œ")
                print("=" * 50)
                continue

            # /ë¶„ì•¼ ë˜ëŠ” /field ëª…ë ¹ì–´: ìƒˆë¡œìš´ ê²€ìƒ‰ë¶„ì•¼ ì¶”ê°€
            if question.lower() in ['/ë¶„ì•¼', '/field', '/ê²€ìƒ‰ë¶„ì•¼']:
                print("\n" + "=" * 50)
                print("ğŸ“š ìƒˆë¡œìš´ ê²€ìƒ‰ë¶„ì•¼ ì¶”ê°€")
                print("=" * 50)

                new_field = input("ğŸ” ê²€ìƒ‰ë¶„ì•¼ (ë…¼ë¬¸ ê²€ìƒ‰ í‚¤ì›Œë“œ): ").strip()
                if not new_field:
                    print("   âš ï¸ ê²€ìƒ‰ë¶„ì•¼ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    continue

                max_papers = input("ğŸ“„ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜ [5]: ").strip()
                max_papers = int(max_papers) if max_papers.isdigit() else 5

                # ê²€ìƒ‰ë¶„ì•¼ ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
                field_lang = detect_language(new_field)
                search_field = new_field
                if field_lang == 'ko':
                    search_field = translate_to_english(new_field, openai_api_key)
                    print(f"   ğŸ”„ ë²ˆì—­: '{new_field}' â†’ '{search_field}'")

                # ë…¼ë¬¸ ê²€ìƒ‰
                print(f"\nğŸ” '{search_field}' ê²€ìƒ‰ ì¤‘...")
                searcher = PaperSearcher(
                    api_key=os.getenv('PUBMED_API_KEY'),
                    email=os.getenv('PUBMED_EMAIL')
                )
                new_papers = searcher.search(
                    query=search_field,
                    source='pubmed',
                    max_results=max_papers
                )

                if not new_papers:
                    print("   âŒ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                print(f"   âœ… {len(new_papers)}ê°œ ë…¼ë¬¸ ê²€ìƒ‰ë¨")

                # ê¸°ì¡´ VectorDBì— ì¶”ê°€
                print("\nğŸ’¾ VectorDBì— ì¶”ê°€ ì¤‘...")
                rag.build_vectorstore_from_abstracts(new_papers, append=True)

                new_chunks = rag.get_all_chunks()
                print(f"   âœ… ì™„ë£Œ! í˜„ì¬ ì´ {len(new_chunks)}ê°œ ì²­í¬")
                print("=" * 50)
                continue

            # ê²°ê³¼ ê°œìˆ˜ ì¡°ì ˆ
            k = 3
            if 'k=' in question:
                try:
                    k_part = question.split('k=')[1].split()[0]
                    k = int(k_part)
                    question = question.replace(f'k={k_part}', '').strip()
                except:
                    pass

            # ì§ˆë¬¸ ì–¸ì–´ ê°ì§€
            q_lang = detect_language(question)

            # í•œêµ­ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ê²€ìƒ‰
            search_question = question
            if q_lang == 'ko':
                search_question = translate_to_english(question, openai_api_key)
                if search_question != question:
                    print(f"   ğŸ”„ ê²€ìƒ‰ì–´: '{search_question}'")

            result = rag.answer(search_question, k=k)
            result['language'] = q_lang  # ì›ë³¸ ì§ˆë¬¸ì˜ ì–¸ì–´ ìœ ì§€

            # ì–¸ì–´ë³„ ì¶œë ¥
            if q_lang == 'ko':
                print(f"\nğŸ“š ê´€ë ¨ ë¬¸ì„œ {len(result['contexts'])}ê°œ ê²€ìƒ‰ë¨:\n")
            else:
                print(f"\nğŸ“š Found {len(result['contexts'])} relevant documents:\n")

            for i, ctx in enumerate(result['contexts'], 1):
                similarity = 1 / (1 + ctx['score'])

                if q_lang == 'ko':
                    print(f"[{i}] ì¶œì²˜: {ctx['source'][:50]}")
                    print(f"    ìœ ì‚¬ë„: {similarity:.2%}")
                else:
                    print(f"[{i}] Source: {ctx['source'][:50]}")
                    print(f"    Similarity: {similarity:.2%}")

                content_preview = ctx['content'].replace('\n', ' ')[:300]
                print(f"    {content_preview}...")
                print("-" * 40)

            # OpenAIë¡œ ë‹µë³€ ìƒì„± (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            if client:
                try:
                    context_text = "\n\n".join([ctx['content'] for ctx in result['contexts']])

                    if q_lang == 'ko':
                        system_msg = "ë‹¹ì‹ ì€ ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                    else:
                        system_msg = "You are an expert answering questions based on medical/scientific papers. Answer based on the provided context."

                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": f"Context:\n{context_text}\n\nQuestion: {question}"}
                        ],
                        max_tokens=500,
                        temperature=0.3
                    )

                    answer = response.choices[0].message.content

                    if q_lang == 'ko':
                        print(f"\nğŸ¤– AI ë‹µë³€:\n{answer}")
                    else:
                        print(f"\nğŸ¤– AI Answer:\n{answer}")

                except Exception as e:
                    pass

            if q_lang == 'ko':
                print(f"\nğŸ“– ì°¸ê³  ë¬¸ì„œ: {', '.join(result['sources'])}")
            else:
                print(f"\nğŸ“– References: {', '.join(result['sources'])}")

        except KeyboardInterrupt:
            print(f"\n\n{exit_msg}")
            break
        except Exception as e:
            print(f"âŒ Error: {str(e)}")


# ==================== Agent ëª¨ë“œ ì‹¤í–‰ ====================
def run_agent_mode():
    """LangChain Agent ëª¨ë“œ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("ğŸ¤– LangChain Agent ëª¨ë“œ")
    print("=" * 60)

    # .envì—ì„œ API í‚¤ ë¡œë“œ
    openai_api_key = os.getenv('OPENAI_API_KEY')
    langchain_api_key = os.getenv('LANGCHAIN_API_KEY')

    if not openai_api_key:
        print("\nâŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    print("   âœ… OpenAI API ì—°ê²°ë¨")

    # LangSmith ì„¤ì •
    if langchain_api_key:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_PROJECT"] = "medical-paper-rag-agent"
        os.environ["LANGCHAIN_API_KEY"] = langchain_api_key
        print("   âœ… LangSmith ì¶”ì  í™œì„±í™”")
        print(f"   ğŸ“Š ëŒ€ì‹œë³´ë“œ: https://smith.langchain.com")

    # RAG ì‹œìŠ¤í…œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_rag = input("\nğŸ“š RAG ì‹œìŠ¤í…œ ì—°ë™ (ë…¼ë¬¸ ê²€ìƒ‰ ê¸°ëŠ¥)? [y/N]: ").strip().lower()

    rag_system = None
    if use_rag in ['y', 'yes']:
        print("\nâš™ï¸ RAG ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...")
        config = Config()
        config.openai_api_key = openai_api_key

        # ê°„ë‹¨í•œ ì„¤ì •
        config.search_query = input("ğŸ” ê²€ìƒ‰í•  ë…¼ë¬¸ í‚¤ì›Œë“œ: ").strip() or "medical research"
        config.max_results = 3
        config.embedding_model = 'minilm'  # ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©

        # ë…¼ë¬¸ ê²€ìƒ‰ ë° RAG êµ¬ì¶•
        searcher = PaperSearcher(
            api_key=os.getenv('PUBMED_API_KEY'),
            email=os.getenv('PUBMED_EMAIL')
        )
        papers = searcher.search(config.search_query, 'pubmed', config.max_results)

        if papers:
            print(f"   ğŸ“„ {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")

            # ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            downloader = PDFDownloader(PAPERS_DIR)
            downloaded = downloader.download_all(papers)

            if downloaded:
                documents = TextExtractor.extract_all(downloaded)

                if documents:
                    embeddings = EmbeddingModelFactory.create('minilm')
                    rag_system = RAGSystem(embeddings)
                    rag_system.build_vectorstore(documents)
                    print("   âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")

    # Agent ìƒì„± ë° ì‹¤í–‰
    print("\nğŸš€ Agent ì´ˆê¸°í™” ì¤‘...")
    agent = RAGAgent(openai_api_key=openai_api_key, rag_system=rag_system)

    if agent.create_agent():
        agent.chat()
    else:
        print("\nâŒ Agent ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("   í•„ìš” íŒ¨í‚¤ì§€: pip install langchain langchain-openai langchainhub")


# ==================== LangGraph ëª¨ë“œ ì‹¤í–‰ ====================
def run_langgraph_mode():
    """LangGraph RAG ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("ğŸ”„ LangGraph RAG ì›Œí¬í”Œë¡œìš° ëª¨ë“œ")
    print("=" * 60)

    # .envì—ì„œ API í‚¤ ë¡œë“œ
    openai_api_key = os.getenv('OPENAI_API_KEY')
    langchain_api_key = os.getenv('LANGCHAIN_API_KEY')

    if not openai_api_key:
        print("\nâŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    print("   âœ… OpenAI API ì—°ê²°ë¨")

    # LangSmith ì„¤ì •
    if langchain_api_key:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_PROJECT"] = "langgraph-rag-workflow"
        os.environ["LANGCHAIN_API_KEY"] = langchain_api_key
        print("   âœ… LangSmith ì¶”ì  í™œì„±í™”")
        print(f"   ğŸ“Š ëŒ€ì‹œë³´ë“œ: https://smith.langchain.com")

    # RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    print("\nâš™ï¸ RAG ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...")

    search_query = input("ğŸ” ê²€ìƒ‰í•  ë…¼ë¬¸ í‚¤ì›Œë“œ: ").strip() or "medical research"
    max_results = 5

    # ì–¸ì–´ ê°ì§€
    language = detect_language(search_query)
    if language == 'ko':
        search_query_en = translate_to_english(search_query, openai_api_key)
        print(f"   ğŸ‡ºğŸ‡¸ ë²ˆì—­: {search_query_en}")
    else:
        search_query_en = search_query

    # ë…¼ë¬¸ ê²€ìƒ‰
    print("\nğŸ“š ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...")
    searcher = PaperSearcher(
        api_key=os.getenv('PUBMED_API_KEY'),
        email=os.getenv('PUBMED_EMAIL')
    )
    papers = searcher.search(search_query_en, 'pubmed', max_results)
    print(f"   ğŸ“„ {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")

    if not papers:
        print("âŒ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    print("\nğŸ“¥ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    downloader = PDFDownloader(PAPERS_DIR)
    downloaded = downloader.download_all(papers)

    if not downloaded:
        print("âŒ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("\nğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    documents = TextExtractor.extract_all(downloaded)

    if not documents:
        print("âŒ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì„ë² ë”© ë° RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    print("\nğŸ§  RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...")
    embeddings = EmbeddingModelFactory.create('minilm')
    rag_system = RAGSystem(embeddings, language=language)
    rag_system.build_vectorstore(documents)

    # LangGraph RAG ì›Œí¬í”Œë¡œìš° ìƒì„±
    print("\nğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ì¤‘...")
    langgraph_rag = LangGraphRAG(
        openai_api_key=openai_api_key,
        rag_system=rag_system,
        language=language
    )

    if langgraph_rag.build_graph():
        # ê·¸ë˜í”„ ì‹œê°í™” ë° ì €ì¥
        langgraph_rag.visualize_graph()
        langgraph_rag.save_graph()

        # ëŒ€í™” ëª¨ë“œ
        langgraph_rag.chat()
    else:
        print("\nâŒ LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("   í•„ìš” íŒ¨í‚¤ì§€: pip install langgraph langchain-openai")


# ==================== íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ ====================
def run_trend_analysis():
    """íŠ¸ë Œë“œ ë¶„ì„ ëª¨ë“œ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„")
    print("=" * 60)

    # .envì—ì„œ API í‚¤ ë¡œë“œ
    pubmed_api_key = os.getenv('PUBMED_API_KEY')
    pubmed_email = os.getenv('PUBMED_EMAIL')
    openai_api_key = os.getenv('OPENAI_API_KEY')

    if openai_api_key:
        print("   âœ… OpenAI API ì—°ê²°ë¨ - AI ìš”ì•½ ê¸°ëŠ¥ í™œì„±í™”")
    else:
        print("   âš ï¸ OpenAI API ë¯¸ì„¤ì • - ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©")

    # ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)
    print("\nğŸ“– ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ (ë³µìˆ˜ ì„ íƒ: ì½¤ë§ˆë¡œ êµ¬ë¶„, ì˜ˆ: 1,2,3):")
    print("   1. PubMed (ì˜í•™/ìƒë¬¼í•™, NCBI) [ê¸°ë³¸ê°’]")
    print("   2. arXiv (CS/ë¬¼ë¦¬/ìˆ˜í•™, í”„ë¦¬í”„ë¦°íŠ¸)")
    print("   3. Europe PMC (ìœ ëŸ½ ì˜í•™/ìƒëª…ê³¼í•™, Open Access)")
    print("   4. ì „ì²´ (PubMed + arXiv + Europe PMC)")
    source_choice = input("ì„ íƒ [1]: ").strip() or "1"

    # ë³µìˆ˜ ì„ íƒ ì²˜ë¦¬
    source_map = {'1': 'pubmed', '2': 'arxiv', '3': 'europepmc', '4': 'all'}
    if ',' in source_choice:
        selected = []
        for c in source_choice.split(','):
            c = c.strip()
            if c in source_map:
                selected.append(source_map[c])
        source = ','.join(selected) if selected else 'pubmed'
    else:
        source = source_map.get(source_choice, 'pubmed')

    # í‚¤ì›Œë“œ ì…ë ¥
    keyword = input("\nğŸ” ë¶„ì„í•  í‚¤ì›Œë“œ ì…ë ¥: ").strip()
    if not keyword:
        keyword = "diabetes treatment"
        print(f"   ê¸°ë³¸ê°’ ì‚¬ìš©: {keyword}")

    # ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
    if detect_language(keyword) == 'ko':
        openai_api_key = os.getenv('OPENAI_API_KEY')
        print("\nğŸ”„ í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘...")
        keyword_en = translate_to_english(keyword, openai_api_key)
        print(f"   ğŸ‡°ğŸ‡· ì›ë³¸: {keyword}")
        print(f"   ğŸ‡ºğŸ‡¸ ë²ˆì—­: {keyword_en}")
        keyword = keyword_en

    # ë…¼ë¬¸ ìˆ˜ ì„¤ì •
    max_results = input("\nğŸ“„ ë¶„ì„í•  ìµœëŒ€ ë…¼ë¬¸ ìˆ˜ [50]: ").strip()
    max_results = int(max_results) if max_results.isdigit() else 50

    print("\n" + "-" * 60)
    print("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì„¤ì • ì™„ë£Œ!")
    print(f"   ğŸ” í‚¤ì›Œë“œ: {keyword}")
    print(f"   ğŸ“– ì†ŒìŠ¤: {source}")
    print(f"   ğŸ“„ ìµœëŒ€ ë…¼ë¬¸: {max_results}")
    print("-" * 60)

    # íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰
    analyzer = TrendAnalyzer(api_key=pubmed_api_key, email=pubmed_email, openai_api_key=openai_api_key)
    analyzer.analyze(keyword, max_results=max_results, source=source)

    # ì¶”ê°€ ë¶„ì„ ì—¬ë¶€
    while True:
        another = input("\nğŸ”„ ë‹¤ë¥¸ í‚¤ì›Œë“œ ë¶„ì„? (y/n) [n]: ").strip().lower()
        if another == 'y':
            keyword = input("ğŸ” ìƒˆ í‚¤ì›Œë“œ ì…ë ¥: ").strip()
            if keyword:
                if detect_language(keyword) == 'ko':
                    keyword = translate_to_english(keyword, os.getenv('OPENAI_API_KEY'))
                    print(f"   ğŸ‡ºğŸ‡¸ ë²ˆì—­: {keyword}")
                analyzer.analyze(keyword, max_results=max_results, source=source)
        else:
            break

    print("\n" + "=" * 60)
    print("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì¢…ë£Œ!")
    print("=" * 60)


# ==================== ë©”ì¸ ì‹¤í–‰ ====================
def main():
    print("=" * 60)
    print("ğŸš€ Medical/Scientific Paper RAG System")
    print("   with Paper Summarization & Multilingual Support")
    print("=" * 60)

    # ë©”ì¸ ë©”ë‰´
    print("\nğŸ“‹ ê¸°ëŠ¥ ì„ íƒ:")
    print("   1. RAG ì‹œìŠ¤í…œ (ë…¼ë¬¸ ê²€ìƒ‰ + ì§ˆì˜ì‘ë‹µ)")
    print("   2. íŠ¸ë Œë“œ ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜ ì—°êµ¬ ë™í–¥)")
    print("   3. Agent ëª¨ë“œ (LangChain Agent ëŒ€í™”)")
    print("   4. LangGraph ëª¨ë“œ (ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ RAG)")
    mode = input("ì„ íƒ [1]: ").strip() or "1"

    if mode == "2":
        # íŠ¸ë Œë“œ ë¶„ì„ ëª¨ë“œ
        run_trend_analysis()
        return

    if mode == "3":
        # Agent ëª¨ë“œ
        run_agent_mode()
        return

    if mode == "4":
        # LangGraph ëª¨ë“œ
        run_langgraph_mode()
        return

    # 1. ëŒ€í™”í˜• ì„¤ì •
    config = Config().interactive_setup()

    # LangSmith ì„¤ì • (í™œì„±í™”ëœ ê²½ìš°)
    if config.use_langsmith:
        setup_langsmith(config)

    # ê²€ìƒ‰ ëª¨ë“œ 2: VectorDBì—ì„œë§Œ ê²€ìƒ‰ (ë…¼ë¬¸ ê²€ìƒ‰ ìƒëµ)
    if config.search_mode == 2:
        print("\n" + "=" * 60)
        print("ğŸ” VectorDB ê²€ìƒ‰ ëª¨ë“œ - ê¸°ì¡´ ë°ì´í„°ì—ì„œ ê²€ìƒ‰")
        print("=" * 60)

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print("\nğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        embeddings = EmbeddingModelFactory.create(
            model_type=config.embedding_model,
            device='cpu',
            openai_api_key=config.openai_api_key
        )

        # RAG ì‹œìŠ¤í…œ ìƒì„± ë° VectorDB ë¡œë“œ
        rag = RAGSystem(
            embeddings=embeddings,
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            language=config.language
        )

        if not rag.load_vectorstore():
            print("\nâŒ ì €ì¥ëœ VectorDBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ì—¬ VectorDBë¥¼ ìƒì„±í•˜ì„¸ìš”.")
            print("   (ê²€ìƒ‰ ë°©ì‹ 1 ë˜ëŠ” 3ì„ ì‚¬ìš©í•˜ì„¸ìš”)")
            return

        # ë°”ë¡œ ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µìœ¼ë¡œ ì´ë™
        interactive_qa(rag, config.openai_api_key)
        return

    # ê²€ìƒ‰ ëª¨ë“œ 1, 3: ë…¼ë¬¸ ê²€ìƒ‰ ì§„í–‰
    # 2. ë…¼ë¬¸ ê²€ìƒ‰ (ì˜ì–´ë¡œ ê²€ìƒ‰)
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: ë…¼ë¬¸ ê²€ìƒ‰")
    print("=" * 60)

    # í•œêµ­ì–´ì¸ ê²½ìš° ë²ˆì—­ëœ ì˜ì–´ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    search_query = config.search_query_en if config.search_query_en else config.search_query
    if config.language == 'ko' and config.search_query_en != config.search_query:
        print(f"   ğŸ‡°ğŸ‡· â†’ ğŸ‡ºğŸ‡¸ '{search_query}' (ìœ¼)ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤...")

    searcher = PaperSearcher(
        api_key=config.pubmed_api_key,
        email=config.pubmed_email
    )
    papers = searcher.search(
        query=search_query,
        source=config.search_source,
        max_results=config.max_results
    )

    print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ì´ {len(papers)}ê°œ ë…¼ë¬¸")

    if papers:
        print("\nğŸ“‹ ê²€ìƒ‰ëœ ë…¼ë¬¸ ëª©ë¡:\n")
        for i, paper in enumerate(papers, 1):
            print(f"[{i}] {paper['source']} | {paper['title'][:65]}...")
            print(f"    ì €ì: {', '.join(paper['authors'][:3])}")
            print(f"    ë°œí–‰: {paper['published']}")
            print()

    if not papers:
        print("âŒ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # Abstract ëª¨ë“œ ì„ íƒ (PDF ë‹¤ìš´ë¡œë“œ ìƒëµ)
    print("\nğŸ“Œ ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ:")
    print("   1. Abstractë§Œ ì‚¬ìš© (ë¹ ë¦„, PDF ë‹¤ìš´ë¡œë“œ ìƒëµ)")
    print("   2. Full PDF ì‚¬ìš© (ëŠë¦¼, PDF ë‹¤ìš´ë¡œë“œ í•„ìš”)")
    use_abstract_only = input("\nì„ íƒ [1/2, ê¸°ë³¸ê°’=1]: ").strip()
    use_abstract_only = use_abstract_only != '2'  # 1 ë˜ëŠ” ë¹ˆê°’ì´ë©´ True

    if use_abstract_only:
        # Abstractë§Œ ì‚¬ìš©í•˜ëŠ” ë¹ ë¥¸ ëª¨ë“œ
        print("\n" + "=" * 60)
        print("ğŸš€ Abstract ëª¨ë“œ ì„ íƒ - PDF ë‹¤ìš´ë¡œë“œ ìƒëµ")
        print("=" * 60)

        # Abstract ìš”ì•½ (OpenAI APIê°€ ìˆëŠ” ê²½ìš°)
        if config.openai_api_key:
            print("\n" + "=" * 60)
            lang_name = "í•œêµ­ì–´" if config.summary_language == 'ko' else "English"
            print(f"ğŸ“ Step 2: Abstract AI ìš”ì•½ ({lang_name})")
            print("=" * 60)

            summarizer = PaperSummarizer(
                api_key=config.openai_api_key,
                language=config.language,
                summary_language=config.summary_language
            )
            # Abstract ëª¨ë“œì´ë¯€ë¡œ documents ì—†ì´ í˜¸ì¶œ (ëª¨ë“  ë…¼ë¬¸ì´ abstract_onlyë¡œ ì²˜ë¦¬ë¨)
            papers = summarizer.summarize(papers, [])

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print("\n" + "=" * 60)
        step_num = "3" if config.openai_api_key else "2"
        print(f"ğŸ§  Step {step_num}: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ")
        print("=" * 60)

        embeddings = EmbeddingModelFactory.create(
            model_type=config.embedding_model,
            device='cpu',
            openai_api_key=config.openai_api_key
        )

        # RAG ì‹œìŠ¤í…œ ìƒì„± ë° Abstractì—ì„œ ë²¡í„° DB êµ¬ì¶•
        print("\n" + "=" * 60)
        rag_step_num = "4" if config.openai_api_key else "3"
        print(f"ğŸ’¾ Step {rag_step_num}: Abstract ê¸°ë°˜ RAG ì‹œìŠ¤í…œ êµ¬ì¶•")
        print("=" * 60)

        rag = RAGSystem(
            embeddings=embeddings,
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            language=config.language
        )
        rag.build_vectorstore_from_abstracts(papers)

        # Qdrantë„ ì§€ì› (ì„ íƒëœ ê²½ìš°)
        qdrant_search = None
        if config.vector_db == 'qdrant':
            print(f"\nğŸ—„ï¸ Qdrant Vector DBë„ êµ¬ì¶• ì¤‘...")

            # Abstractë¥¼ documents í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            documents = []
            for paper in papers:
                abstract = paper.get('abstract', '')
                if abstract and abstract != 'No abstract available':
                    full_text = f"Title: {paper.get('title', 'Unknown')}\n\n"
                    full_text += f"Abstract:\n{abstract}"
                    documents.append({
                        'text': full_text,
                        'source': f"{paper.get('source', 'Unknown')}_{paper.get('id', 'Unknown')}"
                    })

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=config.chunk_size,
                chunk_overlap=config.chunk_overlap,
                length_function=len
            )

            qdrant_search = QdrantHybridSearch(
                embeddings=embeddings,
                sparse_method=config.sparse_method,
                collection_name="medical_papers",
                use_memory=True
            )
            qdrant_search.create_collection()
            qdrant_search.add_documents(documents, text_splitter)
            print("   âœ… Qdrant ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ")

    else:
        # ê¸°ì¡´ Full PDF ëª¨ë“œ
        # 3. PDF ë‹¤ìš´ë¡œë“œ
        print("\n" + "=" * 60)
        print("ğŸ“¥ Step 2: PDF ë‹¤ìš´ë¡œë“œ")
        print("=" * 60)

        downloader = PDFDownloader(PAPERS_DIR)
        downloaded_files = downloader.download_all(papers)

        if not downloaded_files:
            print("âŒ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 4. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print("\n" + "=" * 60)
        print("ğŸ“„ Step 3: í…ìŠ¤íŠ¸ ì¶”ì¶œ")
        print("=" * 60)

        documents = TextExtractor.extract_all(downloaded_files)

        if not documents:
            print("âŒ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 5. ë…¼ë¬¸ ìš”ì•½ (OpenAI API ìˆëŠ” ê²½ìš°)
        if config.openai_api_key:
            summarizer = PaperSummarizer(
                api_key=config.openai_api_key,
                language=config.language,
                summary_language=config.summary_language
            )
            papers = summarizer.summarize(papers, documents)

        # 6. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print("\n" + "=" * 60)
        print("ğŸ§  Step 4: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ")
        print("=" * 60)

        embeddings = EmbeddingModelFactory.create(
            model_type=config.embedding_model,
            device='cpu',
            openai_api_key=config.openai_api_key
        )

        # 7. RAG ì‹œìŠ¤í…œ êµ¬ì¶•
        print("\n" + "=" * 60)
        print("ğŸ’¾ Step 5: RAG ì‹œìŠ¤í…œ êµ¬ì¶•")
        print("=" * 60)

        # Text Splitter ìƒì„±
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=len
        )

        # Vector DB ì„ íƒì— ë”°ë¼ ë¶„ê¸°
        qdrant_search = None
        if config.vector_db == 'qdrant':
            # Qdrant ê¸°ë°˜ ì‹œìŠ¤í…œ
            print(f"\nğŸ—„ï¸ Qdrant Vector DB ì‚¬ìš©")

            qdrant_search = QdrantHybridSearch(
                embeddings=embeddings,
                sparse_method=config.sparse_method,
                collection_name="medical_papers",
                use_memory=True  # ì¸ë©”ëª¨ë¦¬ ëª¨ë“œ
            )
            qdrant_search.create_collection()
            qdrant_search.add_documents(documents, text_splitter)

            # RAG ì‹œìŠ¤í…œë„ ìƒì„± (interactive_qaìš©)
            rag = RAGSystem(
                embeddings=embeddings,
                chunk_size=config.chunk_size,
                chunk_overlap=config.chunk_overlap,
                language=config.language
            )
            rag.build_vectorstore(documents)

            # 8. Hybrid Search ë¶„ì„ ë° ì‹œê°í™” (Qdrant)
            print("\n" + "=" * 60)
            print("ğŸ”€ Step 6: Qdrant Hybrid Search ë¶„ì„")
            print("=" * 60)

            test_query = search_query
            # ê²€ìƒ‰í•œ ë…¼ë¬¸ ìˆ˜ì— ë§ê²Œ k ì„¤ì • (ìµœì†Œ 10, ìµœëŒ€ ë…¼ë¬¸ ìˆ˜)
            search_k = max(10, config.max_results)
            print(f"\nğŸ” ê²€ìƒ‰ì–´: '{test_query}'")
            print(f"   ğŸ“Š ë¶„ì„ ëŒ€ìƒ: ìƒìœ„ {search_k}ê°œ ê²°ê³¼")
            print("-" * 40)

            # Qdrant ê²€ìƒ‰ ì‹¤í–‰ (ë…¼ë¬¸ ìˆ˜ì— ë§ê²Œ k ì¡°ì •)
            sparse_results = qdrant_search.sparse_search(test_query, k=search_k)
            dense_results = qdrant_search.dense_search(test_query, k=search_k)
            hybrid_results = qdrant_search.hybrid_search(test_query, k=search_k, alpha=0.7)

            # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ sparse ë°©ì‹ (SPLADE ë¡œë“œ ì‹¤íŒ¨ ì‹œ BM25ë¡œ í´ë°±ë  ìˆ˜ ìˆìŒ)
            sparse_name = qdrant_search.get_sparse_method_name()

            # Sparse ê²°ê³¼ (ì „ì²´ í‘œì‹œ)
            print(f"\nğŸ”µ Qdrant Sparse Search ({sparse_name}) ê²°ê³¼: {len(sparse_results)}ê°œ")
            for i, r in enumerate(sparse_results, 1):
                score = r['score']
                source = r['source'][:50]
                print(f"   [{i:2d}] {sparse_name}: {score:.4f} | {source}...")

            # Dense ê²°ê³¼ (HNSW) (ì „ì²´ í‘œì‹œ)
            print(f"\nğŸ”´ Qdrant Dense Search (HNSW, {config.embedding_model}) ê²°ê³¼: {len(dense_results)}ê°œ")
            for i, r in enumerate(dense_results, 1):
                score = r['score']
                source = r['source'][:50]
                print(f"   [{i:2d}] Cosine: {score:.4f} | {source}...")

            # Hybrid ê²°ê³¼ (ì „ì²´ í‘œì‹œ)
            print(f"\nğŸŸ¢ Qdrant Hybrid Search ê²°ê³¼ ({sparse_name} + Dense, Î±=0.7): {len(hybrid_results)}ê°œ")
            for i, r in enumerate(hybrid_results, 1):
                hybrid_score = r.get('hybrid_score', 0)
                sparse_score = r.get('sparse_score', 0)
                dense_score = r.get('dense_score', 0)
                source = r['source'][:50]
                print(f"   [{i:2d}] Hybrid: {hybrid_score:.3f} | Sparse={sparse_score:.3f} + Dense={dense_score:.3f}")
                print(f"        {source}...")

            # Payload í•„í„°ë§ ì˜ˆì‹œ
            print(f"\nğŸ” Payload í•„í„°ë§ í…ŒìŠ¤íŠ¸:")
            filtered_results = qdrant_search.search_with_filter(
                test_query,
                min_chunk_length=100,
                k=search_k
            )
            print(f"   (ìµœì†Œ ì²­í¬ ê¸¸ì´ 100ì ì´ìƒ í•„í„°): {len(filtered_results)}ê°œ")
            for i, r in enumerate(filtered_results, 1):
                print(f"   [{i:2d}] {r['source'][:50]}...")

            # ì‹œê°í™” ìƒì„±
            print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
            qdrant_search.visualize_comparison(
                query=test_query,
                sparse_results=sparse_results,
                dense_results=dense_results,
                hybrid_results=hybrid_results,
                alpha=0.7,
                sparse_method=sparse_name,
                dense_model=config.embedding_model
            )

        else:
            # FAISS ê¸°ë°˜ ì‹œìŠ¤í…œ (ê¸°ì¡´)
            print(f"\nğŸ—„ï¸ FAISS Vector DB ì‚¬ìš©")

            rag = RAGSystem(
                embeddings=embeddings,
                chunk_size=config.chunk_size,
                chunk_overlap=config.chunk_overlap,
                language=config.language
            )

            vectorstore = rag.build_vectorstore(documents)
            rag.save_vectorstore(VECTORSTORE_DIR)

            # 8. Hybrid Search ë¶„ì„ ë° ì‹œê°í™” (FAISS)
            print("\n" + "=" * 60)
            print("ğŸ”€ Step 6: Hybrid Search ë¶„ì„")
            print("=" * 60)

            hybrid_searcher = HybridSearchSystem(rag, sparse_method=config.sparse_method)

            test_query = search_query
            # ê²€ìƒ‰í•œ ë…¼ë¬¸ ìˆ˜ì— ë§ê²Œ k ì„¤ì • (ìµœì†Œ 10, ìµœëŒ€ ë…¼ë¬¸ ìˆ˜)
            search_k = max(10, config.max_results)
            print(f"\nğŸ” ê²€ìƒ‰ì–´: '{test_query}'")
            print(f"   ğŸ“Š ë¶„ì„ ëŒ€ìƒ: ìƒìœ„ {search_k}ê°œ ê²°ê³¼")
            print("-" * 40)

            search_results = hybrid_searcher.compare_all(test_query, k=search_k, alpha=0.5)

            sparse_name = config.sparse_method.upper()

            # ê²°ê³¼ ì¶œë ¥ (ì „ì²´ í‘œì‹œ)
            print(f"\nğŸ”µ Sparse Search ({sparse_name}) ê²°ê³¼: {len(search_results['sparse'])}ê°œ")
            for i, r in enumerate(search_results['sparse'], 1):
                sparse_score = r['score']
                source = r['source'][:50]
                print(f"   [{i:2d}] {sparse_name}: {sparse_score:.2f} | {source}...")

            print(f"\nğŸ”´ Dense Search ({config.embedding_model}) ê²°ê³¼: {len(search_results['dense'])}ê°œ")
            for i, r in enumerate(search_results['dense'], 1):
                l2_dist = r['score']
                source = r['source'][:50]
                print(f"   [{i:2d}] L2 Dist: {l2_dist:.4f} | {source}...")

            print(f"\nğŸŸ¢ Hybrid Search ê²°ê³¼ ({sparse_name} + {config.embedding_model}, Î±=0.5): {len(search_results['hybrid'])}ê°œ")
            for i, r in enumerate(search_results['hybrid'], 1):
                sparse_raw = r.get('sparse_score', 0)
                sparse_norm = r.get('sparse_score_norm', 0)
                sem_norm = r.get('dense_score_norm', 0)
                hybrid_score = r.get('hybrid_score', 0)
                source = r['source'][:50]
                print(f"   [{i:2d}] Hybrid: {hybrid_score:.2f} | {sparse_name}={sparse_raw:.1f}({sparse_norm:.2f}) + Semantic({sem_norm:.2f})")
                print(f"        {source}...")

            # ì‹œê°í™” ì €ì¥ ë° í‘œì‹œ
            print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
            hybrid_searcher.visualize_comparison(test_query, k=search_k, alpha=0.5)

    # 9. ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ (Abstract ëª¨ë“œì™€ PDF ëª¨ë“œ ëª¨ë‘ ì—¬ê¸°ë¡œ ì˜´)
    interactive_qa(rag, config.openai_api_key)

    print("\n" + "=" * 60)
    print("âœ… RAG ì‹œìŠ¤í…œ ì¢…ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
